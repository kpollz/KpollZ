{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDDUcokDmyX4"
      },
      "source": [
        "# I. Lý thuyết\n",
        "\n",
        "1) Tại sao các mô hình deep learning lại chiếm ưu thế hơn so với các mô hình machine learning truyền thống đối với dữ liệu lớn ?\n",
        "\n",
        "A. Do mô hình deep learning có thể được thiết kế với kích thước tùy ý nên có khả năng xấp xỉ mọi hàm số. Do đó nó có khả năng biểu diễn tốt và hoạt động hiệu quả trên dữ liệu lớn.\n",
        "\n",
        "B. Các mô hình machine learning thường bị overfitting đối với dữ liệu lớn ?\n",
        "\n",
        "C. Các mô hình deep learning có chi phí huấn luyện tốn kém hơn so với machine learning.\n",
        "\n",
        "D. Do kiến trúc của mô hình Machine Learning bao gồm nhiều layers xếp chồng.\n",
        "\n",
        "\n",
        "2) Ý nghĩa của hàm loss function trong mạng neural network là gì ?\n",
        "\n",
        "A. Là hàm số đánh giá độ chính xác của mô hình.\n",
        "\n",
        "B. Mục tiêu của quá trình huấn luyện là tối thiểu hóa hàm loss function bằng thuật toán gradient descent. Giá trị của hàm số này giúp đo lường mức độ khớp của dự báo từ mô hình trên dữ liệu huấn luyện.\n",
        "\n",
        "C. Khi loss function giảm thì luôn đảm bảo độ chính xác của mô hình tăng.\n",
        "\n",
        "D. Là hàm số cần tối đa hóa trong quá trình huấn luyện.\n",
        "\n",
        "\n",
        "3) Khi huấn luyện trên các bộ dữ liệu bigdata thì chúng ta nên sử dụng phương pháp nào ?\n",
        "\n",
        "A) Sử dụng gradient descent trên toàn bộ dữ liệu.\n",
        "\n",
        "B) Sử dụng stochastic gradient descent trên từng điểm dữ liệu.\n",
        "\n",
        "C) Mini-batch gradient descent huấn luyện mô hình trên từng tập dữ liệu con có kích thước nhỏ hơn memory CPU/GPU.\n",
        "\n",
        "D) Có thể sử dụng stochastic gradient descent hoặc mini-batch gradient descent.\n",
        "\n",
        "\n",
        "4) Quá trình feed forward và backpropagation thực hiện những gì ?\n",
        "\n",
        "A) feed forward tính toán output và loss function, backpropagation tính đạo hàm trên từng layer và cập nhật trọng số.\n",
        "\n",
        "B) feed forward cập nhật trọng số cho mô hình, backpropagation tính toán output và loss function.\n",
        "\n",
        "C) feed forward tính ra output của mô hình, backpropagation tính toán loss function\n",
        "\n",
        "D) feed forward được thực hiện sau backpropagation.\n",
        "\n",
        "5) Tác dụng của batch normalization là gì ?\n",
        "\n",
        "A) Loại bỏ một tỷ lệ ngẫu nhiên số lượng units tại mỗi layer để tạo thành nhiều kiến trúc kết hợp ngẫu nhiên.\n",
        "\n",
        "B) Tìm ra các tham số phân phối là trung bình và phương sai trên từng mini-batch.\n",
        "\n",
        "C) Đồng nhất phân phối xác suất của $z^{[l]}$ trên mỗi layer $l$.\n",
        "\n",
        "D) Giảm thiểu ảnh hưởng của input distribution shift nhằm giúp huấn luyện loss function nhanh và ổn định hơn."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Câu 1: A"
      ],
      "metadata": {
        "id": "JJnM5YxVXl05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Câu 2: B"
      ],
      "metadata": {
        "id": "haP6oZfZXWqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Câu 3: C"
      ],
      "metadata": {
        "id": "YUtKyRRDXdGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Câu 4: A"
      ],
      "metadata": {
        "id": "keR-3uplXgcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Câu 5: D"
      ],
      "metadata": {
        "id": "KyUKZaG9XiXO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYoLCUgcm1Km"
      },
      "source": [
        "# II. Thực hành\n",
        "\n",
        "Xuất phát từ mô hình tốt nhất của bạn xây dựng được đối với bài toán phân loại income classification tại bài trước. Bạn hãy thực hiện một số thử nghiệm sau:\n",
        "\n",
        "6) Thay đổi hàm loss function, batch size và optimizer.\n",
        "\n",
        "7) Thử nghiệm thêm các layers mà bạn đã học được trong bài này vào kiến trúc của mình.\n",
        "\n",
        "8) Thay đổi các khởi tạo trọng số theo các phân phối khác nhau và đánh giá độ chính xác của kết quả huấn luyện.\n",
        "\n",
        "9) Thiết lập không gian search và tự động hóa tìm kiếm kiến trúc tốt nhất trên optuna.\n",
        "\n",
        "10) Deploy model sử dụng flask ap. Tham khảo [Flaskapp tutorial](https://drive.google.com/file/d/1AZNtzrmnhJ-OBgijWoaAqXbPhJ6xL0Po/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEoIJm8BZH4V",
        "outputId": "2bf47a9a-d5c4-4bf4-ed26-5a930cbacd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.7/dist-packages (2.3.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtlDsJCuZVvx",
        "outputId": "8b316aae-6eac-44c5-b23e-5d8da54a11de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.5)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.6)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.5.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.8.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.0.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.3.3)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir('gdrive/MyDrive/DeepLearning Course 3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiXxN2xTZYmx",
        "outputId": "f5f9a72c-fdc2-4f2a-e735-0267fd6569cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDV_FpI3CPKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc3938a-4220-431d-8703-f802ea066f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5d517192d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch.utils.data as td\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import category_encoders as ce\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhoUBNRZCnXR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDIdRcQdERzy"
      },
      "outputs": [],
      "source": [
        "# Xử lý outlier\n",
        "\n",
        "def find_boxplot_boundaries(feature, whisker_coeff=1.5):\n",
        "    Q1 = feature.quantile(0.25)\n",
        "    Q3 = feature.quantile(0.75)\n",
        "    \n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - IQR * whisker_coeff\n",
        "    upper = Q3 + IQR * whisker_coeff\n",
        "    \n",
        "    return lower, upper\n",
        "\n",
        "\n",
        "class BoxplotOutlierClipper(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def __init__(self, whisker_coeff=1.5):\n",
        "        self.whisker_coeff = whisker_coeff\n",
        "        self.lower = None\n",
        "        self.upper = None\n",
        "    \n",
        "    def fit(self, feature: pd.Series):\n",
        "        self.lower, self.upper = find_boxplot_boundaries(feature)\n",
        "        return self\n",
        "    \n",
        "    def transform(self, feature):\n",
        "        return feature.clip(self.lower, self.upper)\n",
        "    \n",
        "    \n",
        "df['final_weight'] = BoxplotOutlierClipper().fit_transform(df['final_weight'])\n",
        "df['hrs_per_week'] = BoxplotOutlierClipper().fit_transform(df['hrs_per_week'])\n",
        "df['total_education_yrs'] = BoxplotOutlierClipper().fit_transform(df['total_education_yrs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpm1EXznEWEF"
      },
      "outputs": [],
      "source": [
        "# Những quốc gia có số lượng ít hơn 100 chuyển thành other\n",
        "def convert_nationality(x):\n",
        "    if x=='US':\n",
        "        return x\n",
        "    elif x=='Mexico':\n",
        "        return x\n",
        "    elif x=='?':\n",
        "        return x\n",
        "    elif x=='Philippines':\n",
        "        return x\n",
        "    elif x=='Germany':\n",
        "        return x\n",
        "    else:\n",
        "        return 'other'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaQ4gT9xEXNp"
      },
      "outputs": [],
      "source": [
        "# xóa bỏ những giá trị có captital gain từ 40000 tới 100000 (chính là giá trị 99999)\n",
        "df = df[~(df['capital_gain'] >= 40000) & (df['capital_gain'] <= 100000)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwApPeqVEYTi"
      },
      "outputs": [],
      "source": [
        "def fe(X_train, X_val):\n",
        "    cols = df.select_dtypes(include='object')\n",
        "    onehot = ce.OneHotEncoder(cols)\n",
        "    X_train = onehot.fit_transform(X_train)\n",
        "    X_val = onehot.transform(X_val)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    \n",
        "    return X_train, X_val    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyrJuzy-FK-G"
      },
      "outputs": [],
      "source": [
        "df_new = df.sample(frac=1, ignore_index=True, random_state=42)\n",
        "\n",
        "X = df_new.drop(['ID', 'target_income'], axis=1)\n",
        "y = df_new['target_income']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G5sujNfFOQ1"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_val = fe(X_train, X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGSwjnPyIYk8"
      },
      "source": [
        "## Câu 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym2bFPeAFRLx"
      },
      "outputs": [],
      "source": [
        "# thay đổi batch_size=16\n",
        "\n",
        "X_train = torch.tensor(X_train).float()\n",
        "y_train = torch.tensor(y_train.values).long()\n",
        "train_ds = td.TensorDataset(X_train, y_train)\n",
        "train_loader = td.DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=1)\n",
        "\n",
        "X_val = torch.tensor(X_val).float()\n",
        "y_val = torch.tensor(y_val.values).long()\n",
        "val_ds = td.TensorDataset(X_val, y_val)\n",
        "val_loader = td.DataLoader(val_ds, batch_size=16, num_workers=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oAjIBfPGvdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3e8d4d-1b75-4368-d485-c09b72f5062c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (fc1): Linear(in_features=108, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
              "  (fc4): Linear(in_features=10, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "class MyModel(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(X_train.size()[1], 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "        self.fc4 = nn.Linear(10, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x\n",
        "\n",
        "model = MyModel()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR4SnAEcImjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa78c00a-deda-44e3-f9cc-0b3a47987f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss in train set: 0.465883\n",
            "Loss in validation set: 0.454548, Accuracy 85.212%\n",
            "--------------------------------------------------\n",
            "Epoch 2\n",
            "Loss in train set: 0.455059\n",
            "Loss in validation set: 0.458043, Accuracy 83.605%\n",
            "--------------------------------------------------\n",
            "Epoch 3\n",
            "Loss in train set: 0.452820\n",
            "Loss in validation set: 0.454828, Accuracy 85.031%\n",
            "--------------------------------------------------\n",
            "Epoch 4\n",
            "Loss in train set: 0.453779\n",
            "Loss in validation set: 0.450096, Accuracy 85.533%\n",
            "--------------------------------------------------\n",
            "Epoch 5\n",
            "Loss in train set: 0.449484\n",
            "Loss in validation set: 0.451400, Accuracy 84.890%\n",
            "--------------------------------------------------\n",
            "Epoch 6\n",
            "Loss in train set: 0.448029\n",
            "Loss in validation set: 0.451688, Accuracy 85.252%\n",
            "--------------------------------------------------\n",
            "Epoch 7\n",
            "Loss in train set: 0.448486\n",
            "Loss in validation set: 0.453600, Accuracy 84.931%\n",
            "--------------------------------------------------\n",
            "Epoch 8\n",
            "Loss in train set: 0.500436\n",
            "Loss in validation set: 0.451109, Accuracy 85.413%\n",
            "--------------------------------------------------\n",
            "Epoch 9\n",
            "Loss in train set: 0.447685\n",
            "Loss in validation set: 0.454838, Accuracy 85.373%\n",
            "--------------------------------------------------\n",
            "Epoch 10\n",
            "Loss in train set: 0.446314\n",
            "Loss in validation set: 0.450062, Accuracy 85.614%\n",
            "--------------------------------------------------\n",
            "Epoch 11\n",
            "Loss in train set: 0.444338\n",
            "Loss in validation set: 0.450739, Accuracy 85.754%\n",
            "--------------------------------------------------\n",
            "Epoch 12\n",
            "Loss in train set: 0.444655\n",
            "Loss in validation set: 0.453680, Accuracy 85.453%\n",
            "--------------------------------------------------\n",
            "Epoch 13\n",
            "Loss in train set: 0.443863\n",
            "Loss in validation set: 0.451511, Accuracy 85.493%\n",
            "--------------------------------------------------\n",
            "Epoch 14\n",
            "Loss in train set: 0.442330\n",
            "Loss in validation set: 0.452198, Accuracy 85.453%\n",
            "--------------------------------------------------\n",
            "Epoch 15\n",
            "Loss in train set: 0.443868\n",
            "Loss in validation set: 0.452976, Accuracy 85.493%\n",
            "--------------------------------------------------\n",
            "Epoch 16\n",
            "Loss in train set: 0.443719\n",
            "Loss in validation set: 0.452596, Accuracy 85.433%\n",
            "--------------------------------------------------\n",
            "Epoch 17\n",
            "Loss in train set: 0.446509\n",
            "Loss in validation set: 0.456182, Accuracy 84.549%\n",
            "--------------------------------------------------\n",
            "Epoch 18\n",
            "Loss in train set: 0.446215\n",
            "Loss in validation set: 0.455661, Accuracy 84.750%\n",
            "--------------------------------------------------\n",
            "Accuracy: 85.754\n"
          ]
        }
      ],
      "source": [
        "# thay đổi loss function và optimizer\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 100\n",
        "LOSS_CRITERIA = nn.CrossEntropyLoss()\n",
        "MAX_ACC = -float('inf')\n",
        "ERROR_GOING_UP = 0\n",
        "OPTIMIZER = torch.optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def train(model, data_loader, optimizer, loss_criteria):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    for batch, (data, target) in enumerate(data_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = loss_criteria(out, target)\n",
        "        losses += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_loss = losses / (batch + 1)\n",
        "    print(f\"Loss in train set: {avg_loss:.6f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def eval(model, data_loader, optimizer, loss_criteria):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            out = model(data)\n",
        "            loss = loss_criteria(out, target)\n",
        "            losses += loss.item()\n",
        "            _, predict = torch.max(out.data, axis=-1)\n",
        "            acc += predict.eq(target).sum().item()\n",
        "    avg_loss = losses / (batch + 1)\n",
        "    acc = acc * 100 / len(data_loader.dataset)\n",
        "    print(f\"Loss in validation set: {avg_loss:.6f}, Accuracy {acc:.3f}%\")\n",
        "    print('-'*50)\n",
        "    return avg_loss, acc\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f'Epoch {epoch}')\n",
        "    train_loss = train(model, train_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "    val_loss, acc = eval(model, val_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "\n",
        "    if acc > MAX_ACC:\n",
        "        MAX_ACC = acc\n",
        "        ERROR_GOING_UP = 0\n",
        "    else:\n",
        "        ERROR_GOING_UP += 1\n",
        "        if ERROR_GOING_UP == 7:\n",
        "            break\n",
        "print(f\"Accuracy: {MAX_ACC:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Stb67HRPGVK"
      },
      "source": [
        "## Câu 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvy96C6GPF0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e478fb-6ea6-40bf-da14-e91bc6b790ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss in train set: 0.472211\n",
            "Loss in validation set: 0.463843, Accuracy 84.288%\n",
            "--------------------------------------------------\n",
            "Epoch 2\n",
            "Loss in train set: 0.460987\n",
            "Loss in validation set: 0.460270, Accuracy 84.810%\n",
            "--------------------------------------------------\n",
            "Epoch 3\n",
            "Loss in train set: 0.459868\n",
            "Loss in validation set: 0.459252, Accuracy 84.368%\n",
            "--------------------------------------------------\n",
            "Epoch 4\n",
            "Loss in train set: 0.461653\n",
            "Loss in validation set: 0.463527, Accuracy 84.690%\n",
            "--------------------------------------------------\n",
            "Epoch 5\n",
            "Loss in train set: 0.462470\n",
            "Loss in validation set: 0.463754, Accuracy 85.152%\n",
            "--------------------------------------------------\n",
            "Epoch 6\n",
            "Loss in train set: 0.462590\n",
            "Loss in validation set: 0.459368, Accuracy 84.931%\n",
            "--------------------------------------------------\n",
            "Epoch 7\n",
            "Loss in train set: 0.460787\n",
            "Loss in validation set: 0.460105, Accuracy 84.669%\n",
            "--------------------------------------------------\n",
            "Epoch 8\n",
            "Loss in train set: 0.458954\n",
            "Loss in validation set: 0.460095, Accuracy 84.529%\n",
            "--------------------------------------------------\n",
            "Epoch 9\n",
            "Loss in train set: 0.459443\n",
            "Loss in validation set: 0.459086, Accuracy 84.368%\n",
            "--------------------------------------------------\n",
            "Epoch 10\n",
            "Loss in train set: 0.459081\n",
            "Loss in validation set: 0.462535, Accuracy 84.710%\n",
            "--------------------------------------------------\n",
            "Epoch 11\n",
            "Loss in train set: 0.460491\n",
            "Loss in validation set: 0.458140, Accuracy 84.629%\n",
            "--------------------------------------------------\n",
            "Epoch 12\n",
            "Loss in train set: 0.454014\n",
            "Loss in validation set: 0.458443, Accuracy 84.790%\n",
            "--------------------------------------------------\n",
            "Accuracy: 85.152\n"
          ]
        }
      ],
      "source": [
        "class MyModelQ7(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(X_train.size()[1], 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "        self.fc4 = nn.Linear(10, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x\n",
        "\n",
        "model_q7 = MyModelQ7()\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 100\n",
        "LOSS_CRITERIA = nn.CrossEntropyLoss()\n",
        "MAX_ACC = -float('inf')\n",
        "ERROR_GOING_UP = 0\n",
        "OPTIMIZER = torch.optim.RMSprop(model_q7.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def train(model, data_loader, optimizer, loss_criteria):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    for batch, (data, target) in enumerate(data_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = loss_criteria(out, target)\n",
        "        losses += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_loss = losses / (batch + 1)\n",
        "    print(f\"Loss in train set: {avg_loss:.6f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluation(model, data_loader, optimizer, loss_criteria):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            out = model(data)\n",
        "            loss = loss_criteria(out, target)\n",
        "            losses += loss.item()\n",
        "            _, predict = torch.max(out.data, axis=-1)\n",
        "            acc += predict.eq(target).sum().item()\n",
        "    avg_loss = losses / (batch + 1)\n",
        "    acc = acc * 100 / len(data_loader.dataset)\n",
        "    print(f\"Loss in validation set: {avg_loss:.6f}, Accuracy {acc:.3f}%\")\n",
        "    print('-'*50)\n",
        "    return avg_loss, acc\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f'Epoch {epoch}')\n",
        "    train_loss = train(model_q7, train_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "    val_loss, acc = evaluation(model_q7, val_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "\n",
        "    if acc > MAX_ACC:\n",
        "        MAX_ACC = acc\n",
        "        ERROR_GOING_UP = 0\n",
        "    else:\n",
        "        ERROR_GOING_UP += 1\n",
        "        if ERROR_GOING_UP == 7:\n",
        "            break\n",
        "print(f\"Accuracy: {MAX_ACC:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W8rEpzuRJgV"
      },
      "source": [
        "## Câu 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Su8a8uNsV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4f5ffa-09cb-4156-ddb7-f420ef6e614b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss in train set: 0.467085\n",
            "Loss in validation set: 0.453303, Accuracy 85.071%\n",
            "--------------------------------------------------\n",
            "Epoch 2\n",
            "Loss in train set: 0.457219\n",
            "Loss in validation set: 0.454800, Accuracy 85.071%\n",
            "--------------------------------------------------\n",
            "Epoch 3\n",
            "Loss in train set: 0.457227\n",
            "Loss in validation set: 0.456179, Accuracy 84.850%\n",
            "--------------------------------------------------\n",
            "Epoch 4\n",
            "Loss in train set: 0.456489\n",
            "Loss in validation set: 0.457961, Accuracy 84.448%\n",
            "--------------------------------------------------\n",
            "Epoch 5\n",
            "Loss in train set: 0.453232\n",
            "Loss in validation set: 0.454793, Accuracy 84.649%\n",
            "--------------------------------------------------\n",
            "Epoch 6\n",
            "Loss in train set: 0.506342\n",
            "Loss in validation set: 0.460926, Accuracy 85.051%\n",
            "--------------------------------------------------\n",
            "Epoch 7\n",
            "Loss in train set: 0.453881\n",
            "Loss in validation set: 0.460384, Accuracy 84.890%\n",
            "--------------------------------------------------\n",
            "Epoch 8\n",
            "Loss in train set: 0.451247\n",
            "Loss in validation set: 0.453622, Accuracy 85.393%\n",
            "--------------------------------------------------\n",
            "Epoch 9\n",
            "Loss in train set: 0.655239\n",
            "Loss in validation set: 0.668622, Accuracy 84.790%\n",
            "--------------------------------------------------\n",
            "Epoch 10\n",
            "Loss in train set: 0.633036\n",
            "Loss in validation set: 0.460490, Accuracy 84.669%\n",
            "--------------------------------------------------\n",
            "Epoch 11\n",
            "Loss in train set: 0.453306\n",
            "Loss in validation set: 0.454906, Accuracy 84.991%\n",
            "--------------------------------------------------\n",
            "Epoch 12\n",
            "Loss in train set: 0.453237\n",
            "Loss in validation set: 0.454755, Accuracy 84.911%\n",
            "--------------------------------------------------\n",
            "Epoch 13\n",
            "Loss in train set: 0.462735\n",
            "Loss in validation set: 0.452529, Accuracy 85.232%\n",
            "--------------------------------------------------\n",
            "Epoch 14\n",
            "Loss in train set: 0.474954\n",
            "Loss in validation set: 0.668192, Accuracy 84.911%\n",
            "--------------------------------------------------\n",
            "Epoch 15\n",
            "Loss in train set: 0.534154\n",
            "Loss in validation set: 0.454933, Accuracy 85.373%\n",
            "--------------------------------------------------\n",
            "Accuracy: 85.393\n"
          ]
        }
      ],
      "source": [
        "# weights with kaiming_uniform_\n",
        "\n",
        "class MyModelQ8(nn.Module):\n",
        "    \n",
        "    def __init__(self, init_weight=None):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(X_train.size()[1], 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "        self.fc4 = nn.Linear(10, 2)\n",
        "        \n",
        "        if init_weight is not None:\n",
        "            for fc in [self.fc1, self.fc2, self.fc3, self.fc4]:\n",
        "                if init_weight == 'kaiming_uniform_':\n",
        "                    nn.init.kaiming_uniform_(fc.weight, nonlinearity='relu')\n",
        "                elif init_weight == 'kaiming_normal_':\n",
        "                    nn.init.kaiming_normal_(fc.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x\n",
        "\n",
        "model_q8 = MyModelQ8(init_weight='kaiming_uniform_')\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 100\n",
        "LOSS_CRITERIA = nn.CrossEntropyLoss()\n",
        "MAX_ACC = -float('inf')\n",
        "ERROR_GOING_UP = 0\n",
        "OPTIMIZER = torch.optim.RMSprop(model_q8.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def train(model, data_loader, optimizer, loss_criteria):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    for batch, (data, target) in enumerate(data_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = loss_criteria(out, target)\n",
        "        losses += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_loss = losses / (batch + 1)\n",
        "    print(f\"Loss in train set: {avg_loss:.6f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluation(model, data_loader, optimizer, loss_criteria):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            out = model(data)\n",
        "            loss = loss_criteria(out, target)\n",
        "            losses += loss.item()\n",
        "            _, predict = torch.max(out.data, axis=-1)\n",
        "            acc += predict.eq(target).sum().item()\n",
        "    avg_loss = losses / (batch + 1)\n",
        "    acc = acc * 100 / len(data_loader.dataset)\n",
        "    print(f\"Loss in validation set: {avg_loss:.6f}, Accuracy {acc:.3f}%\")\n",
        "    print('-'*50)\n",
        "    return avg_loss, acc\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f'Epoch {epoch}')\n",
        "    train_loss = train(model_q8, train_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "    val_loss, acc = evaluation(model_q8, val_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "\n",
        "    if acc > MAX_ACC:\n",
        "        MAX_ACC = acc\n",
        "        ERROR_GOING_UP = 0\n",
        "    else:\n",
        "        ERROR_GOING_UP += 1\n",
        "        if ERROR_GOING_UP == 7:\n",
        "            break\n",
        "print(f\"Accuracy: {MAX_ACC:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qu3_lGqZzXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670b448e-f4eb-42d3-b6ab-11dbf7d970a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss in train set: 0.498342\n",
            "Loss in validation set: 0.495917, Accuracy 78.119%\n",
            "--------------------------------------------------\n",
            "Epoch 2\n",
            "Loss in train set: 0.526129\n",
            "Loss in validation set: 0.548438, Accuracy 76.411%\n",
            "--------------------------------------------------\n",
            "Epoch 3\n",
            "Loss in train set: 0.489991\n",
            "Loss in validation set: 0.504788, Accuracy 76.411%\n",
            "--------------------------------------------------\n",
            "Epoch 4\n",
            "Loss in train set: 0.497504\n",
            "Loss in validation set: 0.496085, Accuracy 76.411%\n",
            "--------------------------------------------------\n",
            "Epoch 5\n",
            "Loss in train set: 0.494893\n",
            "Loss in validation set: 0.485980, Accuracy 76.411%\n",
            "--------------------------------------------------\n",
            "Epoch 6\n",
            "Loss in train set: 0.490719\n",
            "Loss in validation set: 0.495291, Accuracy 76.411%\n",
            "--------------------------------------------------\n",
            "Epoch 7\n",
            "Loss in train set: 0.490551\n",
            "Loss in validation set: 0.490971, Accuracy 76.411%\n",
            "--------------------------------------------------\n",
            "Epoch 8\n",
            "Loss in train set: 0.490839\n",
            "Loss in validation set: 0.491638, Accuracy 76.411%\n",
            "--------------------------------------------------\n",
            "Accuracy: 78.119\n"
          ]
        }
      ],
      "source": [
        "# weights with kaiming_normal_\n",
        "\n",
        "class MyModelQ8(nn.Module):\n",
        "    \n",
        "    def __init__(self, init_weight=None):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(X_train.size()[1], 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "        self.fc4 = nn.Linear(10, 2)\n",
        "        \n",
        "        if init_weight is not None:\n",
        "            for fc in [self.fc1, self.fc2, self.fc3, self.fc4]:\n",
        "                if init_weight == 'kaiming_uniform_':\n",
        "                    nn.init.kaiming_uniform_(fc.weight, nonlinearity='relu')\n",
        "                elif init_weight == 'kaiming_normal_':\n",
        "                    nn.init.kaiming_normal_(fc.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x\n",
        "\n",
        "model_q8 = MyModelQ8(init_weight='kaiming_normal_')\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 100\n",
        "LOSS_CRITERIA = nn.CrossEntropyLoss()\n",
        "MAX_ACC = -float('inf')\n",
        "ERROR_GOING_UP = 0\n",
        "OPTIMIZER = torch.optim.RMSprop(model_q8.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def train(model, data_loader, optimizer, loss_criteria):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    for batch, (data, target) in enumerate(data_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = loss_criteria(out, target)\n",
        "        losses += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_loss = losses / (batch + 1)\n",
        "    print(f\"Loss in train set: {avg_loss:.6f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluation(model, data_loader, optimizer, loss_criteria):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            out = model(data)\n",
        "            loss = loss_criteria(out, target)\n",
        "            losses += loss.item()\n",
        "            _, predict = torch.max(out.data, axis=-1)\n",
        "            acc += predict.eq(target).sum().item()\n",
        "    avg_loss = losses / (batch + 1)\n",
        "    acc = acc * 100 / len(data_loader.dataset)\n",
        "    print(f\"Loss in validation set: {avg_loss:.6f}, Accuracy {acc:.3f}%\")\n",
        "    print('-'*50)\n",
        "    return avg_loss, acc\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f'Epoch {epoch}')\n",
        "    train_loss = train(model_q8, train_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "    val_loss, acc = evaluation(model_q8, val_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "\n",
        "    if acc > MAX_ACC:\n",
        "        MAX_ACC = acc\n",
        "        ERROR_GOING_UP = 0\n",
        "    else:\n",
        "        ERROR_GOING_UP += 1\n",
        "        if ERROR_GOING_UP == 7:\n",
        "            break\n",
        "print(f\"Accuracy: {MAX_ACC:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YLo00uuWXOW"
      },
      "source": [
        "## Câu 9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna in Pytorch"
      ],
      "metadata": {
        "id": "p9bGGpvlcF9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqvvf0zgK7UD"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.trial import TrialState"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCO-lrwbaTzZ"
      },
      "outputs": [],
      "source": [
        "def define_model(trial):\n",
        "    n_layers = trial.suggest_int('n_layers', 2, 5)\n",
        "    layers = []\n",
        "    in_features = X_train.size()[1]\n",
        "    for i in range(n_layers):\n",
        "        out_features = trial.suggest_int('n_units_l{}'.format(i), 4, 64)\n",
        "        layers.append(nn.Linear(in_features, out_features))\n",
        "        layers.append(nn.ReLU())\n",
        "        prob = trial.suggest_float('dropout_l{}'.format(i), 0.2, 0.5)\n",
        "        layers.append(nn.Dropout(prob))\n",
        "        in_features = out_features\n",
        "    layers.append(nn.Linear(in_features, 2))\n",
        "    layers.append(nn.Sigmoid())\n",
        "    \n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg4b2OrOom9m"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "BATCHSIZE = 16\n",
        "EPOCHS = 30\n",
        "\n",
        "def objective(trial):\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = loss_criteria(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(val_loader):\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "                output = model(data)\n",
        "                _, pred = torch.max(output, axis=-1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "\n",
        "        accuracy = correct / len(val_loader.dataset)\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_IJHr5Dqnof",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfab2b1-d185-408a-a25f-8d1f9de8c9be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-02-18 10:48:47,051]\u001b[0m A new study created in memory with name: no-name-aa9e100b-5ada-49af-83d9-2e56b81e2ffa\u001b[0m\n",
            "\u001b[32m[I 2022-02-18 10:51:49,316]\u001b[0m Trial 0 finished with value: 0.8555354631303999 and parameters: {'n_layers': 3, 'n_units_l0': 18, 'dropout_l0': 0.3192228380390431, 'n_units_l1': 34, 'dropout_l1': 0.41543260127512643, 'n_units_l2': 33, 'dropout_l2': 0.2176656414722933, 'optimizer': 'Adam', 'lr': 0.0007387434703682037}. Best is trial 0 with value: 0.8555354631303999.\u001b[0m\n",
            "\u001b[32m[I 2022-02-18 10:54:50,361]\u001b[0m Trial 1 finished with value: 0.7641149286718907 and parameters: {'n_layers': 3, 'n_units_l0': 48, 'dropout_l0': 0.30857972002455647, 'n_units_l1': 8, 'dropout_l1': 0.22862513268774018, 'n_units_l2': 8, 'dropout_l2': 0.3603128784008549, 'optimizer': 'Adam', 'lr': 0.07123216165129294}. Best is trial 0 with value: 0.8555354631303999.\u001b[0m\n",
            "\u001b[32m[I 2022-02-18 10:57:21,549]\u001b[0m Trial 2 finished with value: 0.8527225236085996 and parameters: {'n_layers': 2, 'n_units_l0': 59, 'dropout_l0': 0.21644544473438812, 'n_units_l1': 26, 'dropout_l1': 0.3771078054927576, 'optimizer': 'SGD', 'lr': 0.07038685167908447}. Best is trial 0 with value: 0.8555354631303999.\u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:00:04,721]\u001b[0m Trial 3 finished with value: 0.8573437813944143 and parameters: {'n_layers': 2, 'n_units_l0': 35, 'dropout_l0': 0.32784109110544923, 'n_units_l1': 37, 'dropout_l1': 0.282523114441712, 'optimizer': 'RMSprop', 'lr': 0.0001417041353571361}. Best is trial 3 with value: 0.8573437813944143.\u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:02:48,649]\u001b[0m Trial 4 finished with value: 0.8549326903757284 and parameters: {'n_layers': 2, 'n_units_l0': 64, 'dropout_l0': 0.31350707369274067, 'n_units_l1': 56, 'dropout_l1': 0.30167108683684507, 'optimizer': 'Adam', 'lr': 7.031925458916296e-05}. Best is trial 3 with value: 0.8573437813944143.\u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:02:59,622]\u001b[0m Trial 5 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:03:11,423]\u001b[0m Trial 6 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:03:16,971]\u001b[0m Trial 7 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:03:22,655]\u001b[0m Trial 8 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:03:33,506]\u001b[0m Trial 9 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:03:45,942]\u001b[0m Trial 10 pruned. \u001b[0m\n",
            "\u001b[32m[I 2022-02-18 11:06:43,153]\u001b[0m Trial 11 finished with value: 0.8507132810930279 and parameters: {'n_layers': 4, 'n_units_l0': 18, 'dropout_l0': 0.2837793560232981, 'n_units_l1': 26, 'dropout_l1': 0.49593422463131864, 'n_units_l2': 52, 'dropout_l2': 0.2059847296825206, 'n_units_l3': 39, 'dropout_l3': 0.26450046676876354, 'optimizer': 'RMSprop', 'lr': 0.0008837841345814392}. Best is trial 3 with value: 0.8573437813944143.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  12\n",
            "  Number of pruned trials:  6\n",
            "  Number of complete trials:  6\n",
            "Best trial:\n",
            "  Value:  0.8573437813944143\n",
            "  Params: \n",
            "    n_layers: 2\n",
            "    n_units_l0: 35\n",
            "    dropout_l0: 0.32784109110544923\n",
            "    n_units_l1: 37\n",
            "    dropout_l1: 0.282523114441712\n",
            "    optimizer: RMSprop\n",
            "    lr: 0.0001417041353571361\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100, timeout=900)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIp9Di-B4s1S"
      },
      "outputs": [],
      "source": [
        "model = define_model(trial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89UG1RqyWXOX",
        "outputId": "b7313522-6540-4e9f-d2b8-f82c6823ba62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss in train set: 0.542352\n",
            "Loss in validation set: 0.491281, Accuracy 78.340%\n",
            "--------------------------------------------------\n",
            "Epoch 2\n",
            "Loss in train set: 0.479595\n",
            "Loss in validation set: 0.464957, Accuracy 84.549%\n",
            "--------------------------------------------------\n",
            "Epoch 3\n",
            "Loss in train set: 0.465264\n",
            "Loss in validation set: 0.456809, Accuracy 84.750%\n",
            "--------------------------------------------------\n",
            "Epoch 4\n",
            "Loss in train set: 0.459294\n",
            "Loss in validation set: 0.454136, Accuracy 85.152%\n",
            "--------------------------------------------------\n",
            "Epoch 5\n",
            "Loss in train set: 0.457008\n",
            "Loss in validation set: 0.452364, Accuracy 85.252%\n",
            "--------------------------------------------------\n",
            "Epoch 6\n",
            "Loss in train set: 0.454927\n",
            "Loss in validation set: 0.452040, Accuracy 85.272%\n",
            "--------------------------------------------------\n",
            "Epoch 7\n",
            "Loss in train set: 0.453844\n",
            "Loss in validation set: 0.450862, Accuracy 85.333%\n",
            "--------------------------------------------------\n",
            "Epoch 8\n",
            "Loss in train set: 0.452495\n",
            "Loss in validation set: 0.450811, Accuracy 85.413%\n",
            "--------------------------------------------------\n",
            "Epoch 9\n",
            "Loss in train set: 0.451445\n",
            "Loss in validation set: 0.450328, Accuracy 85.574%\n",
            "--------------------------------------------------\n",
            "Epoch 10\n",
            "Loss in train set: 0.451089\n",
            "Loss in validation set: 0.450241, Accuracy 85.533%\n",
            "--------------------------------------------------\n",
            "Epoch 11\n",
            "Loss in train set: 0.449513\n",
            "Loss in validation set: 0.450039, Accuracy 85.513%\n",
            "--------------------------------------------------\n",
            "Epoch 12\n",
            "Loss in train set: 0.448851\n",
            "Loss in validation set: 0.449955, Accuracy 85.493%\n",
            "--------------------------------------------------\n",
            "Epoch 13\n",
            "Loss in train set: 0.448613\n",
            "Loss in validation set: 0.448975, Accuracy 85.674%\n",
            "--------------------------------------------------\n",
            "Epoch 14\n",
            "Loss in train set: 0.448672\n",
            "Loss in validation set: 0.448965, Accuracy 85.754%\n",
            "--------------------------------------------------\n",
            "Epoch 15\n",
            "Loss in train set: 0.447862\n",
            "Loss in validation set: 0.449076, Accuracy 85.714%\n",
            "--------------------------------------------------\n",
            "Epoch 16\n",
            "Loss in train set: 0.448126\n",
            "Loss in validation set: 0.449070, Accuracy 85.554%\n",
            "--------------------------------------------------\n",
            "Epoch 17\n",
            "Loss in train set: 0.445909\n",
            "Loss in validation set: 0.448752, Accuracy 85.594%\n",
            "--------------------------------------------------\n",
            "Epoch 18\n",
            "Loss in train set: 0.446971\n",
            "Loss in validation set: 0.449190, Accuracy 85.393%\n",
            "--------------------------------------------------\n",
            "Epoch 19\n",
            "Loss in train set: 0.445392\n",
            "Loss in validation set: 0.449408, Accuracy 85.373%\n",
            "--------------------------------------------------\n",
            "Epoch 20\n",
            "Loss in train set: 0.444971\n",
            "Loss in validation set: 0.448970, Accuracy 85.493%\n",
            "--------------------------------------------------\n",
            "Epoch 21\n",
            "Loss in train set: 0.445058\n",
            "Loss in validation set: 0.448870, Accuracy 85.393%\n",
            "--------------------------------------------------\n",
            "Best accuracy: 85.754\n"
          ]
        }
      ],
      "source": [
        "def train(model, data_loader, optimizer, loss_criteria):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    for batch, (data, target) in enumerate(data_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = loss_criteria(out, target)\n",
        "        losses += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_loss = losses / (batch + 1)\n",
        "    print(f\"Loss in train set: {avg_loss:.6f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluation(model, data_loader, optimizer, loss_criteria):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            out = model(data)\n",
        "            loss = loss_criteria(out, target)\n",
        "            losses += loss.item()\n",
        "            _, predict = torch.max(out.data, axis=-1)\n",
        "            acc += predict.eq(target).sum().item()\n",
        "    avg_loss = losses / (batch + 1)\n",
        "    acc = acc * 100 / len(data_loader.dataset)\n",
        "    print(f\"Loss in validation set: {avg_loss:.6f}, Accuracy {acc:.3f}%\")\n",
        "    print('-'*50)\n",
        "    return avg_loss, acc\n",
        "\n",
        "OPTIMIZER = getattr(optim, trial.params['optimizer'])(model.parameters(), lr=trial.params['lr'])\n",
        "LOSS_CRITERIA = nn.CrossEntropyLoss()\n",
        "MAX_ACC = -float('inf')\n",
        "ERROR_GOING_UP = 0\n",
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f'Epoch {epoch}')\n",
        "    train_loss = train(model, train_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "    val_loss, acc = evaluation(model, val_loader, OPTIMIZER, LOSS_CRITERIA)\n",
        "\n",
        "    if acc > MAX_ACC:\n",
        "        MAX_ACC = acc\n",
        "        ERROR_GOING_UP = 0\n",
        "    else:\n",
        "        ERROR_GOING_UP += 1\n",
        "        if ERROR_GOING_UP == 7:\n",
        "            break\n",
        "print(f\"Best accuracy: {MAX_ACC:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_qDAUDZWXOY"
      },
      "source": [
        "### Optuna in Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUV3xdbZWXOY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtvJMv4kWXOY",
        "outputId": "8e9543c7-fef4-4a7e-b75f-5110181ad729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "623/623 [==============================] - 5s 6ms/step - loss: 0.3780 - acc: 0.8249 - val_loss: 0.3250 - val_acc: 0.8523\n",
            "Epoch 2/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.3266 - acc: 0.8470 - val_loss: 0.3217 - val_acc: 0.8525\n",
            "Epoch 3/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.3188 - acc: 0.8493 - val_loss: 0.3171 - val_acc: 0.8553\n",
            "Epoch 4/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.3126 - acc: 0.8529 - val_loss: 0.3149 - val_acc: 0.8555\n",
            "Epoch 5/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.3103 - acc: 0.8532 - val_loss: 0.3163 - val_acc: 0.8557\n",
            "Epoch 6/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.3076 - acc: 0.8553 - val_loss: 0.3136 - val_acc: 0.8559\n",
            "Epoch 7/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.3040 - acc: 0.8572 - val_loss: 0.3142 - val_acc: 0.8551\n",
            "Epoch 8/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.3026 - acc: 0.8571 - val_loss: 0.3134 - val_acc: 0.8561\n",
            "Epoch 9/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2994 - acc: 0.8570 - val_loss: 0.3128 - val_acc: 0.8559\n",
            "Epoch 10/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2994 - acc: 0.8578 - val_loss: 0.3118 - val_acc: 0.8555\n",
            "Epoch 11/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2957 - acc: 0.8609 - val_loss: 0.3151 - val_acc: 0.8561\n",
            "Epoch 12/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2949 - acc: 0.8614 - val_loss: 0.3134 - val_acc: 0.8541\n",
            "Epoch 13/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2941 - acc: 0.8633 - val_loss: 0.3139 - val_acc: 0.8567\n",
            "Epoch 14/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2910 - acc: 0.8633 - val_loss: 0.3138 - val_acc: 0.8555\n",
            "Epoch 15/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2906 - acc: 0.8645 - val_loss: 0.3138 - val_acc: 0.8547\n",
            "Epoch 16/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2881 - acc: 0.8653 - val_loss: 0.3155 - val_acc: 0.8573\n",
            "Epoch 17/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2874 - acc: 0.8659 - val_loss: 0.3170 - val_acc: 0.8563\n",
            "Epoch 18/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2872 - acc: 0.8653 - val_loss: 0.3177 - val_acc: 0.8547\n",
            "Epoch 19/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2852 - acc: 0.8664 - val_loss: 0.3141 - val_acc: 0.8553\n",
            "Epoch 20/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2851 - acc: 0.8674 - val_loss: 0.3192 - val_acc: 0.8533\n",
            "Epoch 21/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2819 - acc: 0.8674 - val_loss: 0.3218 - val_acc: 0.8567\n",
            "Epoch 22/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2819 - acc: 0.8679 - val_loss: 0.3208 - val_acc: 0.8569\n",
            "Epoch 23/100\n",
            "623/623 [==============================] - 4s 6ms/step - loss: 0.2824 - acc: 0.8686 - val_loss: 0.3205 - val_acc: 0.8559\n",
            "--------------------------------------------------------------------------------\n",
            "Best accuracy: 0.857\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_val = fe(X_train, X_val)\n",
        "\n",
        "train_data = np.column_stack([X_train, y_train.values.reshape(-1, 1)])\n",
        "val_data = np.column_stack([X_val, y_val.values.reshape(-1, 1)])\n",
        "\n",
        "def load_train(train):\n",
        "    train = tf.data.Dataset.from_tensor_slices(train)\n",
        "    train = train.shuffle(10000)\n",
        "    train = train.map(lambda x: (x[:-1], x[-1]))\n",
        "    return train.batch(32).prefetch(1)\n",
        "\n",
        "def load_val(val):\n",
        "    val = tf.data.Dataset.from_tensor_slices(val)\n",
        "    val = val.map(lambda x: (x[:-1], x[-1]))\n",
        "    return val.batch(32).prefetch(1)\n",
        "\n",
        "\n",
        "class MyModelTensorFlow(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense1 = layers.Dense(64, activation='relu')\n",
        "        self.dense2 = layers.Dense(32, activation='relu')\n",
        "        self.dense3 = layers.Dense(1, activation='sigmoid')\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "train =  load_train(train_data)\n",
        "val = load_val(val_data)\n",
        "\n",
        "model = MyModelTensorFlow()\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_acc', patience=7, mode='max', restore_best_weights=True)\n",
        "model.compile('adam', loss='binary_crossentropy', metrics='acc')\n",
        "history = model.fit(train, epochs=100, validation_data=(val), callbacks=[early_stop])\n",
        "print('-'*80)\n",
        "print(f\"Best accuracy: {max(history.history['val_acc']):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PFqbfHuWXOY"
      },
      "outputs": [],
      "source": [
        "def define_model(trial):\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
        "    model = tf.keras.Sequential()\n",
        "    for i in range(num_layers):\n",
        "        n_units = trial.suggest_int('n_units_l{}'.format(i), 4, 128)\n",
        "        model.add(layers.Dense(n_units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n",
        "        prob = trial.suggest_float('dropout_l{}'.format(i), 0.2, 0.4)\n",
        "        model.add(layers.Dropout(prob))\n",
        "    model.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSpN3N6fWXOY"
      },
      "outputs": [],
      "source": [
        "def define_optimizer(trial):\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
        "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(tf.keras.optimizers, optimizer_name)(learning_rate=lr)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZCx83bHWXOY"
      },
      "outputs": [],
      "source": [
        "def learn(model, optimizer, dataset, mode='eval'):\n",
        "    accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "    for batch, (data, target) in enumerate(dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(data, training=(mode == \"train\"))\n",
        "            binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "            loss_value = tf.reduce_mean(binary_cross_entropy(y_true=target, y_pred=logits))\n",
        "            if mode == \"eval\":\n",
        "                accuracy(\n",
        "                    tf.cast(target, tf.int32), logits\n",
        "                )\n",
        "            else:\n",
        "                grads = tape.gradient(loss_value, model.variables)\n",
        "                optimizer.apply_gradients(zip(grads, model.variables))\n",
        "    if mode == 'eval':\n",
        "        return accuracy.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r89HSNeqWXOZ"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    model = define_model(trial)\n",
        "    optimizer = define_optimizer(trial)\n",
        "    for epoch in range(20):\n",
        "        learn(model, optimizer, train, mode='train')\n",
        "        acc = learn(model, optimizer, val, mode='eval')\n",
        "        trial.report(acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkT6EjHUWXOZ",
        "outputId": "7e00c967-550e-45b2-d649-117c11ca49e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-02-18 12:25:27,957]\u001b[0m A new study created in memory with name: no-name-59b29f76-00d2-4111-9a09-b6b412b9d479\u001b[0m\n",
            "\u001b[32m[I 2022-02-18 12:29:56,198]\u001b[0m Trial 0 finished with value: 0.8482041954994202 and parameters: {'num_layers': 1, 'weight_decay': 4.148372512784383e-09, 'n_units_l0': 96, 'dropout_l0': 0.39938897842497756, 'optimizer': 'Adam', 'learning_rate': 1.1341478924715399e-05}. Best is trial 0 with value: 0.8482041954994202.\u001b[0m\n",
            "\u001b[32m[I 2022-02-18 12:36:25,299]\u001b[0m Trial 1 finished with value: 0.8486048579216003 and parameters: {'num_layers': 3, 'weight_decay': 1.1075709272781065e-05, 'n_units_l0': 73, 'dropout_l0': 0.39576221100782805, 'n_units_l1': 81, 'dropout_l1': 0.21499000809671132, 'n_units_l2': 78, 'dropout_l2': 0.21058591882381128, 'optimizer': 'Adam', 'learning_rate': 2.9644946659079335e-05}. Best is trial 1 with value: 0.8486048579216003.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  2\n",
            "  Number of pruned trials:  0\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  0.8486048579216003\n",
            "  Params: \n",
            "    num_layers: 3\n",
            "    weight_decay: 1.1075709272781065e-05\n",
            "    n_units_l0: 73\n",
            "    dropout_l0: 0.39576221100782805\n",
            "    n_units_l1: 81\n",
            "    dropout_l1: 0.21499000809671132\n",
            "    n_units_l2: 78\n",
            "    dropout_l2: 0.21058591882381128\n",
            "    optimizer: Adam\n",
            "    learning_rate: 2.9644946659079335e-05\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50, timeout=600)\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(trial)\n",
        "early_stop = EarlyStopping(monitor='val_acc', patience=7, mode='max', restore_best_weights=True)\n",
        "optimizer = getattr(tf.keras.optimizers, trial.params['optimizer'])(learning_rate=trial.params['learning_rate'])\n",
        "model.compile(optimizer, loss='binary_crossentropy', metrics='acc')\n",
        "history = model.fit(train, epochs=100, validation_data=(val), callbacks=[early_stop])\n",
        "print('-'*127)\n",
        "print(f\"Best accuracy: {max(history.history['val_acc']):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v_3S0zLyYmh",
        "outputId": "fb929c8f-82b4-44cf-d75b-126f712fa417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "623/623 [==============================] - 9s 11ms/step - loss: 0.5905 - acc: 0.7225 - val_loss: 0.5240 - val_acc: 0.7641\n",
            "Epoch 2/100\n",
            "623/623 [==============================] - 7s 11ms/step - loss: 0.5207 - acc: 0.7646 - val_loss: 0.4758 - val_acc: 0.7685\n",
            "Epoch 3/100\n",
            "623/623 [==============================] - 7s 12ms/step - loss: 0.4786 - acc: 0.7775 - val_loss: 0.4353 - val_acc: 0.7882\n",
            "Epoch 4/100\n",
            "623/623 [==============================] - 5s 8ms/step - loss: 0.4469 - acc: 0.7894 - val_loss: 0.4041 - val_acc: 0.8166\n",
            "Epoch 5/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.4263 - acc: 0.8001 - val_loss: 0.3844 - val_acc: 0.8266\n",
            "Epoch 6/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.4110 - acc: 0.8081 - val_loss: 0.3716 - val_acc: 0.8320\n",
            "Epoch 7/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.4008 - acc: 0.8154 - val_loss: 0.3636 - val_acc: 0.8352\n",
            "Epoch 8/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3902 - acc: 0.8180 - val_loss: 0.3580 - val_acc: 0.8371\n",
            "Epoch 9/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3826 - acc: 0.8237 - val_loss: 0.3532 - val_acc: 0.8411\n",
            "Epoch 10/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3798 - acc: 0.8259 - val_loss: 0.3496 - val_acc: 0.8423\n",
            "Epoch 11/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3741 - acc: 0.8229 - val_loss: 0.3464 - val_acc: 0.8437\n",
            "Epoch 12/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3682 - acc: 0.8288 - val_loss: 0.3437 - val_acc: 0.8441\n",
            "Epoch 13/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3680 - acc: 0.8282 - val_loss: 0.3416 - val_acc: 0.8439\n",
            "Epoch 14/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3646 - acc: 0.8298 - val_loss: 0.3398 - val_acc: 0.8447\n",
            "Epoch 15/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3616 - acc: 0.8326 - val_loss: 0.3383 - val_acc: 0.8467\n",
            "Epoch 16/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3597 - acc: 0.8324 - val_loss: 0.3367 - val_acc: 0.8479\n",
            "Epoch 17/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3546 - acc: 0.8350 - val_loss: 0.3350 - val_acc: 0.8487\n",
            "Epoch 18/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3584 - acc: 0.8353 - val_loss: 0.3339 - val_acc: 0.8493\n",
            "Epoch 19/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3533 - acc: 0.8385 - val_loss: 0.3328 - val_acc: 0.8493\n",
            "Epoch 20/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3534 - acc: 0.8358 - val_loss: 0.3317 - val_acc: 0.8499\n",
            "Epoch 21/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3505 - acc: 0.8390 - val_loss: 0.3307 - val_acc: 0.8483\n",
            "Epoch 22/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3476 - acc: 0.8393 - val_loss: 0.3302 - val_acc: 0.8491\n",
            "Epoch 23/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3468 - acc: 0.8373 - val_loss: 0.3291 - val_acc: 0.8493\n",
            "Epoch 24/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3470 - acc: 0.8403 - val_loss: 0.3285 - val_acc: 0.8509\n",
            "Epoch 25/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3422 - acc: 0.8395 - val_loss: 0.3276 - val_acc: 0.8501\n",
            "Epoch 26/100\n",
            "623/623 [==============================] - 5s 8ms/step - loss: 0.3412 - acc: 0.8409 - val_loss: 0.3269 - val_acc: 0.8501\n",
            "Epoch 27/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3447 - acc: 0.8409 - val_loss: 0.3266 - val_acc: 0.8511\n",
            "Epoch 28/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3418 - acc: 0.8419 - val_loss: 0.3261 - val_acc: 0.8515\n",
            "Epoch 29/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3399 - acc: 0.8428 - val_loss: 0.3256 - val_acc: 0.8521\n",
            "Epoch 30/100\n",
            "623/623 [==============================] - 5s 9ms/step - loss: 0.3385 - acc: 0.8431 - val_loss: 0.3251 - val_acc: 0.8525\n",
            "Epoch 31/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3450 - acc: 0.8390 - val_loss: 0.3252 - val_acc: 0.8527\n",
            "Epoch 32/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3382 - acc: 0.8427 - val_loss: 0.3244 - val_acc: 0.8515\n",
            "Epoch 33/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3401 - acc: 0.8414 - val_loss: 0.3241 - val_acc: 0.8519\n",
            "Epoch 34/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3378 - acc: 0.8426 - val_loss: 0.3236 - val_acc: 0.8525\n",
            "Epoch 35/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3363 - acc: 0.8442 - val_loss: 0.3232 - val_acc: 0.8521\n",
            "Epoch 36/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3352 - acc: 0.8455 - val_loss: 0.3227 - val_acc: 0.8529\n",
            "Epoch 37/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3330 - acc: 0.8454 - val_loss: 0.3226 - val_acc: 0.8523\n",
            "Epoch 38/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3330 - acc: 0.8446 - val_loss: 0.3224 - val_acc: 0.8537\n",
            "Epoch 39/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3324 - acc: 0.8465 - val_loss: 0.3222 - val_acc: 0.8525\n",
            "Epoch 40/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3334 - acc: 0.8450 - val_loss: 0.3220 - val_acc: 0.8523\n",
            "Epoch 41/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3335 - acc: 0.8477 - val_loss: 0.3219 - val_acc: 0.8533\n",
            "Epoch 42/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3336 - acc: 0.8419 - val_loss: 0.3217 - val_acc: 0.8537\n",
            "Epoch 43/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3293 - acc: 0.8460 - val_loss: 0.3214 - val_acc: 0.8549\n",
            "Epoch 44/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3329 - acc: 0.8454 - val_loss: 0.3212 - val_acc: 0.8545\n",
            "Epoch 45/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3324 - acc: 0.8441 - val_loss: 0.3212 - val_acc: 0.8551\n",
            "Epoch 46/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3277 - acc: 0.8465 - val_loss: 0.3208 - val_acc: 0.8549\n",
            "Epoch 47/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3305 - acc: 0.8480 - val_loss: 0.3206 - val_acc: 0.8545\n",
            "Epoch 48/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3260 - acc: 0.8484 - val_loss: 0.3203 - val_acc: 0.8547\n",
            "Epoch 49/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3280 - acc: 0.8466 - val_loss: 0.3201 - val_acc: 0.8553\n",
            "Epoch 50/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3295 - acc: 0.8477 - val_loss: 0.3201 - val_acc: 0.8547\n",
            "Epoch 51/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3288 - acc: 0.8473 - val_loss: 0.3200 - val_acc: 0.8555\n",
            "Epoch 52/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3291 - acc: 0.8477 - val_loss: 0.3200 - val_acc: 0.8553\n",
            "Epoch 53/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3280 - acc: 0.8485 - val_loss: 0.3200 - val_acc: 0.8555\n",
            "Epoch 54/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3282 - acc: 0.8500 - val_loss: 0.3198 - val_acc: 0.8557\n",
            "Epoch 55/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3254 - acc: 0.8485 - val_loss: 0.3196 - val_acc: 0.8559\n",
            "Epoch 56/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3248 - acc: 0.8499 - val_loss: 0.3194 - val_acc: 0.8565\n",
            "Epoch 57/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3251 - acc: 0.8477 - val_loss: 0.3193 - val_acc: 0.8559\n",
            "Epoch 58/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3232 - acc: 0.8476 - val_loss: 0.3193 - val_acc: 0.8565\n",
            "Epoch 59/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3252 - acc: 0.8488 - val_loss: 0.3193 - val_acc: 0.8571\n",
            "Epoch 60/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3251 - acc: 0.8495 - val_loss: 0.3191 - val_acc: 0.8569\n",
            "Epoch 61/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3226 - acc: 0.8516 - val_loss: 0.3189 - val_acc: 0.8577\n",
            "Epoch 62/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3211 - acc: 0.8507 - val_loss: 0.3188 - val_acc: 0.8575\n",
            "Epoch 63/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3220 - acc: 0.8518 - val_loss: 0.3187 - val_acc: 0.8577\n",
            "Epoch 64/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3224 - acc: 0.8508 - val_loss: 0.3186 - val_acc: 0.8577\n",
            "Epoch 65/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3221 - acc: 0.8492 - val_loss: 0.3185 - val_acc: 0.8571\n",
            "Epoch 66/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3227 - acc: 0.8508 - val_loss: 0.3185 - val_acc: 0.8571\n",
            "Epoch 67/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3217 - acc: 0.8519 - val_loss: 0.3185 - val_acc: 0.8569\n",
            "Epoch 68/100\n",
            "623/623 [==============================] - 4s 7ms/step - loss: 0.3225 - acc: 0.8499 - val_loss: 0.3185 - val_acc: 0.8577\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n",
            "Best accuracy: 0.858\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "4.DNNHomeWork_DuongNguyen.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}