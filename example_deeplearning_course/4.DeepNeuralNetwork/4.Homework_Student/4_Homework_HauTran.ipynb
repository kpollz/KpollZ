{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.Homework_HauTran.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# I. Lý thuyết\n",
        "\n",
        "1) Tại sao các mô hình deep learning lại chiếm ưu thế hơn so với các mô hình machine learning truyền thống đối với dữ liệu lớn ?\n",
        "\n",
        "A. Do mô hình deep learning có thể được thiết kế với kích thước tùy ý nên có khả năng xấp xỉ mọi hàm số. Do đó nó có khả năng biểu diễn tốt và hoạt động hiệu quả trên dữ liệu lớn.\n",
        "\n",
        "B. Các mô hình machine learning thường bị overfitting đối với dữ liệu lớn ?\n",
        "\n",
        "C. Các mô hình deep learning có chi phí huấn luyện tốn kém hơn so với machine learning.\n",
        "\n",
        "D. Do kiến trúc của mô hình Machine Learning bao gồm nhiều layers xếp chồng.\n",
        "\n",
        "Đáp án: A\n",
        "\n",
        "\n",
        "2) Ý nghĩa của hàm loss function trong mạng neural network là gì ?\n",
        "\n",
        "\n",
        "B. Mục tiêu của quá trình huấn luyện là tối thiểu hóa hàm loss function bằng thuật toán gradient descent. Giá trị của hàm số này giúp đo lường mức độ khớp của dự báo từ mô hình trên dữ liệu huấn luyện.\n",
        "\n",
        "Đáp án: B\n",
        "\n",
        "\n",
        "3) Khi huấn luyện trên các bộ dữ liệu bigdata thì chúng ta nên sử dụng phương pháp nào ?\n",
        "\n",
        "\n",
        "C) Mini-batch gradient descent huấn luyện mô hình trên từng tập dữ liệu con có kích thước nhỏ hơn memory CPU/GPU.\n",
        "\n",
        "Đáp án: C\n",
        "\n",
        "\n",
        "4) Quá trình feed forward và backpropagation thực hiện những gì ?\n",
        "\n",
        "A) feed forward tính toán output và loss function, backpropagation tính đạo hàm trên từng layer và cập nhật trọng số.\n",
        "\n",
        "Đáp án: A\n",
        "\n",
        "5) Tác dụng của batch normalization là gì ?\n",
        "\n",
        "\n",
        "\n",
        "D) Giảm thiểu ảnh hưởng của input distribution shift nhằm giúp huấn luyện loss function nhanh và ổn định hơn.\n",
        "\n",
        "Đáp án: D"
      ],
      "metadata": {
        "id": "PDDUcokDmyX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Thực hành\n",
        "\n",
        "Xuất phát từ mô hình tốt nhất của bạn xây dựng được đối với bài toán phân loại income classification tại bài trước. Bạn hãy thực hiện một số thử nghiệm sau:\n",
        "\n",
        "6) Thay đổi hàm loss function, batch size và optimizer.\n",
        "\n",
        "7) Thử nghiệm thêm các layers mà bạn đã học được trong bài này vào kiến trúc của mình.\n",
        "\n",
        "8) Thay đổi các khởi tạo trọng số theo các phân phối khác nhau và đánh giá độ chính xác của kết quả huấn luyện.\n",
        "\n",
        "9) Thiết lập không gian search và tự động hóa tìm kiếm kiến trúc tốt nhất trên optuna.\n",
        "\n",
        "10) Deploy model sử dụng flask ap. Tham khảo [Flaskapp tutorial](https://drive.google.com/file/d/1AZNtzrmnhJ-OBgijWoaAqXbPhJ6xL0Po/view?usp=sharing)."
      ],
      "metadata": {
        "id": "zYoLCUgcm1Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6, 7, 8"
      ],
      "metadata": {
        "id": "0KZPHC0dMwZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/DL/4.Homework_HauTran\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LTvGJ3zLqYM",
        "outputId": "af62461b-7935-4663-cb4c-fc150b373e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "def seed_all(seed):\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "seed = 33\n",
        "seed_all(seed)"
      ],
      "metadata": {
        "id": "H2KLZTQNM1PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "income_data = pd.read_csv('data/train.csv').dropna()"
      ],
      "metadata": {
        "id": "QFBriR7rM5Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize string\n",
        "def string_normalize(s):\n",
        "  s = str(s).lower().strip()\n",
        "  s = re.sub(' +', ' ', s)\n",
        "  return s\n",
        "#encode category and object colums\n",
        "def process(df):\n",
        "  for col in df.columns:\n",
        "    if df[col].dtype.name =='object' or df[col].dtype.name =='category':\n",
        "      df[col] = df[col].apply(string_normalize).astype(\"category\")\n",
        "  return df\n",
        "income_data = process(income_data.copy())"
      ],
      "metadata": {
        "id": "zE10qroJM5j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDs = income_data.pop('ID')\n",
        "label = income_data.pop('target_income')\n",
        "income_data_one_hot = pd.get_dummies(income_data)"
      ],
      "metadata": {
        "id": "tbBs-S2pM5m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = income_data_one_hot.columns.tolist()\n",
        "label = label.values"
      ],
      "metadata": {
        "id": "ndh7zm78M5pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(income_data_one_hot[features].values,\n",
        "                                                    label,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 0)\n",
        "print ('Training Set: %d, Test Set: %d \\n' % (len(x_train), len(x_test)))\n",
        "\n",
        "#normalize\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LOXRXr9M_b0",
        "outputId": "b9ccf219-3307-41b9-eb35-75613728b752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: 20000, Test Set: 5000 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as td\n",
        "\n",
        "# Set random seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCeTEd__M_e7",
        "outputId": "4197093f-0f17-4af4-bd8d-cb150ca392e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported - ready to use PyTorch 1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the data for PyTorch\n",
        "# Create a dataset and loader for the training data and labels\n",
        "train_x = torch.Tensor(x_train).float()\n",
        "train_y = torch.Tensor(y_train).long()\n",
        "train_ds = td.TensorDataset(train_x,train_y)\n",
        "train_loader = td.DataLoader(train_ds, batch_size=64,\n",
        "    shuffle=True)\n",
        "\n",
        "# Create a dataset and loader for the test data and labels\n",
        "test_x = torch.Tensor(x_test).float()\n",
        "test_y = torch.Tensor(y_test).long()\n",
        "test_ds = td.TensorDataset(test_x,test_y)\n",
        "test_loader = td.DataLoader(test_ds, batch_size=64,\n",
        "    shuffle=False)\n",
        "print('Ready to load data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irCCSG0dM_hw",
        "outputId": "128f18cd-f513-4ff2-f354-9b2f895dbced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to load data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a neural network\n",
        "# Number of hidden layer nodes\n",
        "hl = 10\n",
        "\n",
        "# Define the neural network\n",
        "class Income(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Income, self).__init__()\n",
        "        self.fc1 = nn.Linear(len(features), hl)\n",
        "        self.fc2 = nn.Linear(hl, hl)\n",
        "        self.fc3 = nn.Linear(hl, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.nn.Softmax()(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Create a model instance from the network\n",
        "model = Income()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX9igvjyNHH6",
        "outputId": "046680ea-48c5-406f-80a6-fcf56ba43231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Income(\n",
            "  (fc1): Linear(in_features=108, out_features=10, bias=True)\n",
            "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def train(model, data_loader, optimizer):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch, tensor in enumerate(data_loader):\n",
        "        data, target = tensor\n",
        "\n",
        "        #feedforward: calculate y_pred and loss function\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "\n",
        "        loss = loss_criteria(out, target)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # backpropagate: compute gradient descent and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    #Return average loss\n",
        "    avg_loss = train_loss / (batch+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss\n",
        "           \n",
        "            \n",
        "def test(model, data_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for batch, tensor in enumerate(data_loader):\n",
        "            batch_count += 1\n",
        "            data, target = tensor\n",
        "\n",
        "            # Get the predictions\n",
        "            out = model(data)\n",
        "\n",
        "            # calculate the loss\n",
        "            test_loss += loss_criteria(out, target).item()\n",
        "\n",
        "            # Calculate the accuracy\n",
        "            predicted = torch.tensor(out.data[:, 1]>=0.5).float()\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_loss, correct, len(data_loader.dataset),\n",
        "        100. * correct / len(data_loader.dataset)))\n",
        "    \n",
        "    # return average loss for the epoch\n",
        "    return avg_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "DeMcyoDTNKqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss funtion\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.ASGD(model.parameters(), lr=learning_rate)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# We'll track metrics for each epoch in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over 50 epochs\n",
        "epochs = 30\n",
        "for epoch in range(1, epochs + 1):\n",
        "\n",
        "    # print the epoch number\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    \n",
        "    # Feed training data into the model to optimize the weights\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    \n",
        "    # Feed the test data into the model to check its performance\n",
        "    test_loss = test(model, test_loader)\n",
        "    \n",
        "    # Log the metrics for this epoch\n",
        "    epoch_nums.append(epoch)\n",
        "    training_loss.append(train_loss)\n",
        "    validation_loss.append(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmwKyQUrPnO5",
        "outputId": "ca6ecb19-2a22-4898-d048-ff7316b313c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: Average loss: 0.465607\n",
            "Validation set: Average loss: 0.470846, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: Average loss: 0.465537\n",
            "Validation set: Average loss: 0.470858, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 3\n",
            "Training set: Average loss: 0.465715\n",
            "Validation set: Average loss: 0.470867, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 4\n",
            "Training set: Average loss: 0.465632\n",
            "Validation set: Average loss: 0.470874, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 5\n",
            "Training set: Average loss: 0.465570\n",
            "Validation set: Average loss: 0.470873, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 6\n",
            "Training set: Average loss: 0.465718\n",
            "Validation set: Average loss: 0.470873, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 7\n",
            "Training set: Average loss: 0.465565\n",
            "Validation set: Average loss: 0.470873, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 8\n",
            "Training set: Average loss: 0.465550\n",
            "Validation set: Average loss: 0.470872, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 9\n",
            "Training set: Average loss: 0.465620\n",
            "Validation set: Average loss: 0.470872, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 10\n",
            "Training set: Average loss: 0.465469\n",
            "Validation set: Average loss: 0.470871, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 11\n",
            "Training set: Average loss: 0.465469\n",
            "Validation set: Average loss: 0.470871, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 12\n",
            "Training set: Average loss: 0.465418\n",
            "Validation set: Average loss: 0.470871, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 13\n",
            "Training set: Average loss: 0.465367\n",
            "Validation set: Average loss: 0.470870, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 14\n",
            "Training set: Average loss: 0.465715\n",
            "Validation set: Average loss: 0.470870, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 15\n",
            "Training set: Average loss: 0.465614\n",
            "Validation set: Average loss: 0.470869, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 16\n",
            "Training set: Average loss: 0.465460\n",
            "Validation set: Average loss: 0.470869, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 17\n",
            "Training set: Average loss: 0.465353\n",
            "Validation set: Average loss: 0.470869, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 18\n",
            "Training set: Average loss: 0.465738\n",
            "Validation set: Average loss: 0.470869, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 19\n",
            "Training set: Average loss: 0.465577\n",
            "Validation set: Average loss: 0.470869, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 20\n",
            "Training set: Average loss: 0.465522\n",
            "Validation set: Average loss: 0.470871, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 21\n",
            "Training set: Average loss: 0.465517\n",
            "Validation set: Average loss: 0.470874, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 22\n",
            "Training set: Average loss: 0.465487\n",
            "Validation set: Average loss: 0.470879, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 23\n",
            "Training set: Average loss: 0.465372\n",
            "Validation set: Average loss: 0.470879, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 24\n",
            "Training set: Average loss: 0.465472\n",
            "Validation set: Average loss: 0.470878, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 25\n",
            "Training set: Average loss: 0.465572\n",
            "Validation set: Average loss: 0.470877, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 26\n",
            "Training set: Average loss: 0.465471\n",
            "Validation set: Average loss: 0.470877, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 27\n",
            "Training set: Average loss: 0.465471\n",
            "Validation set: Average loss: 0.470876, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 28\n",
            "Training set: Average loss: 0.465571\n",
            "Validation set: Average loss: 0.470875, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 29\n",
            "Training set: Average loss: 0.465371\n",
            "Validation set: Average loss: 0.470874, Accuracy: 4210/5000 (84%)\n",
            "\n",
            "Epoch: 30\n",
            "Training set: Average loss: 0.465421\n",
            "Validation set: Average loss: 0.470874, Accuracy: 4210/5000 (84%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "#metric\n",
        "def get_metrics(y_test, y_pred):\n",
        "    print('ACCURACY_SCORE: ', round(accuracy_score(y_test, y_pred), 4))\n",
        "    print('F1_SCORE: ', round(f1_score(y_test, y_pred, average='macro'), 4))\n",
        "    print('CONFUSION_MATRIX:\\n', confusion_matrix(y_test, y_pred),'\\n')\n",
        "    print(classification_report(y_test, y_pred, digits=4), '\\n')"
      ],
      "metadata": {
        "id": "PtTGmtH4Vfy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "def _evaluate(model, x_test):\n",
        "  # Set the model to evaluate mode\n",
        "  model.eval()\n",
        "\n",
        "  # Get predictions for the test data\n",
        "  x = torch.Tensor(x_test).float()\n",
        "  _, predictions = torch.max(model(x).data, 1)\n",
        "  get_metrics(y_test, predictions)\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  income_classes=['0','1']\n",
        "  cm = confusion_matrix(y_test, predictions)\n",
        "  plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(2)\n",
        "  plt.xticks(tick_marks, income_classes, rotation=85)\n",
        "  plt.yticks(tick_marks, income_classes)\n",
        "  plt.xlabel(\"Predicted\")\n",
        "  plt.ylabel(\"Actual\")\n",
        "  plt.show()\n",
        "\n",
        "_evaluate(model, x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "1FUuNRTGVWP5",
        "outputId": "e68d49a0-5ac6-460b-da0e-aac13da86067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY_SCORE:  0.842\n",
            "F1_SCORE:  0.7642\n",
            "CONFUSION_MATRIX:\n",
            " [[3541  270]\n",
            " [ 520  669]] \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8720    0.9292    0.8996      3811\n",
            "           1     0.7125    0.5627    0.6288      1189\n",
            "\n",
            "    accuracy                         0.8420      5000\n",
            "   macro avg     0.7922    0.7459    0.7642      5000\n",
            "weighted avg     0.8340    0.8420    0.8352      5000\n",
            " \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) Optuna"
      ],
      "metadata": {
        "id": "8uVcIYJpV1Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "oChUc4jYWYuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "eL4Oe7jMWn1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(trial):\n",
        "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 4, 8)\n",
        "    layers = []\n",
        "\n",
        "    in_features = len(features)\n",
        "    for i in range(n_layers):\n",
        "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 64, 256)\n",
        "        layers.append(nn.Linear(in_features, out_features))\n",
        "        layers.append(nn.ReLU())\n",
        "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.3, 0.5)\n",
        "        layers.append(nn.Dropout(p))\n",
        "\n",
        "        in_features = out_features\n",
        "    layers.append(nn.Linear(in_features, 2))\n",
        "    layers.append(nn.Softmax())\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "hdF3fiBIW9JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "BATCHSIZE = 64\n",
        "EPOCHS = 30\n",
        "LOG_INTERVAL = 10\n",
        "N_TRAIN_EXAMPLES = BATCHSIZE * 300\n",
        "N_VALID_EXAMPLES = BATCHSIZE * 100\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Generate the model.\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "\n",
        "    # Generate the optimizers.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    loss_criteria = nn.CrossEntropyLoss()\n",
        "    # Training of the model.\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Limiting training data for faster epochs.\n",
        "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
        "                break\n",
        "\n",
        "            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            # loss = F.nll_loss(output, target)\n",
        "            loss = loss_criteria(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation of the model.\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(test_loader):\n",
        "                # Limiting validation data.\n",
        "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
        "                    break\n",
        "                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
        "                output = model(data)\n",
        "                # Get the index of the max log-probability.\n",
        "                pred = torch.tensor(output.data[:, 1]>=0.5).float()\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        accuracy = correct / min(len(test_loader.dataset), N_VALID_EXAMPLES)\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "yl_bz-8RXcLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100, timeout=600)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxSWpWoNYJlV",
        "outputId": "1cc286f5-db0d-4f71-efd3-a458a492f0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-02-19 19:09:42,583]\u001b[0m A new study created in memory with name: no-name-dbcfa2eb-e298-4fb1-862b-884a422b7b5f\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\u001b[32m[I 2022-02-19 19:11:09,255]\u001b[0m Trial 0 finished with value: 0.7622 and parameters: {'n_layers': 7, 'n_units_l0': 179, 'dropout_l0': 0.3860837708410817, 'n_units_l1': 199, 'dropout_l1': 0.4524483084689435, 'n_units_l2': 84, 'dropout_l2': 0.4273248524957388, 'n_units_l3': 80, 'dropout_l3': 0.34167195110219223, 'n_units_l4': 123, 'dropout_l4': 0.3876112789207476, 'n_units_l5': 199, 'dropout_l5': 0.44919861634408925, 'n_units_l6': 252, 'dropout_l6': 0.4238426548816232, 'optimizer': 'Adam', 'lr': 0.008638394010274143}. Best is trial 0 with value: 0.7622.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\u001b[32m[I 2022-02-19 19:12:26,033]\u001b[0m Trial 1 finished with value: 0.7622 and parameters: {'n_layers': 7, 'n_units_l0': 167, 'dropout_l0': 0.39133583790334886, 'n_units_l1': 222, 'dropout_l1': 0.305817737831781, 'n_units_l2': 119, 'dropout_l2': 0.43785980530698554, 'n_units_l3': 228, 'dropout_l3': 0.4396427040053408, 'n_units_l4': 100, 'dropout_l4': 0.39377695354631376, 'n_units_l5': 76, 'dropout_l5': 0.4485722572754802, 'n_units_l6': 193, 'dropout_l6': 0.39175829215452185, 'optimizer': 'RMSprop', 'lr': 0.0040882032242168095}. Best is trial 0 with value: 0.7622.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\u001b[32m[I 2022-02-19 19:13:18,886]\u001b[0m Trial 2 finished with value: 0.7622 and parameters: {'n_layers': 5, 'n_units_l0': 204, 'dropout_l0': 0.3897920247837273, 'n_units_l1': 123, 'dropout_l1': 0.39823184505262377, 'n_units_l2': 146, 'dropout_l2': 0.3552397785012145, 'n_units_l3': 78, 'dropout_l3': 0.41831963649268206, 'n_units_l4': 175, 'dropout_l4': 0.3648403861415044, 'optimizer': 'RMSprop', 'lr': 0.049802605465175745}. Best is trial 0 with value: 0.7622.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\u001b[32m[I 2022-02-19 19:14:42,072]\u001b[0m Trial 3 finished with value: 0.8356 and parameters: {'n_layers': 7, 'n_units_l0': 224, 'dropout_l0': 0.41144740307261657, 'n_units_l1': 99, 'dropout_l1': 0.4809833584764789, 'n_units_l2': 156, 'dropout_l2': 0.40920661684146353, 'n_units_l3': 130, 'dropout_l3': 0.3571697825364535, 'n_units_l4': 232, 'dropout_l4': 0.4542428858505263, 'n_units_l5': 216, 'dropout_l5': 0.324058419657756, 'n_units_l6': 143, 'dropout_l6': 0.40489630051726055, 'optimizer': 'Adam', 'lr': 0.00274817399378836}. Best is trial 3 with value: 0.8356.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\u001b[32m[I 2022-02-19 19:17:10,351]\u001b[0m Trial 4 finished with value: 0.7622 and parameters: {'n_layers': 7, 'n_units_l0': 227, 'dropout_l0': 0.44498918594698966, 'n_units_l1': 163, 'dropout_l1': 0.420758158572931, 'n_units_l2': 124, 'dropout_l2': 0.3079966691273271, 'n_units_l3': 93, 'dropout_l3': 0.4567634614673588, 'n_units_l4': 232, 'dropout_l4': 0.33016221287824016, 'n_units_l5': 231, 'dropout_l5': 0.32526444411542527, 'n_units_l6': 247, 'dropout_l6': 0.39466685771211923, 'optimizer': 'RMSprop', 'lr': 0.0018106694776897538}. Best is trial 3 with value: 0.8356.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\u001b[32m[I 2022-02-19 19:17:49,226]\u001b[0m Trial 5 finished with value: 0.8544 and parameters: {'n_layers': 7, 'n_units_l0': 110, 'dropout_l0': 0.420491206579514, 'n_units_l1': 92, 'dropout_l1': 0.3259758480108703, 'n_units_l2': 111, 'dropout_l2': 0.36531894014539423, 'n_units_l3': 138, 'dropout_l3': 0.3658193217106271, 'n_units_l4': 116, 'dropout_l4': 0.36328172168561645, 'n_units_l5': 206, 'dropout_l5': 0.4328625224261037, 'n_units_l6': 170, 'dropout_l6': 0.35691963885936673, 'optimizer': 'RMSprop', 'lr': 0.00013002409199182837}. Best is trial 5 with value: 0.8544.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\u001b[32m[I 2022-02-19 19:20:06,019]\u001b[0m Trial 6 finished with value: 0.7622 and parameters: {'n_layers': 8, 'n_units_l0': 66, 'dropout_l0': 0.3740993441249396, 'n_units_l1': 173, 'dropout_l1': 0.4473371685148585, 'n_units_l2': 183, 'dropout_l2': 0.4282614921411645, 'n_units_l3': 152, 'dropout_l3': 0.47174262978395326, 'n_units_l4': 142, 'dropout_l4': 0.41098475760864517, 'n_units_l5': 110, 'dropout_l5': 0.4758149994524806, 'n_units_l6': 209, 'dropout_l6': 0.416552252734437, 'n_units_l7': 247, 'dropout_l7': 0.38329407242125924, 'optimizer': 'Adam', 'lr': 0.0021503691255244503}. Best is trial 5 with value: 0.8544.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  7\n",
            "  Number of pruned trials:  0\n",
            "  Number of complete trials:  7\n",
            "Best trial:\n",
            "  Value:  0.8544\n",
            "  Params: \n",
            "    n_layers: 7\n",
            "    n_units_l0: 110\n",
            "    dropout_l0: 0.420491206579514\n",
            "    n_units_l1: 92\n",
            "    dropout_l1: 0.3259758480108703\n",
            "    n_units_l2: 111\n",
            "    dropout_l2: 0.36531894014539423\n",
            "    n_units_l3: 138\n",
            "    dropout_l3: 0.3658193217106271\n",
            "    n_units_l4: 116\n",
            "    dropout_l4: 0.36328172168561645\n",
            "    n_units_l5: 206\n",
            "    dropout_l5: 0.4328625224261037\n",
            "    n_units_l6: 170\n",
            "    dropout_l6: 0.35691963885936673\n",
            "    optimizer: RMSprop\n",
            "    lr: 0.00013002409199182837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(trial)"
      ],
      "metadata": {
        "id": "uKV5DYi7ZDRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=trial.params['lr'])\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# We'll track metrics for each epoch in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over 30 epochs\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "ycxKgfW6bkwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, optimizer):\n",
        "    '''\n",
        "    Train model through data loader and optimizer\n",
        "    Args:\n",
        "      model: model to train\n",
        "      data_loader: data loader to manage batch loading\n",
        "      optimizer: control update gradient descent\n",
        "    '''\n",
        "    # enable train mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch, tensor in enumerate(data_loader):\n",
        "        data, target = tensor\n",
        "        # reset optimizer into zero\n",
        "        optimizer.zero_grad()\n",
        "        # feed forward to compute output and loss\n",
        "        out = model(data)\n",
        "        loss = loss_criteria(out, target)\n",
        "        # accumulate loss\n",
        "        train_loss += loss.item()\n",
        "        # compute gradient descent\n",
        "        loss.backward()\n",
        "        # update into weight\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = train_loss / (batch+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss\n",
        "            "
      ],
      "metadata": {
        "id": "1eW9Yse7cANc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training through epoch\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # print the epoch number\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    \n",
        "    # Feed training data into the model to optimize the weights\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    \n",
        "    # Feed the test data into the model to check its performance\n",
        "    test_loss = test(model, test_loader)\n",
        "    \n",
        "    # Log the metrics for this epoch\n",
        "    epoch_nums.append(epoch)\n",
        "    training_loss.append(train_loss)\n",
        "    validation_loss.append(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebkrEMMdcDbo",
        "outputId": "7faf84f7-b4bc-4323-9ced-88d99aa32e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: Average loss: 0.519137\n",
            "Validation set: Average loss: 0.480868, Accuracy: 4162/5000 (83%)\n",
            "\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: Average loss: 0.481585\n",
            "Validation set: Average loss: 0.467636, Accuracy: 4205/5000 (84%)\n",
            "\n",
            "Epoch: 3\n",
            "Training set: Average loss: 0.472813\n",
            "Validation set: Average loss: 0.461035, Accuracy: 4227/5000 (85%)\n",
            "\n",
            "Epoch: 4\n",
            "Training set: Average loss: 0.468209\n",
            "Validation set: Average loss: 0.458768, Accuracy: 4247/5000 (85%)\n",
            "\n",
            "Epoch: 5\n",
            "Training set: Average loss: 0.466156\n",
            "Validation set: Average loss: 0.458483, Accuracy: 4238/5000 (85%)\n",
            "\n",
            "Epoch: 6\n",
            "Training set: Average loss: 0.464852\n",
            "Validation set: Average loss: 0.456665, Accuracy: 4239/5000 (85%)\n",
            "\n",
            "Epoch: 7\n",
            "Training set: Average loss: 0.462019\n",
            "Validation set: Average loss: 0.455723, Accuracy: 4243/5000 (85%)\n",
            "\n",
            "Epoch: 8\n",
            "Training set: Average loss: 0.460924\n",
            "Validation set: Average loss: 0.455184, Accuracy: 4247/5000 (85%)\n",
            "\n",
            "Epoch: 9\n",
            "Training set: Average loss: 0.460485\n",
            "Validation set: Average loss: 0.454950, Accuracy: 4253/5000 (85%)\n",
            "\n",
            "Epoch: 10\n",
            "Training set: Average loss: 0.457439\n",
            "Validation set: Average loss: 0.454174, Accuracy: 4255/5000 (85%)\n",
            "\n",
            "Epoch: 11\n",
            "Training set: Average loss: 0.456312\n",
            "Validation set: Average loss: 0.453934, Accuracy: 4250/5000 (85%)\n",
            "\n",
            "Epoch: 12\n",
            "Training set: Average loss: 0.454586\n",
            "Validation set: Average loss: 0.452988, Accuracy: 4265/5000 (85%)\n",
            "\n",
            "Epoch: 13\n",
            "Training set: Average loss: 0.455736\n",
            "Validation set: Average loss: 0.453460, Accuracy: 4262/5000 (85%)\n",
            "\n",
            "Epoch: 14\n",
            "Training set: Average loss: 0.454536\n",
            "Validation set: Average loss: 0.452405, Accuracy: 4257/5000 (85%)\n",
            "\n",
            "Epoch: 15\n",
            "Training set: Average loss: 0.452632\n",
            "Validation set: Average loss: 0.453759, Accuracy: 4259/5000 (85%)\n",
            "\n",
            "Epoch: 16\n",
            "Training set: Average loss: 0.453621\n",
            "Validation set: Average loss: 0.452110, Accuracy: 4265/5000 (85%)\n",
            "\n",
            "Epoch: 17\n",
            "Training set: Average loss: 0.451299\n",
            "Validation set: Average loss: 0.451919, Accuracy: 4265/5000 (85%)\n",
            "\n",
            "Epoch: 18\n",
            "Training set: Average loss: 0.450323\n",
            "Validation set: Average loss: 0.452218, Accuracy: 4268/5000 (85%)\n",
            "\n",
            "Epoch: 19\n",
            "Training set: Average loss: 0.450207\n",
            "Validation set: Average loss: 0.450956, Accuracy: 4280/5000 (86%)\n",
            "\n",
            "Epoch: 20\n",
            "Training set: Average loss: 0.449821\n",
            "Validation set: Average loss: 0.451402, Accuracy: 4274/5000 (85%)\n",
            "\n",
            "Epoch: 21\n",
            "Training set: Average loss: 0.449290\n",
            "Validation set: Average loss: 0.451225, Accuracy: 4274/5000 (85%)\n",
            "\n",
            "Epoch: 22\n",
            "Training set: Average loss: 0.450329\n",
            "Validation set: Average loss: 0.450278, Accuracy: 4272/5000 (85%)\n",
            "\n",
            "Epoch: 23\n",
            "Training set: Average loss: 0.448681\n",
            "Validation set: Average loss: 0.450131, Accuracy: 4273/5000 (85%)\n",
            "\n",
            "Epoch: 24\n",
            "Training set: Average loss: 0.448314\n",
            "Validation set: Average loss: 0.450201, Accuracy: 4271/5000 (85%)\n",
            "\n",
            "Epoch: 25\n",
            "Training set: Average loss: 0.447902\n",
            "Validation set: Average loss: 0.449510, Accuracy: 4267/5000 (85%)\n",
            "\n",
            "Epoch: 26\n",
            "Training set: Average loss: 0.445395\n",
            "Validation set: Average loss: 0.450055, Accuracy: 4264/5000 (85%)\n",
            "\n",
            "Epoch: 27\n",
            "Training set: Average loss: 0.447707\n",
            "Validation set: Average loss: 0.449839, Accuracy: 4275/5000 (86%)\n",
            "\n",
            "Epoch: 28\n",
            "Training set: Average loss: 0.446667\n",
            "Validation set: Average loss: 0.449318, Accuracy: 4269/5000 (85%)\n",
            "\n",
            "Epoch: 29\n",
            "Training set: Average loss: 0.446573\n",
            "Validation set: Average loss: 0.448997, Accuracy: 4277/5000 (86%)\n",
            "\n",
            "Epoch: 30\n",
            "Training set: Average loss: 0.446081\n",
            "Validation set: Average loss: 0.449582, Accuracy: 4270/5000 (85%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model/income.pth')"
      ],
      "metadata": {
        "id": "vq3UgmwodnDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "def _evaluate(model, x_test):\n",
        "  model.eval()\n",
        "  x = torch.Tensor(x_test).float()\n",
        "  _, predictions = torch.max(model(x).data, 1)\n",
        "  predictions = torch.tensor(predictions)\n",
        "  print('Evaluation on test dataset')\n",
        "  get_metrics(y_test, predictions)\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  classes=['0','1']\n",
        "  cm = confusion_matrix(y_test, predictions.numpy())\n",
        "  plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(2)\n",
        "  plt.xticks(tick_marks, classes, rotation=85)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "  plt.xlabel(\"Predicted\")\n",
        "  plt.ylabel(\"Actual\")\n",
        "  plt.show()\n",
        "_evaluate(model, x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "DH2DGP7Jcu-r",
        "outputId": "a111761a-5e98-4082-e227-39481a5ac57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test dataset\n",
            "ACCURACY_SCORE:  0.854\n",
            "F1_SCORE:  0.7908\n",
            "CONFUSION_MATRIX:\n",
            " [[3509  302]\n",
            " [ 428  761]] \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8913    0.9208    0.9058      3811\n",
            "           1     0.7159    0.6400    0.6758      1189\n",
            "\n",
            "    accuracy                         0.8540      5000\n",
            "   macro avg     0.8036    0.7804    0.7908      5000\n",
            "weighted avg     0.8496    0.8540    0.8511      5000\n",
            " \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: Optuna find a neural architecture archiving up to 85.4% accuracy on train dataset and was higher than the original model with only 84.2%."
      ],
      "metadata": {
        "id": "yZ6qW_9fdExs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10) Deploy model sử dụng flask"
      ],
      "metadata": {
        "id": "6wg52gA9dQbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income_data.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47BlpnnadXWA",
        "outputId": "9b6a72c9-5499-4e77-f854-3166055b9abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                       int64\n",
              "work_type              category\n",
              "final_weight              int64\n",
              "education              category\n",
              "total_education_yrs       int64\n",
              "marital_state          category\n",
              "job                    category\n",
              "status                 category\n",
              "ethnicity              category\n",
              "sex                    category\n",
              "capital_gain              int64\n",
              "capital_loss              int64\n",
              "hrs_per_week              int64\n",
              "nationality            category\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding numpy to json\n",
        "import json\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    '''\n",
        "    Encoding numpy into json\n",
        "    '''\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        if isinstance(obj, np.int32):\n",
        "            return int(obj)\n",
        "        if isinstance(obj, np.int64):\n",
        "            return int(obj)\n",
        "        if isinstance(obj, np.float32):\n",
        "            return float(obj)\n",
        "        if isinstance(obj, np.float64):\n",
        "            return float(obj)\n",
        "        return json.JSONEncoder.default(self, obj)"
      ],
      "metadata": {
        "id": "WNSmRblXgy2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from flask import Flask, request\n",
        "import flask\n",
        "import json\n",
        "\n",
        "# Khởi tạo model.\n",
        "global model\n",
        "model = None\n",
        "# Khởi tạo flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "#normalize string\n",
        "def string_normalize(s):\n",
        "  s = str(s).lower().strip()\n",
        "  s = re.sub(' +', ' ', s)\n",
        "  return s\n",
        "\n",
        "# Khai báo route cho API\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "# Khai báo hàm xử lý dữ liệu.\n",
        "def _predict():\n",
        "  data = {\"success\": False}\n",
        "  request_body = request.json()\n",
        "  if request_body:\n",
        "    X_input =[]\n",
        "    for i in income_data.columns.tolist():\n",
        "      #Thêm trường có dữ liệu\n",
        "      try:\n",
        "        X_input.append(string_normalize(request_body[i]))\n",
        "      # Không có dữ liệu thì thay bằng giá trị 0\n",
        "      except:\n",
        "        X_input.append('0')\n",
        "    # Convert sang numpy array input\n",
        "    X_input = torch.nn.functional.one_hot(X_input)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # Get the predictions\n",
        "      out = model(X_input)\n",
        "\n",
        "      # Calculate the accuracy\n",
        "      predicted = torch.tensor(out.data[:, 1]>=0.5).float()\n",
        "    # Truyền vào data form response\n",
        "    data[\"probability\"] = predicted\n",
        "    data[\"success\"] = True\n",
        "    return json.dumps(data, ensure_ascii=False, cls=NumpyEncoder)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print(\"App run!\")\n",
        "  # Load model\n",
        "  model = torch.load('model/income.pth')\n",
        "  app.run(debug=False, host='localhost', threaded=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjhnYSG-hDQa",
        "outputId": "282367a0-b8ea-406f-81db-87c6f9f46e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "App run!\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://localhost:5000/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    }
  ]
}