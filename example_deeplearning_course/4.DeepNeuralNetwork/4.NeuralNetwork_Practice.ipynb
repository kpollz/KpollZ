{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4.NeuralNetwork_Practice.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#9 Từ bộ dữ liệu Income-Classification hãy chuẩn hóa dữ liệu và phân chia tập train/test theo tỷ lệ 80/20.\n","from google.colab import drive\n","import os\n","drive.mount(\"/content/gdrive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMJTCBJWlsoA","outputId":"9ca8f36d-482b-4eb3-b3e8-94cd882c9d28","executionInfo":{"status":"ok","timestamp":1644730315146,"user_tz":-420,"elapsed":20193,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["os.chdir(\"gdrive/MyDrive/Colab Notebooks/DeepLearning2/4.DeepNeuralNetwork\")"],"metadata":{"id":"iIuu_mUouj4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import random\n","import pandas as pd\n","import re\n","import torch.nn as nn\n","import torch\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n","def seed_all(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","seed = 42 \n","seed_all(seed)"],"metadata":{"id":"o-JovLXCl0Qy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#metric\n","def get_metrics(y_test, y_pred):\n","    print('ACCURACY_SCORE: ', round(accuracy_score(y_test, y_pred), 4))\n","    print('F1_SCORE: ', round(f1_score(y_test, y_pred, average='macro'), 4))\n","    print('CONFUSION_MATRIX:\\n', confusion_matrix(y_test, y_pred),'\\n')\n","    print(classification_report(y_test, y_pred, digits=4), '\\n')"],"metadata":{"id":"UwfYIt8TmLhi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_all = pd.read_csv('../data/income_classification/train.csv')"],"metadata":{"id":"EnPdF38mmNSi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize string\n","def str_normalize(s):\n","    # convert into lowercase and replace multiple spaces into single space\n","    s = str(s).strip().lower()\n","    s = re.sub(' +', \" \", s)\n","    return s\n","# Encode category and object columns \n","def process(df):        \n","  for col in df.columns:\n","      if df[col].dtype.name == \"object\" or df[col].dtype.name == \"category\":\n","          df[col] = df[col].apply(str_normalize).astype(\"category\")\n","  return df\n","df_all = process(df_all.copy())\n","IDs=df_all.pop('ID')\n","label = df_all.pop('target_income')\n","df_all_one_hot = pd.get_dummies(df_all)"],"metadata":{"id":"mu3MrPENmPER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = df_all_one_hot.columns.tolist()\n","label = label.values"],"metadata":{"id":"hNJLdU8OmQnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = df_all_one_hot.columns.tolist()\n","\n","x_train, x_test, y_train, y_test = train_test_split(df_all_one_hot[features].values, # input variable\n","                                                    label, # output variable\n","                                                    test_size=0.2, # test dataset proportion\n","                                                    # stratify=df_all['target_income'], # assign equal proportion of target label in train/test \n","                                                    random_state=0) # keep train/test split the same if run again. \n","print ('Training Set: %d, Test Set: %d \\n' % (len(x_train), len(x_test)))\n","#normalize\n","sc = StandardScaler()\n","x_train = sc.fit_transform(x_train)\n","x_test = sc.transform(x_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1E9S8YJHmTDO","outputId":"5b2d4d1a-e147-45bb-fae5-9705bfe1139d","executionInfo":{"status":"ok","timestamp":1644730354794,"user_tz":-420,"elapsed":333,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Set: 20000, Test Set: 5000 \n","\n"]}]},{"cell_type":"markdown","source":["# Optuna Neural Architecture Search\n","\n","[Auto-Tuning Hyperparameters with Optuna and PyTorch\n","](https://www.youtube.com/watch?v=P6NwZVl8ttc) is a good tutorial about search architecture.\n","\n","In the below, we are going to use optuna to search architecture of model that include:\n","\n","* Number of layers\n","* Number of hidden units of each layers\n","* the optimizer that applies to create the architecture.\n","* learning rate"],"metadata":{"id":"j2CdRxzv6QBT"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKel7UH47LAP","executionInfo":{"status":"ok","timestamp":1644730363785,"user_tz":-420,"elapsed":6411,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"9738978b-98d4-4bf2-a061-a0a1f1bd62e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n","Collecting alembic\n","  Downloading alembic-1.7.6-py3-none-any.whl (210 kB)\n","\u001b[K     |████████████████████████████████| 210 kB 55.7 MB/s \n","\u001b[?25hCollecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Collecting cliff\n","  Downloading cliff-3.10.0-py3-none-any.whl (80 kB)\n","\u001b[K     |████████████████████████████████| 80 kB 11.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Collecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.7)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.10.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Collecting Mako\n","  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n","\u001b[K     |████████████████████████████████| 113 kB 58.4 MB/s \n","\u001b[?25hCollecting autopage>=0.4.0\n","  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n","Collecting cmd2>=1.0.0\n","  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n","\u001b[K     |████████████████████████████████| 149 kB 73.3 MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.0.0)\n","Collecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=ccd1614948dce1038e6efad8f3c02ffe31a91cc997b9db18fa3511e63ef2c144\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.1.6 alembic-1.7.6 autopage-0.5.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 stevedore-3.5.0\n"]}]},{"cell_type":"code","source":["import optuna\n","from optuna.trial import TrialState\n","import torch.optim as optim\n","import torch.nn.functional as F"],"metadata":{"id":"td4N9KCc7I3H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the dataset and dataloader for train and test dataset"],"metadata":{"id":"6S3QDWfgmcXN"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.utils.data as td\n","\n","# Set random seed for reproducability\n","torch.manual_seed(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpeJvoY6kGHY","executionInfo":{"status":"ok","timestamp":1644730371857,"user_tz":-420,"elapsed":5,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"1469bd17-1298-4776-e36e-46d72ce9a1f5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f2a3d30e3d0>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Create a dataset and loader for the training data and labels\n","train_x = torch.Tensor(x_train).float()\n","# Reshape y target into [-1, 1] to fit with Binary Cross Entropy\n","train_y = torch.Tensor(y_train).view(-1, 1).float()\n","train_ds = td.TensorDataset(train_x,train_y)\n","train_loader = td.DataLoader(train_ds, batch_size=16,\n","    shuffle=True, num_workers=1)\n","\n","# Create a dataset and loader for the test data and labels\n","test_x = torch.Tensor(x_test).float()\n","test_y = torch.Tensor(y_test).view(-1, 1).float()\n","test_ds = td.TensorDataset(test_x,test_y)\n","test_loader = td.DataLoader(test_ds, batch_size=16,\n","    shuffle=False, num_workers=1)\n","print('Ready to load data')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eExyqaTF9KbN","executionInfo":{"status":"ok","timestamp":1644730376165,"user_tz":-420,"elapsed":4,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"fe9ae213-5438-4bda-be4f-f18b51a59bb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ready to load data\n"]}]},{"cell_type":"markdown","source":["Define trial for model in which we define network architecture in random way. That architecture will be trained in the next step."],"metadata":{"id":"aUdwzcsJma1m"}},{"cell_type":"code","source":["def define_model(trial):\n","    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n","    n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n","    layers = []\n","\n","    in_features = len(features)\n","    for i in range(n_layers):\n","        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n","        layers.append(nn.Linear(in_features, out_features))\n","        layers.append(nn.ReLU())\n","        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n","        layers.append(nn.Dropout(p))\n","\n","        in_features = out_features\n","    layers.append(nn.Linear(in_features, 1))\n","    layers.append(nn.Sigmoid())\n","\n","    return nn.Sequential(*layers)"],"metadata":{"id":"FXeMOnRj6UVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Objective function to train and evaluate the model. Finally, we select the best model corresponding with highest accuracy on evaluation."],"metadata":{"id":"fpahEOpOnAiw"}},{"cell_type":"code","source":["DEVICE = torch.device(\"cpu\")\n","BATCHSIZE = 16\n","EPOCHS = 30\n","LOG_INTERVAL = 10\n","N_TRAIN_EXAMPLES = BATCHSIZE * 300\n","N_VALID_EXAMPLES = BATCHSIZE * 100\n","\n","def objective(trial):\n","\n","    # Generate the model.\n","    model = define_model(trial).to(DEVICE)\n","\n","    # Generate the optimizers.\n","    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n","    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n","    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n","    loss_criteria = nn.BCELoss()\n","    # Training of the model.\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            # Limiting training data for faster epochs.\n","            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n","                break\n","\n","            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            output = model(data)\n","            # loss = F.nll_loss(output, target)\n","            loss = loss_criteria(output, target)\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Validation of the model.\n","        model.eval()\n","        correct = 0\n","        with torch.no_grad():\n","            for batch_idx, (data, target) in enumerate(test_loader):\n","                # Limiting validation data.\n","                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n","                    break\n","                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n","                output = model(data)\n","                # Get the index of the max log-probability.\n","                pred = torch.tensor(output.data>=0.5).float()\n","                correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","        accuracy = correct / min(len(test_loader.dataset), N_VALID_EXAMPLES)\n","\n","        trial.report(accuracy, epoch)\n","\n","        # Handle pruning based on the intermediate value.\n","        if trial.should_prune():\n","            raise optuna.exceptions.TrialPruned()\n","\n","    return accuracy"],"metadata":{"id":"sXf_PlLk77VS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=100, timeout=600)\n","\n","pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","print(\"Study statistics: \")\n","print(\"  Number of finished trials: \", len(study.trials))\n","print(\"  Number of pruned trials: \", len(pruned_trials))\n","print(\"  Number of complete trials: \", len(complete_trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHINEKPC9Ocd","executionInfo":{"status":"ok","timestamp":1644731001111,"user_tz":-420,"elapsed":613922,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"1cbefcd5-8c85-4940-966a-9121b96ba432"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-13 05:33:07,122]\u001b[0m A new study created in memory with name: no-name-5e768791-af51-47a9-b62f-af5c194c2498\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:33:40,370]\u001b[0m Trial 0 finished with value: 0.74625 and parameters: {'n_layers': 2, 'n_units_l0': 98, 'dropout_l0': 0.36241406579143887, 'n_units_l1': 84, 'dropout_l1': 0.2514160382033417, 'optimizer': 'RMSprop', 'lr': 0.03782293167472393}. Best is trial 0 with value: 0.74625.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:34:10,852]\u001b[0m Trial 1 finished with value: 0.851875 and parameters: {'n_layers': 4, 'n_units_l0': 18, 'dropout_l0': 0.3682719405889697, 'n_units_l1': 75, 'dropout_l1': 0.37731064488179267, 'n_units_l2': 39, 'dropout_l2': 0.3096423601104187, 'n_units_l3': 45, 'dropout_l3': 0.3338605876629569, 'optimizer': 'RMSprop', 'lr': 0.00014519798313389457}. Best is trial 1 with value: 0.851875.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:34:39,950]\u001b[0m Trial 2 finished with value: 0.8 and parameters: {'n_layers': 2, 'n_units_l0': 28, 'dropout_l0': 0.38004372284655563, 'n_units_l1': 103, 'dropout_l1': 0.3985437629820426, 'optimizer': 'RMSprop', 'lr': 0.030785537478349834}. Best is trial 1 with value: 0.851875.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:35:12,637]\u001b[0m Trial 3 finished with value: 0.853125 and parameters: {'n_layers': 4, 'n_units_l0': 118, 'dropout_l0': 0.48074596085541044, 'n_units_l1': 103, 'dropout_l1': 0.47228768652791314, 'n_units_l2': 79, 'dropout_l2': 0.29414039956311977, 'n_units_l3': 22, 'dropout_l3': 0.35129810912956094, 'optimizer': 'RMSprop', 'lr': 5.745104810219676e-05}. Best is trial 3 with value: 0.853125.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:35:42,242]\u001b[0m Trial 4 finished with value: 0.764375 and parameters: {'n_layers': 3, 'n_units_l0': 36, 'dropout_l0': 0.35901840747561464, 'n_units_l1': 126, 'dropout_l1': 0.4601487670536613, 'n_units_l2': 78, 'dropout_l2': 0.34084811237959434, 'optimizer': 'RMSprop', 'lr': 1.2632183250125678e-05}. Best is trial 3 with value: 0.853125.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:35:48,101]\u001b[0m Trial 5 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:36:16,493]\u001b[0m Trial 6 finished with value: 0.81875 and parameters: {'n_layers': 2, 'n_units_l0': 95, 'dropout_l0': 0.4744210219828284, 'n_units_l1': 124, 'dropout_l1': 0.2829750923887958, 'optimizer': 'RMSprop', 'lr': 0.013394790425080098}. Best is trial 3 with value: 0.853125.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:36:17,542]\u001b[0m Trial 7 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:36:49,572]\u001b[0m Trial 8 finished with value: 0.593125 and parameters: {'n_layers': 5, 'n_units_l0': 13, 'dropout_l0': 0.2707686437103676, 'n_units_l1': 100, 'dropout_l1': 0.37093205839578425, 'n_units_l2': 55, 'dropout_l2': 0.42888326112421693, 'n_units_l3': 40, 'dropout_l3': 0.34423170478545506, 'n_units_l4': 124, 'dropout_l4': 0.23659462698295497, 'optimizer': 'RMSprop', 'lr': 0.0047590306981072795}. Best is trial 3 with value: 0.853125.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:36:50,551]\u001b[0m Trial 9 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:37:27,625]\u001b[0m Trial 10 finished with value: 0.84875 and parameters: {'n_layers': 6, 'n_units_l0': 128, 'dropout_l0': 0.499340511452038, 'n_units_l1': 43, 'dropout_l1': 0.49954755282780205, 'n_units_l2': 121, 'dropout_l2': 0.20900516043267156, 'n_units_l3': 12, 'dropout_l3': 0.22837513729459397, 'n_units_l4': 100, 'dropout_l4': 0.4636740587697137, 'n_units_l5': 128, 'dropout_l5': 0.22166308921938024, 'optimizer': 'Adam', 'lr': 0.0006266777840338908}. Best is trial 3 with value: 0.853125.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:37:28,693]\u001b[0m Trial 11 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:37:29,672]\u001b[0m Trial 12 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:37:30,722]\u001b[0m Trial 13 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:37:59,930]\u001b[0m Trial 14 finished with value: 0.85625 and parameters: {'n_layers': 3, 'n_units_l0': 80, 'dropout_l0': 0.46595751045863393, 'n_units_l1': 89, 'dropout_l1': 0.33887034920025794, 'n_units_l2': 39, 'dropout_l2': 0.3468839982622389, 'optimizer': 'RMSprop', 'lr': 0.0008702736554910898}. Best is trial 14 with value: 0.85625.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:38:31,011]\u001b[0m Trial 15 finished with value: 0.865 and parameters: {'n_layers': 3, 'n_units_l0': 80, 'dropout_l0': 0.46057143132451156, 'n_units_l1': 92, 'dropout_l1': 0.32344085905260583, 'n_units_l2': 113, 'dropout_l2': 0.3778099757956277, 'optimizer': 'Adam', 'lr': 0.0011904086997786484}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:39:02,009]\u001b[0m Trial 16 finished with value: 0.854375 and parameters: {'n_layers': 3, 'n_units_l0': 76, 'dropout_l0': 0.4514439061142824, 'n_units_l1': 87, 'dropout_l1': 0.32392550836414485, 'n_units_l2': 127, 'dropout_l2': 0.38272286762549945, 'optimizer': 'Adam', 'lr': 0.0014178859634542541}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:39:32,824]\u001b[0m Trial 17 finished with value: 0.856875 and parameters: {'n_layers': 3, 'n_units_l0': 83, 'dropout_l0': 0.40161488061466677, 'n_units_l1': 52, 'dropout_l1': 0.30771764070986235, 'n_units_l2': 104, 'dropout_l2': 0.41928235173535144, 'optimizer': 'Adam', 'lr': 0.0008512043715586017}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:40:04,567]\u001b[0m Trial 18 finished with value: 0.855625 and parameters: {'n_layers': 3, 'n_units_l0': 52, 'dropout_l0': 0.398493488715146, 'n_units_l1': 49, 'dropout_l1': 0.2981924768626163, 'n_units_l2': 107, 'dropout_l2': 0.43093240723708753, 'optimizer': 'Adam', 'lr': 0.0022476119647596325}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:40:08,402]\u001b[0m Trial 19 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:40:39,137]\u001b[0m Trial 20 finished with value: 0.846875 and parameters: {'n_layers': 3, 'n_units_l0': 112, 'dropout_l0': 0.4085149619915078, 'n_units_l1': 37, 'dropout_l1': 0.31351944672928544, 'n_units_l2': 108, 'dropout_l2': 0.4184183081856254, 'optimizer': 'Adam', 'lr': 0.0037449454804344704}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:41:11,090]\u001b[0m Trial 21 finished with value: 0.859375 and parameters: {'n_layers': 3, 'n_units_l0': 78, 'dropout_l0': 0.459744886891277, 'n_units_l1': 58, 'dropout_l1': 0.3465672128658423, 'n_units_l2': 111, 'dropout_l2': 0.39649010051241224, 'optimizer': 'Adam', 'lr': 0.0007808754276631095}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:41:12,125]\u001b[0m Trial 22 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:41:48,400]\u001b[0m Trial 23 finished with value: 0.853125 and parameters: {'n_layers': 2, 'n_units_l0': 89, 'dropout_l0': 0.3957091447694264, 'n_units_l1': 28, 'dropout_l1': 0.35206846929508107, 'optimizer': 'Adam', 'lr': 0.0014966013452700237}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:41:52,515]\u001b[0m Trial 24 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:41:53,636]\u001b[0m Trial 25 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:42:24,457]\u001b[0m Trial 26 finished with value: 0.850625 and parameters: {'n_layers': 3, 'n_units_l0': 77, 'dropout_l0': 0.46514513564395843, 'n_units_l1': 35, 'dropout_l1': 0.3088955621377863, 'n_units_l2': 128, 'dropout_l2': 0.3586782267195434, 'optimizer': 'Adam', 'lr': 0.0008715789768287872}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:42:52,979]\u001b[0m Trial 27 finished with value: 0.855 and parameters: {'n_layers': 2, 'n_units_l0': 85, 'dropout_l0': 0.3885478589258727, 'n_units_l1': 59, 'dropout_l1': 0.3511185371982664, 'optimizer': 'Adam', 'lr': 0.002044383214440333}. Best is trial 15 with value: 0.865.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:42:54,090]\u001b[0m Trial 28 pruned. \u001b[0m\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","\u001b[32m[I 2022-02-13 05:43:20,718]\u001b[0m Trial 29 finished with value: 0.86 and parameters: {'n_layers': 2, 'n_units_l0': 100, 'dropout_l0': 0.33643843369264814, 'n_units_l1': 112, 'dropout_l1': 0.24503388566411793, 'optimizer': 'SGD', 'lr': 0.07280425239282005}. Best is trial 15 with value: 0.865.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Study statistics: \n","  Number of finished trials:  30\n","  Number of pruned trials:  11\n","  Number of complete trials:  19\n","Best trial:\n","  Value:  0.865\n","  Params: \n","    n_layers: 3\n","    n_units_l0: 80\n","    dropout_l0: 0.46057143132451156\n","    n_units_l1: 92\n","    dropout_l1: 0.32344085905260583\n","    n_units_l2: 113\n","    dropout_l2: 0.3778099757956277\n","    optimizer: Adam\n","    lr: 0.0011904086997786484\n"]}]},{"cell_type":"markdown","source":["## Train model\n","\n","In that step, we retrain the model with the best architecture."],"metadata":{"id":"ebGkEpr1Dwc-"}},{"cell_type":"code","source":["model = define_model(trial)"],"metadata":{"id":"tgahjtARBRnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the loss function and optimizer that suits with hyperparameter search result. "],"metadata":{"id":"BXX57BymngDT"}},{"cell_type":"code","source":["loss_criteria = nn.BCELoss()\n","\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=trial.params['lr'])\n","optimizer.zero_grad()\n","\n","# We'll track metrics for each epoch in these arrays\n","epoch_nums = []\n","training_loss = []\n","validation_loss = []\n","\n","# Train over 30 epochs\n","epochs = 30"],"metadata":{"id":"cKcQr9kMD7t5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate model\n","\n","Finally, we train the best model in train dataset and evaluate in test dataset."],"metadata":{"id":"Fx6Y4QkDD1hC"}},{"cell_type":"code","source":["def train(model, data_loader, optimizer):\n","    '''\n","    Train model through data loader and optimizer\n","    Args:\n","      model: model to train\n","      data_loader: data loader to manage batch loading\n","      optimizer: control update gradient descent\n","    '''\n","    # enable train mode\n","    model.train()\n","    train_loss = 0\n","    \n","    for batch, tensor in enumerate(data_loader):\n","        data, target = tensor\n","        # reset optimizer into zero\n","        optimizer.zero_grad()\n","        # feed forward to compute output and loss\n","        out = model(data)\n","        loss = loss_criteria(out, target)\n","        # accumulate loss\n","        train_loss += loss.item()\n","        # compute gradient descent\n","        loss.backward()\n","        # update into weight\n","        optimizer.step()\n","\n","    avg_loss = train_loss / (batch+1)\n","    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n","    return avg_loss\n","            "],"metadata":{"id":"oFhLm6p6FdCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training through epoch\n","for epoch in range(1, epochs + 1):\n","    # print the epoch number\n","    print('Epoch: {}'.format(epoch))\n","    \n","    # Feed training data into the model to optimize the weights\n","    train_loss = train(model, train_loader, optimizer)\n","    \n","    # Feed the test data into the model to check its performance\n","    test_loss = test(model, test_loader)\n","    \n","    # Log the metrics for this epoch\n","    epoch_nums.append(epoch)\n","    training_loss.append(train_loss)\n","    validation_loss.append(test_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfIbce4dDVb0","executionInfo":{"status":"ok","timestamp":1644509728276,"user_tz":-420,"elapsed":120398,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"014cffb4-9a99-4649-9eaf-766fdd490602"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1\n","Training set: Average loss: 0.429173\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["3811  :  5000\n","Validation set: Average loss: 0.386142, Accuracy: 3811/5000 (76%)\n","\n","Epoch: 2\n","Training set: Average loss: 0.406665\n","3811  :  5000\n","Validation set: Average loss: 0.372143, Accuracy: 3811/5000 (76%)\n","\n","Epoch: 3\n","Training set: Average loss: 0.393294\n","3842  :  5000\n","Validation set: Average loss: 0.363422, Accuracy: 3842/5000 (77%)\n","\n","Epoch: 4\n","Training set: Average loss: 0.386185\n","4211  :  5000\n","Validation set: Average loss: 0.357916, Accuracy: 4211/5000 (84%)\n","\n","Epoch: 5\n","Training set: Average loss: 0.384698\n","4220  :  5000\n","Validation set: Average loss: 0.352550, Accuracy: 4220/5000 (84%)\n","\n","Epoch: 6\n","Training set: Average loss: 0.377709\n","4236  :  5000\n","Validation set: Average loss: 0.347386, Accuracy: 4236/5000 (85%)\n","\n","Epoch: 7\n","Training set: Average loss: 0.373994\n","4240  :  5000\n","Validation set: Average loss: 0.343644, Accuracy: 4240/5000 (85%)\n","\n","Epoch: 8\n","Training set: Average loss: 0.367028\n","4249  :  5000\n","Validation set: Average loss: 0.338773, Accuracy: 4249/5000 (85%)\n","\n","Epoch: 9\n","Training set: Average loss: 0.361450\n","4252  :  5000\n","Validation set: Average loss: 0.334148, Accuracy: 4252/5000 (85%)\n","\n","Epoch: 10\n","Training set: Average loss: 0.359301\n","4248  :  5000\n","Validation set: Average loss: 0.330503, Accuracy: 4248/5000 (85%)\n","\n","Epoch: 11\n","Training set: Average loss: 0.358090\n","4263  :  5000\n","Validation set: Average loss: 0.326179, Accuracy: 4263/5000 (85%)\n","\n","Epoch: 12\n","Training set: Average loss: 0.354088\n","4263  :  5000\n","Validation set: Average loss: 0.323427, Accuracy: 4263/5000 (85%)\n","\n","Epoch: 13\n","Training set: Average loss: 0.348248\n","4272  :  5000\n","Validation set: Average loss: 0.320506, Accuracy: 4272/5000 (85%)\n","\n","Epoch: 14\n","Training set: Average loss: 0.344627\n","4274  :  5000\n","Validation set: Average loss: 0.318201, Accuracy: 4274/5000 (85%)\n","\n","Epoch: 15\n","Training set: Average loss: 0.341776\n","4281  :  5000\n","Validation set: Average loss: 0.316190, Accuracy: 4281/5000 (86%)\n","\n","Epoch: 16\n","Training set: Average loss: 0.340730\n","4279  :  5000\n","Validation set: Average loss: 0.317541, Accuracy: 4279/5000 (86%)\n","\n","Epoch: 17\n","Training set: Average loss: 0.337602\n","4285  :  5000\n","Validation set: Average loss: 0.315195, Accuracy: 4285/5000 (86%)\n","\n","Epoch: 18\n","Training set: Average loss: 0.338438\n","4291  :  5000\n","Validation set: Average loss: 0.315590, Accuracy: 4291/5000 (86%)\n","\n","Epoch: 19\n","Training set: Average loss: 0.336051\n","4283  :  5000\n","Validation set: Average loss: 0.315674, Accuracy: 4283/5000 (86%)\n","\n","Epoch: 20\n","Training set: Average loss: 0.333602\n","4278  :  5000\n","Validation set: Average loss: 0.313383, Accuracy: 4278/5000 (86%)\n","\n","Epoch: 21\n","Training set: Average loss: 0.334606\n","4287  :  5000\n","Validation set: Average loss: 0.315143, Accuracy: 4287/5000 (86%)\n","\n","Epoch: 22\n","Training set: Average loss: 0.334003\n","4280  :  5000\n","Validation set: Average loss: 0.314747, Accuracy: 4280/5000 (86%)\n","\n","Epoch: 23\n","Training set: Average loss: 0.332114\n","4281  :  5000\n","Validation set: Average loss: 0.313046, Accuracy: 4281/5000 (86%)\n","\n","Epoch: 24\n","Training set: Average loss: 0.329035\n","4287  :  5000\n","Validation set: Average loss: 0.313944, Accuracy: 4287/5000 (86%)\n","\n","Epoch: 25\n","Training set: Average loss: 0.329300\n","4283  :  5000\n","Validation set: Average loss: 0.313104, Accuracy: 4283/5000 (86%)\n","\n","Epoch: 26\n","Training set: Average loss: 0.330384\n","4282  :  5000\n","Validation set: Average loss: 0.314730, Accuracy: 4282/5000 (86%)\n","\n","Epoch: 27\n","Training set: Average loss: 0.334789\n","4278  :  5000\n","Validation set: Average loss: 0.315325, Accuracy: 4278/5000 (86%)\n","\n","Epoch: 28\n","Training set: Average loss: 0.329534\n","4280  :  5000\n","Validation set: Average loss: 0.314783, Accuracy: 4280/5000 (86%)\n","\n","Epoch: 29\n","Training set: Average loss: 0.325046\n","4284  :  5000\n","Validation set: Average loss: 0.313353, Accuracy: 4284/5000 (86%)\n","\n","Epoch: 30\n","Training set: Average loss: 0.327140\n","4284  :  5000\n","Validation set: Average loss: 0.316210, Accuracy: 4284/5000 (86%)\n","\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","def _evaluate(model, x_test):\n","  model.eval()\n","  x = torch.Tensor(x_test).float()\n","  predictions = [1 if i>0.5 else 0 for i in model(x).data]\n","  predictions = torch.tensor(predictions)\n","  print('Evaluation on test dataset')\n","  get_metrics(y_test, predictions)\n","\n","  # Plot the confusion matrix\n","  classes=['0','1']\n","  cm = confusion_matrix(y_test, predictions.numpy())\n","  plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n","  plt.colorbar()\n","  tick_marks = np.arange(2)\n","  plt.xticks(tick_marks, classes, rotation=85)\n","  plt.yticks(tick_marks, classes)\n","  plt.xlabel(\"Predicted\")\n","  plt.ylabel(\"Actual\")\n","  plt.show()\n","_evaluate(model, x_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"id":"RMh0QAXnlYI1","executionInfo":{"status":"ok","timestamp":1644512516167,"user_tz":-420,"elapsed":919,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"fa41ee2d-bbc8-413a-9a98-b9b7be7279d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation on test dataset\n","ACCURACY_SCORE:  0.8568\n","F1_SCORE:  0.7894\n","CONFUSION_MATRIX:\n"," [[3556  255]\n"," [ 461  728]] \n","\n","              precision    recall  f1-score   support\n","\n","           0     0.8852    0.9331    0.9085      3811\n","           1     0.7406    0.6123    0.6703      1189\n","\n","    accuracy                         0.8568      5000\n","   macro avg     0.8129    0.7727    0.7894      5000\n","weighted avg     0.8508    0.8568    0.8519      5000\n"," \n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAT8AAAEECAYAAABeGzPFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWwUlEQVR4nO3dfbBdVX3G8e9zLxBAwSQE0kiiMBjByGigEWhpKUINAdsJOIrQGU0pNdAG34Z2BKdjFMVppyhVVGqQSOgokIpIpJEQQyjgCCRAiCRIueWlJAZCCESQFw38+sdeV47x3nP2ujnnnpf9fJg995x19ss6Znxmrb3WXkcRgZlZ1fS1uwJmZu3g8DOzSnL4mVklOfzMrJIcfmZWSQ4/M6ukXdpdgVraZY/Qbnu1uxqW4bC3vandVbAMjz32KFu2bNHOnKN/7zdHbH+x1L7x4lPLImLWUJ9J2h24FRhDkUXfi4j5kq4A/gzYlnb964hYI0nAV4CTgBdS+T3pXHOAf0r7fyEiFjWqW2eF3257MebgU9tdDcvwkzu/1u4qWIajj5yx0+eI7S8x5pDTSu370r2XTKjz8cvAcRHxvKRdgdsl/Sh99o8R8b0d9j8RmJq2I4FLgSMljQfmAzOAAO6WtCQinqlXN3d7zSyPAKncVkcUnk9vd01bvacuZgNXpuPuAMZKmgScACyPiK0p8JYDQ7Y2azn8zCyf+sptjU4j9UtaA2ymCLA700cXSlor6WJJY1LZ/sDjNYdvSGXDldfl8DOzfOVbfhMkra7Z5taeJiJeiYjpwGTgCEmHAucDhwDvAsYDn2rFV+ioe35m1g1UqlWXbImIhjcaI+JZSSuBWRFxUSp+WdK3gX9I7zcCU2oOm5zKNgLH7lB+S6NruuVnZvmacM9P0r6SxqbXewDvAX6e7uORRndPBu5PhywBPqzCUcC2iNgELANmShonaRwwM5XV5ZafmeWRoK+/GWeaBCyS1E/REFscETdIulnSvhRDK2uAs9P+SymmuQxQTHU5AyAitkr6PLAq7XdBRGxtdHGHn5nlK9/tHVZErAUOG6L8uGH2D2DeMJ8tBBbmXN/hZ2b5GnRpu4HDz8wyZQ14dCyHn5nlGZzk3OUcfmaWzy0/M6seQX9TRnvbyuFnZnmEW35mVlG+52dm1ePRXjOrKrf8zKxymvd4W1s5/Mwsn7u9ZlZJ7vaaWfV4wMPMqsotPzOrHE9yNrNq8mivmVWVW35mVkm+52dmlSOP9ppZVbnlZ2ZVJIefmVVN0et1+JlZ5cgtPzOrpl4Iv+4fsjGzUSep1NbgHLtLukvSfZLWSfpcKj9Q0p2SBiRdI2m3VD4mvR9Inx9Qc67zU/mDkk4o8x0cfmaWrRnhB7wMHBcR7wSmA7MkHQX8C3BxRLwFeAY4M+1/JvBMKr847YekacBpwNuBWcA3JDV8BMXhZ2ZZJKG+cls9UXg+vd01bQEcB3wvlS8CTk6vZ6f3pM+PV5Gws4GrI+LliHgEGACOaPQ9HH5mlq1JLT8k9UtaA2wGlgP/CzwbEdvTLhuA/dPr/YHHAdLn24B9asuHOGZYHvAws2wZAx4TJK2ueb8gIhYMvomIV4DpksYC1wGHNK+W9Tn8zCxbRvhtiYgZjXaKiGclrQT+CBgraZfUupsMbEy7bQSmABsk7QK8AXi6pnxQ7THDcrfXzPIoY6t3Gmnf1OJD0h7Ae4AHgJXA+9Nuc4Dr0+sl6T3p85sjIlL5aWk0+EBgKnBXo6/hlp+ZZWvSPL9JwKI0MtsHLI6IGyStB66W9AXgXuDytP/lwH9IGgC2UozwEhHrJC0G1gPbgXmpO12Xw8/MsgjR17fzncaIWAscNkT5wwwxWhsRLwEfGOZcFwIX5lzf4Wdm+br/AQ+Hn5llUm883ubwM7NsDj8zqySHn5lVjryklZlVkhczNbOqcsvPzCrJ4Wdm1dT92dfaZ3slzUorqw5IOq+V1zKz0dOsJa3aqWUtv/S83tcpHlbeAKyStCQi1rfqmmbWelJzHm9rt1Z+gyOAgYh4OCJ+DVxNseKqmXW5Xmj5tTL8Sq2uKmmupNWSVsf2F1tYHTNrmiYsadVubR/wSKu6LgDo23O/aHN1zKyETm/VldHK8BvR6qpm1uF6ZGGDVnZ7VwFT029w7kax8OCSFl7PzEaBAKnc1sla1vKLiO2SzgGWAf3AwohY16rrmdloEX1+vK2+iFgKLG3lNcxs9PVCt7ftAx5m1mW6oEtbhsPPzLII3O01s2pyy8/Mqkdu+ZlZBRVTXRx+ZlY5nf/cbhndvzSDmY26ZkxyljRF0kpJ6yWtk/TxVP5ZSRslrUnbSTXHnJ+WyHtQ0gk15dnL57nlZ2bZmtTy2w6cGxH3SNoLuFvS8vTZxRFx0Q7XnEbxpNjbgTcCP5b01vRx9vJ5Dj8zy9OkeX4RsQnYlF4/J+kBhlj5qcZs4OqIeBl4RNIAxdJ5kJbPA5A0uHxe3fBzt9fMsgzO8yuzlT6ndABwGHBnKjpH0lpJCyWNS2XDLZNXavm8HTn8zCxbxmKmEwbX60zb3CHO9XrgWuATEfFL4FLgIGA6RcvwS634Du72mlm2jG7vloiYMfx5tCtF8H0nIr4PEBFP1nx+GXBDeltvmbzs5fPc8jOzPGrOMvYqdrgceCAivlxTPqlmt1OA+9PrJcBpksZIOhCYCtzFCJfPc8vPzLIMrufXBEcDHwJ+JmlNKvs0cLqk6UAAjwJnAUTEOkmLKQYytgPzIuIVgJEsn+fwM7NMzZnkHBG3M/QvfQy7DF5EXAhcOER59vJ5Dj8zy+Zne82seryen5lVkRc2MLPKcviZWSX1QPY5/MwskxczNbMqUo+s5+fwM7NsPZB9Dj8zy9fXA+nn8DOzbD2QfQ4/M8sjeaqLmVVUv0d7zayKeqDh5/AzszyimO7S7Rx+ZpatB3q9Dj8zy1RileZu4PAzs2w9kH0OPzPLIzzaa2YV5W6vmVWOvJKzmVVVTz/bK+kSip+OG1JEfKwlNTKzjtf90Ve/5bd61GphZl2j5wc8ImLRaFbEzLpEj8zz62u0g6R9JV0kaamkmwe30aicmXWmwUGPRlv9c2iKpJWS1ktaJ+njqXy8pOWSHkp/x6VySfqqpAFJayUdXnOuOWn/hyTNKfMdGoYf8B3gAeBA4HPAo8CqMic3s96k1PprtDWwHTg3IqYBRwHzJE0DzgNWRMRUYEV6D3AiMDVtc4FLU13GA/OBI4EjgPmDgVlPmfDbJyIuB34TEf8dEX8DHFfiODPrQaJ4trfMVk9EbIqIe9Lr5ygaWfsDs4HB226LgJPT69nAlVG4AxgraRJwArA8IrZGxDPAcmBWo+9RZqrLb9LfTZLeC/wCGF/iODPrUc2+5yfpAOAw4E5gYkRsSh89AUxMr/cHHq85bEMqG668rjLh9wVJbwDOBS4B9gY+WeI4M+tBEvSXD78JkmpnjiyIiAW/ez69HrgW+ERE/LI2WCMiJA075W5nNAy/iLghvdwGvLsVlTCz7pLR8NsSETOGP492pQi+70TE91Pxk5ImRcSm1K3dnMo3AlNqDp+cyjYCx+5QfkujijUMP0nfZojJzunen5lVUDO6vSpOcjnwQER8ueajJcAc4J/T3+trys+RdDXF4Ma2FJDLgC/WDHLMBM5vdP0y3d4bal7vDpxCcd/PzCqqSbf8jgY+BPxM0ppU9mmK0Fss6UzgMeDU9NlS4CRgAHgBOAMgIrZK+jyvzUK5ICK2Nrp4mW7vtbXvJV0F3N7oODPrTUJNebY3Im5n+Cfljh9i/wDmDXOuhcDCnOuPZGGDqcB+IzjOzHpBVVZ1kfQcv3vP7wngU62ozDsOmcKPb/23VpzaWuTxp19odxUsw6+3v9qU82SM9nasMt3evUajImbWHURvLGZa5tneFWXKzKw6mvGER7vVW89vd2BPikmK43jtxuTelJg9bWa9q9ODrYx63d6zgE8AbwTu5rXw+yXwtRbXy8w6VLFiS/enX731/L4CfEXSRyPiklGsk5l1uP4yS6J0uDJf4VVJYwffSBon6e9bWCcz62DFqi4qtXWyMuH3kYh4dvBNWjLmI62rkpl1ur6SWycrM8m5X5LS7Gok9QO7tbZaZtbJOrxRV0qZ8LsRuEbSN9P7s4Afta5KZtbJ1AVd2jLKhN+nKJaMPju9Xwv8QctqZGYdrweyr9QTHq9KuhM4iGJ1hQkU62+ZWQUJ2KUHJvrVm+T8VuD0tG0BrgGICC9oalZxvd7y+zlwG/AXETEAIMnL15tVXRc8ulZGvdHo9wGbgJWSLpN0PMOvvWVmFaKS/3WyYcMvIn4QEacBhwArKR5120/SpZJmjlYFzayzNOunK9ut4TzEiPhVRHw3Iv6S4odB7qVF6/mZWXfo71OprZNlTcKOiGciYkFE/N4S02ZWDb3S8hvJMvZmVmVVWcbezGxHVXnCw8zstwa7vd3O4Wdm2Xqg4efwM7M8Qj3x622dvuSWmXWakiO9ZbrGkhZK2izp/pqyz0raKGlN2k6q+ex8SQOSHpR0Qk35rFQ2IOm8Ml/D4Wdm2Zq4kvMVwKwhyi+OiOlpWwogaRpwGvD2dMw3JPWnNUa/DpwITANOT/vW5W6vmWUpfre3OeeKiFslHVBy99nA1RHxMvCIpAHgiPTZQEQ8DCDp6rTv+nonc8vPzLKNwm94nCNpbeoWj0tl+wOP1+yzIZUNV17/O+xM7cysmqRyG8Xvfq+u2eaWOP2lFOuHTqdYXOVLrfgO7vaaWRaJnNHeLRExI+f8EfHka9fSZcAN6e1GYErNrpNTGXXKh+WWn5llU8ltROeWJtW8PQUYHAleApwmaYykA4GpwF3AKmCqpAMl7UYxKLKk0XXc8jOzLIO/29uUc0lXAcdSdI83APOBYyVNBwJ4lOJH04iIdZIWUwxkbAfmRcQr6TznAMuAfmBhRKxrdG2Hn5lla9YU54g4fYjiy+vsfyFw4RDlS4GlOdd2+JlZth54wMPhZ2Z5euXxNoefmWWTw8/Mqqj7o8/hZ2a55JafmVWQ6I0Jwg4/M8vmlp+ZVZKXsTezyim6vd2ffg4/M8vWA71eh5+Z5RJyy8/MqsgtPzOrHN/zM7NqEvT1wEQ/h5+ZZfM9PzOrnGIx03bXYuc5/Mwsm1t+ZlZJHu01s8oRWb/e1rFaNmaTfmx4s6T7G+9tZt1Dpf/rZK0csL4CmNXC85tZO5T8wfJObxy2LPwi4lZga6vOb2bt08rf7R0tbb/nJ2kuMBdg8pQ3tbk2ZtZIM3+3t53aPk87IhZExIyImLHPhAntro6ZldALLb+2h5+ZdR9JpbYS5/m9gVFJ4yUtl/RQ+jsulUvSVyUNSFor6fCaY+ak/R+SNKfMd3D4mVm2Jg54XMHvD4yeB6yIiKnAivQe4ERgatrmApcWddF4YD5wJHAEMH8wMOtp5VSXq4CfAgdL2iDpzFZdy8xGV7O6vcMMjM4GFqXXi4CTa8qvjMIdwFhJk4ATgOURsTUingGWU2KmScsGPCLi9Fad28zarLU39CZGxKb0+glgYnq9P/B4zX4bUtlw5XW1fbTXzLpL0aornX4TJK2ueb8gIhaUPTgiQlLk1K8sh5+Z5cmbwLwlImZkXuFJSZMiYlPq1m5O5RuBKTX7TU5lG4Fjdyi/pdFFPOBhZtla/ITHEmBwxHYOcH1N+YfTqO9RwLbUPV4GzJQ0Lg10zExldbnlZ2aZmvfcbhoYPZaie7yBYtT2n4HFaZD0MeDUtPtS4CRgAHgBOAMgIrZK+jywKu13QUQ0fLrM4Wdm2Zr1gEedgdHjh9g3gHnDnGchsDDn2g4/M8vSDU9vlOHwM7N8PZB+Dj8zy9YLCxs4/MwsW/dHn8PPzHL1yE0/h5+ZZev0JerLcPiZWRbR+UvUl+HwM7NsPZB9Dj8zy1dmodJO5/Azs2w9kH0OPzPL1wPZ5/AzsxHogfRz+JlZlszFTDuWw8/M8gj6uj/7HH5mNgIOPzOrnuYtZtpODj8zy+apLmZWOT2yroHDz8xGoAfSz+FnZtm8mKmZVVL3R5/Dz8xy7dxv8nYMh5+ZjUD3p5/Dz8yy9Mpipn3troCZdR+V3BqeR3pU0s8krZG0OpWNl7Rc0kPp77hULklflTQgaa2kw3fmOzj8zCxbn1RqK+ndETE9Imak9+cBKyJiKrAivQc4EZiatrnApTv1HXbmYDOrqGY1/YY2G1iUXi8CTq4pvzIKdwBjJU0a6UUcfmaWLSP7JkhaXbPN3eFUAdwk6e6azyZGxKb0+glgYnq9P/B4zbEbUtmIeMDDzLIob6rLlpru7FD+JCI2StoPWC7p57UfRkRIihFWtS63/Mwsm0r+10hEbEx/NwPXAUcATw52Z9PfzWn3jcCUmsMnp7IRcfiZWbbB1l+jrf459DpJew2+BmYC9wNLgDlptznA9en1EuDDadT3KGBbTfc4m7u9ZpatSfP8JgLXpZ/B3AX4bkTcKGkVsFjSmcBjwKlp/6XAScAA8AJwxs5c3OFnZpmas5hpRDwMvHOI8qeB44coD2DeTl84cfiZWRY/4WFm1sXc8jOzbL3Q8nP4mVkeeTFTM6sg/4aHmVVXD6Sfw8/Msvl3e82sknrglp/Dz8zy9UD2OfzMLJ96oOnn8DOzLL3yhIeKx+U6g6SnKB5k7jUTgC3troRl6dV/szdHxL47cwJJN1L871PGloiYtTPXa5WOCr9eJWl1gwUdrcP436z3+dleM6skh5+ZVZLDb3QsaHcFLJv/zXqc7/mZWSW55WdmleTwazL1wuxPswpwt7cFJB0AHAy8CDwNPBoRv2pnnczsdzn8mkjSQcDHgHHAU8DuwKsUE7evjYhH2lg9G0Jqqdf2gAIgIl6VtEtEbG9PzazVHH5NJOnrwHPAD4FtQD/FTPgPUvzA8icj4sH21dBySPoA8D8RcV+762LN52d7m+sPgXMj4ic7lK+QtBw4CHD4dRBJ5wOHAk8CT6S/W4DVwHnAfMDh14Mcfs11EfBZSXcA9wJbgZeBMcCe9OZzy93uTGAJxW2KNwOHAeMp/r9xGPBo22pmLeXwa64fAL8G3gUcA+xGcf9vKvAZYH37qmbDeBD4YUSsHCyQpIgISQMUA1bWg3zPrwUk9QN7Uwx4vBQRz7S5SjYMSbsDRMRLQ3z2GeCLHvToTQ4/M6skT3I2s0py+JlZJTn8KkDSK5LWSLpf0n9K2nMnznWFpPen19+SNK3OvsdK+uMRXONRSWVXCjYbEYdfNbwYEdMj4lCK0eizaz+UNKJR/4j424ioN4J9LJAdfmajweFXPbcBb0mtstskLQHWS+qX9K+SVklaK+ksKKZ9SPqapAcl/RjYb/BEkm6RNCO9niXpHkn3SVqRnm8+G/hkanX+qaR9JV2brrFK0tHp2H0k3SRpnaRv0Ru/jGgdzvP8KiS18E4EbkxFhwOHRsQjkuYC2yLiXZLGAD+RdBPFRN+DgWnARIq5igt3OO++wGXAMelc4yNiq6R/B56PiIvSft8FLo6I2yW9CVgGvI3iKYrbI+ICSe+lmHhs1lIOv2rYQ9Ka9Po24HKK7uhdNYstzATeMXg/D3gDxeTsY4CrIuIV4BeSbh7i/EcBtw6eKyK2DlOPPwem1az6tbek16drvC8d+1+SPC/SWs7hVw0vRsT02oIUQLXLbAn4aEQs22G/k5pYjz7gqB0nFHsJRGsH3/OzQcuAv5O0K4Ckt0p6HXAr8MF0T3AS8O4hjr0DOEbSgenY8an8OWCvmv1uAj46+EbSYCDfCvxVKjuR4pFAs5Zy+Nmgb1Hcz7tH0v3ANyl6BtcBD6XPrgR+uuOBEfEUMBf4vqT7gGvSRz8EThkc8KBY63BGGlBZz2ujzp+jCM91FN3f/2vRdzT7LT/eZmaV5JafmVWSw8/MKsnhZ2aV5PAzs0py+JlZJTn8zKySHH5mVkkOPzOrpP8Hu1rIPlZUXasAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Conclusion: Optuna find a neural architecture archiving up to 85.68% accuracy on train dataset and was higher than the original model with only 84.86%."],"metadata":{"id":"uy_QI8ban_YR"}}]}