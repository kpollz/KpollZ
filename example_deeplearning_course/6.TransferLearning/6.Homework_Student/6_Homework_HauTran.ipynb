{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.Homework_HauTran.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b4746b500f344f7f8f0525be736c0748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_06ac077b019e4b6da60d5e2e02dc663b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eee50bdc7f9f4ff393174954d1168430",
              "IPY_MODEL_dcf0817179bd437790ec8dfc704f2983",
              "IPY_MODEL_82b04b150be44936996b7e0775474d43"
            ]
          }
        },
        "06ac077b019e4b6da60d5e2e02dc663b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eee50bdc7f9f4ff393174954d1168430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b76c0aac9e364f2ea5b0c8e3a5a2abf8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f1998cb0a6c4e96b7d905e4b05ea69d"
          }
        },
        "dcf0817179bd437790ec8dfc704f2983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_07e0d285757146e2931edcf8d3c6d821",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31934f111dd14bafac03fbea6ade0ae7"
          }
        },
        "82b04b150be44936996b7e0775474d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7032d7b2056845c987cd0d160b558530",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 220MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c2d407c007b4f7da7d4e917732c13c3"
          }
        },
        "b76c0aac9e364f2ea5b0c8e3a5a2abf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f1998cb0a6c4e96b7d905e4b05ea69d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07e0d285757146e2931edcf8d3c6d821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31934f111dd14bafac03fbea6ade0ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7032d7b2056845c987cd0d160b558530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c2d407c007b4f7da7d4e917732c13c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# I. Lý thuyết"
      ],
      "metadata": {
        "id": "3oII0r_lpubh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Transfer Learning thường được thực hiện trên hai dữ liệu nguồn (huấn luyện mô hình nguồn) và dữ liệu đích (huấn luyện mô hình đích) thế nào ?\n",
        "\n",
        "Đáp án: \n",
        "\n",
        "A. Dữ liệu nguồn và dữ liệu đích có sự liên quan tới nhau. Những đặc trưng trong dữ liệu đích xuất hiện ở những dữ liệu nguồn.\n",
        "\n"
      ],
      "metadata": {
        "id": "l5Ty0elJpzS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Khi nào thì chúng ta nên thực hiện fine tuning trên toàn bộ các layers của mô hình đích ?\n",
        "\n",
        "Đáp án:\n",
        "\n",
        "A. Khi hai dữ liệu nguồn và đích có mức độ tương đồng cao và dữ liệu đích có kích thước lớn.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wHXvMCwaxlyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Các phương pháp augmentation nào sẽ kết hợp nội dung từ hai ảnh lẫn nhau và tạo ra một nhãn mềm (_soft label_) cho ảnh?\n",
        "\n",
        "Đáp án:\n",
        "\n",
        "\n",
        "C. CutMix, MixUp\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NOweiRmVrNeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Quá trình xây dựng một mô hình AI trong dự án là một chu trình Machine Learning Cycle kế hợp giữa huấn luyện và gán nhãn dữ liệu. Để tiết kiệm chi phí gán nhãn chúng ta nên sử dụng phương pháp nào ?\n",
        "\n",
        "Đáp án:\n",
        "\n",
        "B. Sử dụng Active Learning để lựa chọn mẫu mang lại thông tin giúp cải thiện nhiều nhất cho hiệu suất mô hình.\n",
        "\n"
      ],
      "metadata": {
        "id": "D_mh5GpdtA6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Mô hình lớn thường đạt độ chính xác cao nhưng không deploy được trên các thiết bị edge device, IoT,... Trong khi mô hình nhỏ có thể deploy được nhưng thường có độ chính xác thấp. Phương pháp nào có thể giúp mô hình nhỏ cải thiện được độ chính xác ? Có thể lựa chọn nhiều đáp án.\n",
        "\n",
        "Đáp án:\n",
        "\n",
        "B. Áp dụng augmentation để huấn luyện mô hình nhỏ.\n",
        "\n",
        "C. Fine tuning các layers của mô hình lớn sang mô hình nhỏ.\n",
        "\n",
        "D. Sử dụng knowledge distillation để chuyển giao tri thức từ mô hình lớn sang mô hình nhỏ."
      ],
      "metadata": {
        "id": "aVBcCa80uqaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Thực hành"
      ],
      "metadata": {
        "id": "RLNVdWEupwu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Từ bộ dữ liệu [Dog and Cat](https://www.kaggle.com/c/dog-vs-cat-classification/data), hãy huấn luyện một mô hình large (chẳng hạn ResNet50) bằng cách fine-tuning lại các trọng số từ pretrained model của bộ dữ liệu ImageNet. Huấn luyện trên 5 epochs.\n",
        "\n",
        "7) Hãy huấn luyện một mô hình small (chẳng hạn MobileNetV3) không sử dụng pretrained model trên 1 epochs.\n",
        "\n",
        "8) Sử dụng mô hình large làm teacher để cải thiện mô hình small là student theo phương pháp knowledge distillation.\n",
        "\n",
        "9) Áp dụng thêm các kĩ thuật data augmentation kết hợp ảnh khác nhãn để tạo thành nhãn mềm và huấn luyện cải thiện tiếp model student.\n",
        "\n",
        "10) Giả định cần huấn luyện tiếp mô hình student với các dữ liệu mới chưa được gán nhãn. Hãy xây dựng một kĩ thuật lựa chọn mẫu dựa trên đánh giá uncertainty."
      ],
      "metadata": {
        "id": "TiNt8X4zxsvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/DL/6.Homework_HauTran/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhNZzdKVQYDh",
        "outputId": "8b2b9bc0-295a-4800-97e2-1cf18d61fd8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6) Từ bộ dữ liệu Dog and Cat, hãy huấn luyện một mô hình large (chẳng hạn ResNet50) bằng cách fine-tuning lại các trọng số từ pretrained model của bộ dữ liệu ImageNet. Huấn luyện trên 5 epochs."
      ],
      "metadata": {
        "id": "3kRFhgiURYwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import time\n",
        "import pickle\n",
        "import pprint\n",
        "import math\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import tqdm\n",
        "import random\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "oy_JQJVMR4dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prepare Dataset"
      ],
      "metadata": {
        "id": "b2XriB_yRnxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = '../data/Dog-and-Cat/train/train'\n",
        "TEST_DIR = '../data/Dog-and-Cat/test/test'\n",
        "train_images = glob.glob(TRAIN_DIR+\"/**/**.jpg\")\n",
        "test_images = glob.glob(TEST_DIR+\"/**.jpg\")"
      ],
      "metadata": {
        "id": "2H_MhnueRN__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dogs_list = [img for img in train_images if img.split(\"/\")[-2] == \"dogs\"]\n",
        "cats_list = [img for img in train_images if img.split(\"/\")[-2] == \"cats\"]\n",
        "\n",
        "print(\"Dogs Images: \",len(dogs_list))\n",
        "print(\"Cats Images: \",len(cats_list))\n",
        "\n",
        "class_to_int = {\"dogs\" : 0, \"cats\" : 1}\n",
        "int_to_class = {0 : \"dogs\", 1 : \"cats\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcRYz-CxSPnM",
        "outputId": "48286a23-226f-4987-c295-3ecf3e070a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dogs Images:  12500\n",
            "Cats Images:  12500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "class CatDogDataset(Dataset):\n",
        "    def __init__(self, imgs, class_to_int, mode = \"train\", \n",
        "                 transforms = None):\n",
        "        super().__init__()\n",
        "        self.imgs = imgs\n",
        "        self.class_to_int = class_to_int\n",
        "        self.mode = mode\n",
        "        self.transforms = transforms\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.imgs[idx]\n",
        "        if self.mode == \"train\" or self.mode == \"val\":\n",
        "            img = Image.open(image_name)\n",
        "            # img = img.resize((256, 256))\n",
        "            ### Preparing class label\n",
        "            label = self.class_to_int[image_name.split(\"/\")[-2]]\n",
        "            label = torch.tensor(label, dtype = torch.float32)\n",
        "            ### Apply Transforms on image\n",
        "            img = self.transforms(img)\n",
        "            return img, label\n",
        "        elif self.mode == \"test\":\n",
        "            img = Image.open(image_name)\n",
        "            # img = img.resize((256, 256))\n",
        "            ### Apply Transforms on image\n",
        "            img = self.transforms(img)\n",
        "            return img, image_name\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "JgeyNqFUVcPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_transform():\n",
        "    return T.Compose([\n",
        "        T.RandomHorizontalFlip(p=0.5), # Random flip with probability = 0.5\n",
        "        T.RandomRotation(15), # Random rotation with angle <= 15\n",
        "        # T.ColorJitter(brightness=.5, hue=.3), # Bright contrast\n",
        "        T.Resize((256, 256)),\n",
        "        T.RandomResizedCrop(224), # Random crop Image with shape 224\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)), # Normalize according to ImageNet distribution\n",
        "    ])\n",
        "    \n",
        "def get_val_transform():\n",
        "    return T.Compose([\n",
        "        T.Resize((224, 224)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "    ])"
      ],
      "metadata": {
        "id": "NddfUWnAVhpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_imgs, val_imgs = train_test_split(train_images, test_size = 0.2)"
      ],
      "metadata": {
        "id": "uabIUbPgVppq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CatDogDataset(train_imgs, class_to_int, mode = \"train\", \n",
        "                              transforms = get_train_transform())\n",
        "val_dataset = CatDogDataset(val_imgs, class_to_int, mode = \"val\", \n",
        "                            transforms = get_val_transform())\n",
        "test_dataset = CatDogDataset(test_images, class_to_int, mode = \"test\", \n",
        "                             transforms = get_val_transform())\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    num_workers = 2,\n",
        "    batch_size = 32,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "val_data_loader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    num_workers = 2,\n",
        "    batch_size = 16,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    num_workers = 2,\n",
        "    batch_size = 1,\n",
        "    shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "bLIuvwTVVr9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fine tuning ResNet50"
      ],
      "metadata": {
        "id": "BjvvBmm-WSvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50\n",
        "model = resnet50(pretrained = True)\n",
        "\n",
        "num_classes = 1\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc =nn.Sequential(\n",
        "    nn.Linear(num_ftrs, num_classes, bias = True) ,\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b4746b500f344f7f8f0525be736c0748",
            "06ac077b019e4b6da60d5e2e02dc663b",
            "eee50bdc7f9f4ff393174954d1168430",
            "dcf0817179bd437790ec8dfc704f2983",
            "82b04b150be44936996b7e0775474d43",
            "b76c0aac9e364f2ea5b0c8e3a5a2abf8",
            "9f1998cb0a6c4e96b7d905e4b05ea69d",
            "07e0d285757146e2931edcf8d3c6d821",
            "31934f111dd14bafac03fbea6ade0ae7",
            "7032d7b2056845c987cd0d160b558530",
            "7c2d407c007b4f7da7d4e917732c13c3"
          ]
        },
        "id": "XdH06ttbWKFY",
        "outputId": "d0dac5b6-6f7b-40b3-b4a7-ab82773d0958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4746b500f344f7f8f0525be736c0748",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2Hdzr2ya90l",
        "outputId": "185efe71-826e-408e-a9f3-914a690bb778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2048, out_features=1, bias=True)\n",
              "  (1): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Xavier initialize\n",
        "for layer in model.fc:\n",
        "  if isinstance(layer, nn.modules.linear.Linear):\n",
        "    nn.init.xavier_uniform_(layer.weight)"
      ],
      "metadata": {
        "id": "1LcyIoLBckdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Warm up\n",
        "We need to warm up model in 5 epochs"
      ],
      "metadata": {
        "id": "77MjgTTAc1VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Logs - Helpful for plotting after training finishes\n",
        "train_logs = {\"loss\" : [], \"accuracy\" : [], \"time\" : []}\n",
        "val_logs = {\"loss\" : [], \"accuracy\" : [], \"time\" : []}\n",
        "\n",
        "# setup device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Loading model to device\n",
        "model.to(device)\n",
        "\n",
        "# No of epochs \n",
        "warm_up_epochs = 3\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "aplUcIXBcu-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, trues):\n",
        "    preds = [1 if preds[i] >= 0.5 else 0 for i in range(len(preds))]\n",
        "    acc = [1 if preds[i] == trues[i] else 0 for i in range(len(preds))]\n",
        "    acc = np.sum(acc) / len(preds)\n",
        "    return (acc * 100)"
      ],
      "metadata": {
        "id": "kuZz8c9GdvT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(train_data_loader, model, optimizer):\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    start_time = time.time()\n",
        "    # model.to(device)\n",
        "  \n",
        "    model.train()\n",
        "    \n",
        "    for images, labels in train_data_loader:\n",
        "        \n",
        "        #Loading images and labels to device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels.reshape((labels.shape[0], 1)) # [N, 1] - to match with preds shape\n",
        "        \n",
        "        #Reseting Gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Forward\n",
        "        preds = model(images)\n",
        "        \n",
        "        #Calculating Loss\n",
        "        _loss = criterion(preds, labels)\n",
        "        loss = _loss.item()\n",
        "        epoch_loss.append(loss)\n",
        "        \n",
        "        #Calculating Accuracy\n",
        "        acc = accuracy(preds, labels)\n",
        "        epoch_acc.append(acc)\n",
        "        \n",
        "        #Backward\n",
        "        _loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    ###Overall Epoch Results\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    \n",
        "    ###Acc and Loss\n",
        "    epoch_loss = np.mean(epoch_loss)\n",
        "    epoch_acc = np.mean(epoch_acc)\n",
        "    \n",
        "    ###Storing results to logs\n",
        "    train_logs[\"loss\"].append(epoch_loss)\n",
        "    train_logs[\"accuracy\"].append(epoch_acc)\n",
        "    train_logs[\"time\"].append(total_time)\n",
        "        \n",
        "    return epoch_loss, epoch_acc, total_time"
      ],
      "metadata": {
        "id": "9WLstN9Ld7ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_one_epoch(val_data_loader, model, best_val_acc, model_name):\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    start_time = time.time()\n",
        "    # model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    for images, labels in val_data_loader:\n",
        "        \n",
        "        #Loading images and labels to device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels.reshape((labels.shape[0], 1)) # [N, 1] - to match with preds shape\n",
        "        \n",
        "        #Forward\n",
        "        preds = model(images)\n",
        "        \n",
        "        #Calculating Loss\n",
        "        _loss = criterion(preds, labels)\n",
        "        loss = _loss.item()\n",
        "        epoch_loss.append(loss)\n",
        "        \n",
        "        #Calculating Accuracy\n",
        "        acc = accuracy(preds, labels)\n",
        "        epoch_acc.append(acc)\n",
        "    \n",
        "    ###Overall Epoch Results\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    \n",
        "    ###Acc and Loss\n",
        "    epoch_loss = np.mean(epoch_loss)\n",
        "    epoch_acc = np.mean(epoch_acc)\n",
        "    \n",
        "    ###Storing results to logs\n",
        "    val_logs[\"loss\"].append(epoch_loss)\n",
        "    val_logs[\"accuracy\"].append(epoch_acc)\n",
        "    val_logs[\"time\"].append(total_time)\n",
        "    \n",
        "    ###Saving best model\n",
        "    if epoch_acc > best_val_acc:\n",
        "        best_val_acc = epoch_acc\n",
        "        torch.save(model.state_dict(),model_name+\"_best.pth\")\n",
        "        \n",
        "    return epoch_loss, epoch_acc, total_time, best_val_acc"
      ],
      "metadata": {
        "id": "3zh_Kp9aeEWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "learning_rate = 0.00001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "best_val_acc = 0\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
        "\n",
        "# params_1x = [param for name, param in model.named_parameters()\n",
        "#              if name not in [\"classifier.1.0.weight\", \"classifier.1.0.bias\"]]\n",
        "# trainer = torch.optim.SGD([{'params': params_1x},\n",
        "#                             {'params': model.fc.parameters(), 'lr': learning_rate * 10}],\n",
        "#                         lr=learning_rate, weight_decay=0.001)\n"
      ],
      "metadata": {
        "id": "bE4F3o-neSQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    ###Training\n",
        "    loss, acc, _time = train_one_epoch(train_data_loader, model, optimizer)\n",
        "    \n",
        "    #Print Epoch Details\n",
        "    print(\"\\nTraining\")\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    print(\"Loss : {}\".format(round(loss, 4)))\n",
        "    print(\"Acc : {}\".format(round(acc, 4)))\n",
        "    print(\"Time : {}\".format(round(_time, 4)))\n",
        "    \n",
        "    ###Validation\n",
        "    loss, acc, _time, best_val_acc = val_one_epoch(val_data_loader, model, best_val_acc, \"resnet50\")\n",
        "    \n",
        "    #Print Epoch Details\n",
        "    print(\"\\nValidating\")\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    print(\"Loss : {}\".format(round(loss, 4)))\n",
        "    print(\"Acc : {}\".format(round(acc, 4)))\n",
        "    print(\"Time : {}\".format(round(_time, 4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oQ9FrREj_pS",
        "outputId": "d164fbc2-f638-48ec-a165-148986e337db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training\n",
            "Epoch 1\n",
            "Loss : 0.1485\n",
            "Acc : 94.695\n",
            "Time : 185.3243\n",
            "\n",
            "Validating\n",
            "Epoch 1\n",
            "Loss : 0.0315\n",
            "Acc : 98.9217\n",
            "Time : 32.5047\n",
            "\n",
            "Training\n",
            "Epoch 2\n",
            "Loss : 0.0786\n",
            "Acc : 96.83\n",
            "Time : 183.3233\n",
            "\n",
            "Validating\n",
            "Epoch 2\n",
            "Loss : 0.0275\n",
            "Acc : 99.0415\n",
            "Time : 32.4375\n",
            "\n",
            "Training\n",
            "Epoch 3\n",
            "Loss : 0.0689\n",
            "Acc : 97.335\n",
            "Time : 182.0122\n",
            "\n",
            "Validating\n",
            "Epoch 3\n",
            "Loss : 0.026\n",
            "Acc : 99.0615\n",
            "Time : 32.1791\n",
            "\n",
            "Training\n",
            "Epoch 4\n",
            "Loss : 0.0666\n",
            "Acc : 97.43\n",
            "Time : 182.0747\n",
            "\n",
            "Validating\n",
            "Epoch 4\n",
            "Loss : 0.0244\n",
            "Acc : 99.1414\n",
            "Time : 32.2885\n",
            "\n",
            "Training\n",
            "Epoch 5\n",
            "Loss : 0.0578\n",
            "Acc : 97.755\n",
            "Time : 181.1974\n",
            "\n",
            "Validating\n",
            "Epoch 5\n",
            "Loss : 0.0271\n",
            "Acc : 98.9617\n",
            "Time : 32.2572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7) Hãy huấn luyện một mô hình small (chẳng hạn MobileNetV3) không sử dụng pretrained model trên 1 epochs."
      ],
      "metadata": {
        "id": "J1RSnU2ugYqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import mobilenet_v3_small\n",
        "model_mnv3 = mobilenet_v3_small(pretrained = False)\n",
        "model_mnv3.classifier._modules['3']  = nn.Sequential(\n",
        "    nn.Linear(1024, 1, bias = True),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "model_mnv3.to(device)"
      ],
      "metadata": {
        "id": "CUo7oEE1gLpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    ###Training\n",
        "    loss, acc, _time = train_one_epoch(train_data_loader, model_mnv3, optimizer)\n",
        "    \n",
        "    #Print Epoch Details\n",
        "    print(\"\\nTraining\")\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    print(\"Loss : {}\".format(round(loss, 4)))\n",
        "    print(\"Acc : {}\".format(round(acc, 4)))\n",
        "    print(\"Time : {}\".format(round(_time, 4)))\n",
        "    \n",
        "    ###Validation\n",
        "    loss, acc, _time, best_val_acc = val_one_epoch(val_data_loader, model_mnv3, best_val_acc, \"mobilenet_v3\")\n",
        "    \n",
        "    #Print Epoch Details\n",
        "    print(\"\\nValidating\")\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    print(\"Loss : {}\".format(round(loss, 4)))\n",
        "    print(\"Acc : {}\".format(round(acc, 4)))\n",
        "    print(\"Time : {}\".format(round(_time, 4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSxD8jHO32_t",
        "outputId": "475ed7e6-6e20-4463-e2ab-d70f754d100c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training\n",
            "Epoch 1\n",
            "Loss : 0.6924\n",
            "Acc : 49.71\n",
            "Time : 2430.5089\n",
            "\n",
            "Validating\n",
            "Epoch 1\n",
            "Loss : 0.6932\n",
            "Acc : 51.1382\n",
            "Time : 661.9869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8) Sử dụng mô hình large làm teacher để cải thiện mô hình small là student theo phương pháp knowledge distillation."
      ],
      "metadata": {
        "id": "jqCrqOR6g_wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(\n",
        "    teacher,\n",
        "    student,\n",
        "    optimizer,\n",
        "    student_loss_fn,\n",
        "    divergence_loss_fn,\n",
        "    temp,\n",
        "    alpha,\n",
        "    epoch,\n",
        "    device\n",
        "):\n",
        "    losses = []\n",
        "    epoch_acc = []\n",
        "    for images, labels in train_data_loader:\n",
        "        # Get data to cuda if possible\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels.reshape((labels.shape[0], 1)) # [N, 1] - to match with preds shape\n",
        "\n",
        "        # forward\n",
        "        with torch.no_grad():\n",
        "            teacher_preds = teacher(images)\n",
        "\n",
        "        student_preds = student_model(images)\n",
        "        student_loss = student_loss_fn(student_preds, labels)\n",
        "        \n",
        "        ditillation_loss = divergence_loss_fn(\n",
        "            F.softmax(student_preds / temp, dim=1),\n",
        "            F.softmax(teacher_preds / temp, dim=1)\n",
        "        )\n",
        "        loss = alpha * student_loss + (1 - alpha) * ditillation_loss\n",
        "        losses.append(loss.item())\n",
        "        #Calculating Accuracy\n",
        "        acc = accuracy(student_preds, labels)\n",
        "        epoch_acc.append(acc)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "    epoch_acc = np.mean(epoch_acc)\n",
        "    avg_loss = np.mean(losses)\n",
        "    return avg_loss, epoch_acc\n",
        "  \n",
        "def main(epochs, teacher, student, temp=3, alpha=0.3):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  teacher = teacher.to(device)\n",
        "  student = student.to(device)\n",
        "  student_loss_fn = nn.BCELoss()\n",
        "  divergence_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "  optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
        "\n",
        "  teacher.eval()\n",
        "  student.train()\n",
        "  for epoch in range(epochs):\n",
        "      loss, acc = train_step(\n",
        "          teacher,\n",
        "          student,\n",
        "          optimizer,\n",
        "          student_loss_fn,\n",
        "          divergence_loss_fn,\n",
        "          temp,\n",
        "          alpha,\n",
        "          epoch,\n",
        "          device\n",
        "      )\n",
        "      print(f\"Loss:{loss:.2f}\\tAccuracy:{acc:.2f}\")\n",
        "        \n",
        "student_model = model_mnv3\n",
        "main(epochs=3, teacher=model, student=student_model, temp=2, alpha=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYcUgBUGghV2",
        "outputId": "0b4151f2-c603-4a80-8f8f-1a9124878ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:-0.69\tAccuracy:71.48\n",
            "Loss:-0.69\tAccuracy:73.44\n",
            "Loss:-0.70\tAccuracy:73.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9) Áp dụng thêm các kĩ thuật data augmentation kết hợp ảnh khác nhãn để tạo thành nhãn mềm và huấn luyện cải thiện tiếp model student."
      ],
      "metadata": {
        "id": "Mks-O5drhFK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mixup"
      ],
      "metadata": {
        "id": "WBJLdptp9qp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "class CatDogDataset(Dataset):\n",
        "    def __init__(self, imgs, class_to_int, mode = \"train\", \n",
        "                 transforms = None):\n",
        "        super().__init__()\n",
        "        self.imgs = imgs\n",
        "        self.class_to_int = class_to_int\n",
        "        self.mode = mode\n",
        "        self.transforms = transforms\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.imgs[idx]\n",
        "        if self.mode == \"train\" or self.mode == \"val\":\n",
        "            img = Image.open(image_name)\n",
        "            if idx > 0 and idx%10 == 0:\n",
        "\n",
        "              # Choose another image/label randomly\n",
        "              mixup_idx = random.randint(0, len(self.imgs)-1)\n",
        "              label = self.class_to_int[self.imgs[mixup_idx].split(\"/\")[-2]]\n",
        "              label = torch.tensor(label, dtype = torch.float32)\n",
        "              \n",
        "              mixup_image = self.transform(Image.open(self.imgs[mixup_idx]))\n",
        "\n",
        "              # Select a random number from the given beta distribution\n",
        "              # Mixup the images accordingly\n",
        "              alpha = 0.2\n",
        "              lam = np.random.beta(alpha, alpha)\n",
        "              img = lam * img + (1 - lam) * mixup_image\n",
        "              label = lam * label + (1 - lam) * label\n",
        "            else:\n",
        "              # img = img.resize((256, 256))\n",
        "              ### Preparing class label\n",
        "              label = self.class_to_int[image_name.split(\"/\")[-2]]\n",
        "              label = torch.tensor(label, dtype = torch.float32)\n",
        "              ### Apply Transforms on image\n",
        "              img = self.transforms(img)\n",
        "            return img, label\n",
        "        elif self.mode == \"test\":\n",
        "            img = Image.open(image_name)\n",
        "            # img = img.resize((256, 256))\n",
        "            ### Apply Transforms on image\n",
        "            img = self.transforms(img)\n",
        "            return img, image_name\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "Q4rOC3Dc6p2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cutmix"
      ],
      "metadata": {
        "id": "nusm2nIH9tep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cutmix(batch, alpha):\n",
        "    data, targets = batch\n",
        "\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets = targets[indices]\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "    image_h, image_w = data.shape[2:]\n",
        "    cx = np.random.uniform(0, image_w)\n",
        "    cy = np.random.uniform(0, image_h)\n",
        "    w = image_w * np.sqrt(1 - lam)\n",
        "    h = image_h * np.sqrt(1 - lam)\n",
        "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
        "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
        "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
        "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
        "\n",
        "    data[:, :, y0:y1, x0:x1] = shuffled_data[:, :, y0:y1, x0:x1]\n",
        "    targets = (targets, shuffled_targets, lam)\n",
        "\n",
        "    return data, targets\n",
        "class CutMixCollator:\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = torch.utils.data.dataloader.default_collate(batch)\n",
        "        batch = cutmix(batch, self.alpha)\n",
        "        return batch"
      ],
      "metadata": {
        "id": "9RPW0ns-z596"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CatDogDataset(train_imgs, class_to_int, mode = \"train\", \n",
        "                              transforms = get_train_transform())\n",
        "val_dataset = CatDogDataset(val_imgs, class_to_int, mode = \"val\", \n",
        "                            transforms = get_val_transform())\n",
        "test_dataset = CatDogDataset(test_images, class_to_int, mode = \"test\", \n",
        "                             transforms = get_val_transform())\n",
        "cutmix_alpha =1.0\n",
        "if cutmix_alpha:\n",
        "        collator = CutMixCollator(cutmix_alpha)\n",
        "else:\n",
        "    collator = torch.utils.data.dataloader.default_collate\n",
        "train_data_loader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    num_workers = 2,\n",
        "    batch_size = 32,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "val_data_loader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    num_workers = 2,\n",
        "    batch_size = 16,\n",
        "    collate_fn=collator,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    num_workers = 2,\n",
        "    batch_size = 1,\n",
        "    collate_fn=collator,\n",
        "    shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "dahhXxLx20zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=1, teacher=model, student=student_model, temp=2, alpha=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuz8oPPS3kX-",
        "outputId": "091aeb10-be01-4f69-af3f-c932a46ec8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:-0.70\tAccuracy:75.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10) Giả định cần huấn luyện tiếp mô hình student với các dữ liệu mới chưa được gán nhãn. Hãy xây dựng một kĩ thuật lựa chọn mẫu dựa trên đánh giá uncertainty."
      ],
      "metadata": {
        "id": "uVzpnZb-hH_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lấy ra các điểm dữ liệu chính và tiến hành phân cụm:\n",
        "- Lấy một vài điểm dữ đặc trưng cho bộ dữ liệu\n",
        "- Gán nhãn cho các điểm dữ liệu này\n",
        "- Sử dụng các mô hình phân cụm như k-means clustering để phân cụm dữ liệu"
      ],
      "metadata": {
        "id": "7DoYgb9f-Eic"
      }
    }
  ]
}