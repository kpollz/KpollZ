{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11.Transformer_Nguyễn Viết Long",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Lý thuyết"
      ],
      "metadata": {
        "id": "P1Bh6Lv0RQrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Mô hình Transformer có ưu điểm gì hơn so với RNN ?\n",
        "\n",
        "A. Transformer có thể xử lý song song mà không cần truy hồi theo thứ tự; khắc phục được sự phụ thuộc dài hạn kém; nghĩa của từ cố định theo bối cảnh; kiến trúc có độ sâu theo các layers nên có thể transfer learning được.\n",
        "\n",
        "B. Transformer có thể xử lý song song mà không cần truy hồi theo thứ tự; khắc phục được sự phụ thuộc dài hạn kém; có thể thay đổi nghĩa của từ theo bối cảnh; kiến trúc có độ sâu theo các layers nên có thể transfer learning được.\n",
        "\n",
        "C. Transformer có thể xử lý song song mà không cần truy hồi theo thứ tự; các từ ở xa có thể diễn giải từ hiện tại; nghĩa của từ cố định theo bối cảnh; kiến trúc có độ sâu theo các layers nên có thể transfer learning được. \n",
        "\n",
        "D. Transformer có thể xử lý song song mà không cần truy hồi theo thứ tự; các từ ở xa có thể diễn giải từ hiện tại; có thể thay đổi nghĩa của từ theo bối cảnh; kiến trúc có độ sâu theo các layers nên hạn chế transfer learning. \n",
        "\n",
        "**=> Đáp án B**\n",
        "\n",
        "2) Ý nghĩa của các véc tơ query, key và value trong Transformer là gì ?\n",
        "\n",
        "A. query: véc tơ mà dựa trên đó để tính attention cho key; key: véc tơ cần được tính attention; value: véc tơ biểu diễn tương ứng với query.\n",
        "\n",
        "B. query: véc tơ mà dựa trên đó để tính attention cho key; key: véc tơ cần được tính attention; value: véc tơ biểu diễn tương ứng với key.\n",
        "\n",
        "C. query: véc tơ cần được tính attention; key: véc tơ mà dựa trên đó để tính attention cho query; value: véc tơ biểu diễn tương ứng với key.\n",
        "\n",
        "D. query: véc tơ mà dựa trên đó để tính attention cho key; key: véc tơ cần được tính attention; value: véc tơ biểu diễn tương ứng với query.\n",
        "\n",
        "**=> Đáp án C**\n",
        "\n",
        "3) Để tính ra ma trận trọng số attention giữa mỗi một cặp từ trong câu source với target thì chúng ta tính toán như thế nào?\n",
        "\n",
        "A. Lấy phân phối softmax của dot product hai ma trận Query và Key\n",
        "\n",
        "B. Lấy phân phối softmax của dot product  hai ma trận Query và Key và chia cho căn bậc hai của kích thước dimension.\n",
        "\n",
        "C. Lấy phân phối softmax của dot product  hai ma trận Query và Value \n",
        "\n",
        "D. Lấy phân phối softmax của dot product  hai ma trận Query và Value và chia cho căn bậc hai của kích thước dimension.\n",
        "\n",
        "**=> Đáp án B**\n",
        "\n",
        "4) Để tạo ra các ma trận Q (query), K (key) và V (value) trong self-attention của Encoder trong mô hình BERT thì chúng ta phải thực hiện như thế nào ?\n",
        "\n",
        "A. Từ đầu vào là ma trận X, sử dụng 3 phép chiếu linear projections thông qua việc nhân với các ma trận trọng số Wq, Wk, Wv để chiếu lên không gian mới sao cho kích thước dimension của Q phải bằng V.\n",
        "\n",
        "B. Từ đầu vào là ma trận X, sử dụng 3 mạng CNN thông qua việc nhân với các ma trận trọng số Wq, Wk, Wv để chiếu lên không gian mới sao cho kích thước dimension của Q phải bằng V.\n",
        "\n",
        "C. Các ma trận Q, K, V được khởi tạo một cách ngẫu nhiên từ trước.\n",
        "\n",
        "D. Từ đầu vào là ma trận X, Sử dụng 3 phép chiếu linear projections thông qua việc nhân với các ma trận trọng số Wq, Wk, Wv để chiếu lên không gian mới sao cho kích thước dimension của Q phải bằng K.\n",
        "\n",
        "**=> Đáp án D**\n",
        "\n",
        "5) Cơ chế multi-head attention là gì ?\n",
        "\n",
        "A. Áp dụng nhiều head Scaled dot-product attention một cách song song, head sẽ khởi tạo ra một bộ ba ma trận Q, K, V khác nhau. Sau đó thực hiện phép concatenate output của các nhánh.\n",
        "\n",
        "B. Áp dụng nhiều head Scaled dot-product attention theo thứ tự chuỗi, head sẽ khởi tạo ra một bộ ba ma trận Q, K, V khác nhau. Sau đó thực hiện phép concatenate output của các nhánh.\n",
        "\n",
        "C. Áp dụng nhiều head Scaled dot-product attention một cách song song, head sẽ khởi tạo ra một bộ ba ma trận Q, K, V khác nhau. Sau đó thực hiện cộng output của các nhánh.\n",
        "\n",
        "D. Áp dụng nhiều head Scaled dot-product attention một cách song song, head sẽ khởi tạo ra một bộ ba ma trận Q, K, V khác nhau. Sau đó thực hiện tổ hợp tuyến tính output của các nhánh.\n",
        "\n",
        "**=> Đáp án A**\n",
        "\n"
      ],
      "metadata": {
        "id": "MzRBFcFDRYKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Thực hành\n",
        "\n",
        "5) Hãy xây dựng một kiến trúc BERT bao gồm các thành phần Encoder và Decoder cho bài toán machine learning translation (1 điểm).\n",
        "\n",
        "6) Từ bộ dữ liệu về [Name Entity Recognition](https://drive.google.com/drive/u/1/folders/1rWYJR9VL7zGXkQCohS_glLEH_9ZJWxtq). Hãy phân chia tập train/test và sử dụng BERT để huấn luyện một mô hình dự báo các thực thể trong câu. Gợi ý: Có thể bắt đầu với từ khóa [NER model with pytorch](https://www.google.com/search?q=NER+model+with+pytorch&oq=NER+model+with+pytorch&aqs=chrome..69i57j0i22i30j0i390l5.8001j0j7&sourceid=chrome&ie=UTF-8) (2 điểm)\n",
        "\n",
        "7) Huấn luyện mô hình Question and Answering từ bộ dữ liệu [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) (2 điểm)"
      ],
      "metadata": {
        "id": "aEJg5s0LRUgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. BERT for machine learning translation"
      ],
      "metadata": {
        "id": "QpGt5lNHrnY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pV2Bt5dXY_Km"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}