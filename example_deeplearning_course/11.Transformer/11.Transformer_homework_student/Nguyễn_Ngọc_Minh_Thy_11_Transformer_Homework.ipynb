{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nguyễn Ngọc Minh Thy_11.Transformer_Homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1B 2C 3B 4D 5A"
      ],
      "metadata": {
        "id": "_j4jH1dO2Dq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Lý thuyết"
      ],
      "metadata": {
        "id": "P1Bh6Lv0RQrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Mô hình Transformer có ưu điểm gì hơn so với RNN ?\n",
        "\n",
        "A. Transformer có thể xử lý song song mà không cần truy hồi theo thứ tự; khắc phục được sự phụ thuộc dài hạn kém; nghĩa của từ cố định theo bối cảnh; kiến trúc có độ sâu theo các layers nên có thể transfer learning được.\n",
        "\n",
        "B. Transformer có thể xử lý song song mà không cần truy hồi theo thứ tự; khắc phục được sự phụ thuộc dài hạn kém; có thể thay đổi nghĩa của từ theo bối cảnh; kiến trúc có độ sâu theo các layers nên có thể transfer learning được.\n",
        "\n",
        "C. Transformer có thể xử lý song song mà không cần truy hồi theo thứ tự; các từ ở xa có thể diễn giải từ hiện tại; nghĩa của từ cố định theo bối cảnh; kiến trúc có độ sâu theo các layers nên có thể transfer learning được. \n",
        "\n",
        "D. Transformer có thể xử lý song song mà không cần truy hồi theo thứ tự; các từ ở xa có thể diễn giải từ hiện tại; có thể thay đổi nghĩa của từ theo bối cảnh; kiến trúc có độ sâu theo các layers nên hạn chế transfer learning. \n",
        "\n",
        "\n",
        "2) Ý nghĩa của các véc tơ query, key và value trong Transformer là gì ?\n",
        "\n",
        "A. query: véc tơ mà dựa trên đó để tính attention cho key; key: véc tơ cần được tính attention; value: véc tơ biểu diễn tương ứng với query.\n",
        "\n",
        "B. query: véc tơ mà dựa trên đó để tính attention cho key; key: véc tơ cần được tính attention; value: véc tơ biểu diễn tương ứng với key.\n",
        "\n",
        "C. query: véc tơ cần được tính attention; key: véc tơ mà dựa trên đó để tính attention cho query; value: véc tơ biểu diễn tương ứng với key.\n",
        "\n",
        "D. query: véc tơ mà dựa trên đó để tính attention cho key; key: véc tơ cần được tính attention; value: véc tơ biểu diễn tương ứng với query.\n",
        "\n",
        "\n",
        "3) Để tính ra ma trận trọng số attention giữa mỗi một cặp từ trong câu source với target thì chúng ta tính toán như thế nào?\n",
        "\n",
        "A. Lấy phân phối softmax của dot product hai ma trận Query và Key\n",
        "\n",
        "B. Lấy phân phối softmax của dot product  hai ma trận Query và Key và chia cho căn bậc hai của kích thước dimension.\n",
        "\n",
        "C. Lấy phân phối softmax của dot product  hai ma trận Query và Value \n",
        "\n",
        "D. Lấy phân phối softmax của dot product  hai ma trận Query và Value và chia cho căn bậc hai của kích thước dimension.\n",
        "\n",
        "\n",
        "4) Để tạo ra các ma trận Q (query), K (key) và V (value) trong self-attention của Encoder trong mô hình BERT thì chúng ta phải thực hiện như thế nào ?\n",
        "\n",
        "A. Từ đầu vào là ma trận X, sử dụng 3 phép chiếu linear projections thông qua việc nhân với các ma trận trọng số Wq, Wk, Wv để chiếu lên không gian mới sao cho kích thước dimension của Q phải bằng V.\n",
        "\n",
        "B. Từ đầu vào là ma trận X, sử dụng 3 mạng CNN thông qua việc nhân với các ma trận trọng số Wq, Wk, Wv để chiếu lên không gian mới sao cho kích thước dimension của Q phải bằng V.\n",
        "\n",
        "C. Các ma trận Q, K, V được khởi tạo một cách ngẫu nhiên từ trước.\n",
        "\n",
        "D. Từ đầu vào là ma trận X, Sử dụng 3 phép chiếu linear projections thông qua việc nhân với các ma trận trọng số Wq, Wk, Wv để chiếu lên không gian mới sao cho kích thước dimension của Q phải bằng K.\n",
        "\n",
        "5) Cơ chế multi-head attention là gì ?\n",
        "\n",
        "A. Áp dụng nhiều head Scaled dot-product attention một cách song song, head sẽ khởi tạo ra một bộ ba ma trận Q, K, V khác nhau. Sau đó thực hiện phép concatenate output của các nhánh.\n",
        "\n",
        "B. Áp dụng nhiều head Scaled dot-product attention theo thứ tự chuỗi, head sẽ khởi tạo ra một bộ ba ma trận Q, K, V khác nhau. Sau đó thực hiện phép concatenate output của các nhánh.\n",
        "\n",
        "C. Áp dụng nhiều head Scaled dot-product attention một cách song song, head sẽ khởi tạo ra một bộ ba ma trận Q, K, V khác nhau. Sau đó thực hiện cộng output của các nhánh.\n",
        "\n",
        "D. Áp dụng nhiều head Scaled dot-product attention một cách song song, head sẽ khởi tạo ra một bộ ba ma trận Q, K, V khác nhau. Sau đó thực hiện tổ hợp tuyến tính output của các nhánh.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzRBFcFDRYKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Thực hành\n",
        "\n",
        "5) Hãy xây dựng một kiến trúc BERT bao gồm các thành phần Encoder và Decoder cho bài toán machine learning translation (1 điểm).\n",
        "\n",
        "6) Từ bộ dữ liệu về [Name Entity Recognition](https://drive.google.com/drive/u/1/folders/1rWYJR9VL7zGXkQCohS_glLEH_9ZJWxtq). Hãy phân chia tập train/test và sử dụng BERT để huấn luyện một mô hình dự báo các thực thế trong câu. Gợi ý: Có thể bắt đầu với từ khóa [NER model with pytorch](https://www.google.com/search?q=NER+model+with+pytorch&oq=NER+model+with+pytorch&aqs=chrome..69i57j0i22i30j0i390l5.8001j0j7&sourceid=chrome&ie=UTF-8) (2 điểm)\n",
        "\n",
        "7) Huấn luyện mô hình Question and Answering từ bộ dữ liệu [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) (2 điểm)"
      ],
      "metadata": {
        "id": "aEJg5s0LRUgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Hãy xây dựng một kiến trúc BERT bao gồm các thành phần Encoder và Decoder cho bài toán machine learning translation (1 điểm)"
      ],
      "metadata": {
        "id": "vQxt85vhdEXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/vivekgohel56/Neural-machine-translation-english-to-polish/blob/master/machine%20translation%20using%20bert.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "LPTj9IzKdKVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train BERT MACHINE TRANSLATION model, we need to downgrade tensorflow 2.8.0 to 2.0.0"
      ],
      "metadata": {
        "id": "cysukXs-JzLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE78fwqHZe7A",
        "outputId": "67ec247c-4b69-43d5-c32c-1ff254f3b6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall tensorflow \n",
        "# !pip install tensorflow==2.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7TeijjbWZjgc",
        "outputId": "93e0efe9-3fae-4d29-99dd-aa6695821730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.8.0\n",
            "Uninstalling tensorflow-2.8.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.8.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.8.0\n",
            "Collecting tensorflow==2.0.0\n",
            "  Downloading tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3 MB 43 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.21.5)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.17.3)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.44.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=87f129a02d89764eb1f2531462a9832116459c555fcfb12433758f395089bdc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets\n",
        "!pip install bert-for-tf2\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "# import tensorflow_datasets as tfds\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import unicodedata\n",
        "\n",
        "import os\n",
        "\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, load_stock_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X63bBeNedAj8",
        "outputId": "d04177d2-245a-473a-f197-b710f8a9c47a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.5.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.63.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.10.0.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.4.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.21.5)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets) (3.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.0)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.63.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.config.list_physical_devices('GPU') \n",
        "# tf 2.8.0\n",
        "tf.config.experimental.list_physical_devices('GPU')\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnY99rF5dP0H",
        "outputId": "e5e5aeb9-bde0-4f41-8a06-0d61d043f2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available:  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiFn9LOxdX7w",
        "outputId": "f7c434ff-73dd-411c-de0b-ed1356cd979f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model Path"
      ],
      "metadata": {
        "id": "oV5ndvfOLfGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcyp-f8adzZU",
        "outputId": "c4fee4d2-1abc-4744-8396-3cc3ff23475c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert_config.json',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_model.ckpt.index',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "if not os.path.exists('/content/drive/My Drive/machine translation/uncased_L-12_H-768_A-12'):\n",
        "  !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "  !unzip uncased_L-12_H-768_A-12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4SZYkAXdVEC",
        "outputId": "24139b12-1164-4891-e2b0-52a8f960ae3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-08 17:24:01--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 74.125.135.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip.1’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   268MB/s    in 1.5s    \n",
            "\n",
            "2022-04-08 17:24:03 (268 MB/s) - ‘uncased_L-12_H-768_A-12.zip.1’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "replace uncased_L-12_H-768_A-12/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "replace uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  y\n",
            "y\n",
            "\n",
            "replace uncased_L-12_H-768_A-12/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "replace uncased_L-12_H-768_A-12/bert_model.ckpt.index? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "replace uncased_L-12_H-768_A-12/bert_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\""
      ],
      "metadata": {
        "id": "WYmS6OVMedFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data and Data Preprocessing"
      ],
      "metadata": {
        "id": "azNHtpfpLl3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/Colab Notebooks/BERT/pol-eng/pol.txt','r') as f:\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/BERT/vie-eng/vie.txt','r') as f:\n",
        "  data = f.read()\n",
        "#/content/drive/MyDrive/Colab Notebooks/BERT/pol-eng/pol.txt"
      ],
      "metadata": {
        "id": "0Jk-fVPReg-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uncleaned_data_list = data.strip().split('\\n')\n",
        "len(uncleaned_data_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzciStr5g82g",
        "outputId": "f4b5610f-35ef-469c-a141-8d3309dd6caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7966"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uncleaned_data_list = uncleaned_data_list[:38695]\n",
        "\n",
        "english_word = []\n",
        "vie_word = []\n",
        "cleaned_data_list = []\n",
        "for word in uncleaned_data_list:\n",
        "  english_word.append(word.split('\\t')[:-1][0])\n",
        "  vie_word.append(word.split('\\t')[:-1][1])"
      ],
      "metadata": {
        "id": "fUdXFKp1hBbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(columns=['English','Vietnamese'])\n",
        "data['English'] = english_word\n",
        "data['Viet'] = vie_word\n",
        "data.to_csv('vie_data.csv', index=False)\n",
        "data = pd.read_csv('vie_data.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "R-0xJijOhL7x",
        "outputId": "0dd3aea0-5680-4c4d-d01f-05b44854c544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  English  Vietnamese            Viet\n",
              "0    Run!         NaN           Chạy!\n",
              "1   Help!         NaN   Giúp tôi với!\n",
              "2  Go on.         NaN    Tiếp tục đi.\n",
              "3  Hello!         NaN       Chào bạn.\n",
              "4  Hurry!         NaN  Nhanh lên nào!"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-708ade0f-d933-43ac-9101-8115e3257c73\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Vietnamese</th>\n",
              "      <th>Viet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Run!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Chạy!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Giúp tôi với!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go on.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Tiếp tục đi.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Chào bạn.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hurry!</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Nhanh lên nào!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-708ade0f-d933-43ac-9101-8115e3257c73')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-708ade0f-d933-43ac-9101-8115e3257c73 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-708ade0f-d933-43ac-9101-8115e3257c73');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will take 80% data in train and remaining in test\n",
        "train = int(len(data)*0.8)\n",
        "test = len(data) - train\n",
        "train_examples, val_examples = data.iloc[0:train,:], data.iloc[train:len(data),:]\n",
        "\n",
        "english_text = train_examples['English'].values\n",
        "vie_text = train_examples['Viet'].values\n",
        "english_val_text = val_examples['English'].values\n",
        "vie_val_text = val_examples['Viet'].values\n",
        "\n",
        "\n",
        "train_examples = tf.data.Dataset.from_tensor_slices((english_text, vie_text))\n",
        "val_examples = tf.data.Dataset.from_tensor_slices((english_val_text, vie_val_text))"
      ],
      "metadata": {
        "id": "5Ndceu4Bhj9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(train_examples)\n",
        "print(train_examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuiJw0-6h6Ik",
        "outputId": "ef3c9f6b-62aa-4423-d45a-467a6d6ceded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for en, viet in train_examples.take(5):\n",
        "  print(tf.compat.as_text(en.numpy()))\n",
        "  print(tf.compat.as_text(viet.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iStZZt8biFdC",
        "outputId": "033d717a-5448-4ec5-df47-e20452484976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run!\n",
            "Chạy!\n",
            "Help!\n",
            "Giúp tôi với!\n",
            "Go on.\n",
            "Tiếp tục đi.\n",
            "Hello!\n",
            "Chào bạn.\n",
            "Hurry!\n",
            "Nhanh lên nào!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a custom subwords tokenizer from the training dataset for the decoder."
      ],
      "metadata": {
        "id": "uf1SN8JGkgjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_unicode(text):\n",
        "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        return text\n",
        "    elif isinstance(text, bytes):\n",
        "        return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    index = 0\n",
        "    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "        while True:\n",
        "            token = convert_to_unicode(reader.readline())\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip()\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "    output = []\n",
        "    for item in items:\n",
        "        output.append(vocab[item])\n",
        "    return output\n",
        "\n",
        "class FullTokenizer(object):\n",
        "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True):\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        for token in self.basic_tokenizer.tokenize(text):\n",
        "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                split_tokens.append(sub_token)\n",
        "\n",
        "        return split_tokens\n",
        "    \n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True):\n",
        "        \"\"\"Constructs a BasicTokenizer.\n",
        "    \n",
        "        Args:\n",
        "          do_lower_case: Whether to lower case the input.\n",
        "        \"\"\"\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "        text = convert_to_unicode(text)\n",
        "        text = self._clean_text(text)\n",
        "\n",
        "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "        # models. This is also applied to the English models now, but it doesn't\n",
        "        # matter since the English models were not trained on any Chinese data\n",
        "        # and generally don't have any Chinese data in them (there are Chinese\n",
        "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "        # words in the English Wikipedia.).\n",
        "        text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _tokenize_chinese_chars(self, text):\n",
        "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if self._is_chinese_char(cp):\n",
        "                output.append(\" \")\n",
        "                output.append(char)\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _is_chinese_char(self, cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "    \n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "    \n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "    \n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer.\n",
        "    \n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        text = convert_to_unicode(text)\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "X_7dZAAvkayZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.1.0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_HVaRombq0QH",
        "outputId": "3d81792d-71ae-47ec-bc98-42ecf108989e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.1.0 in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.1.2)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "  Using cached tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.21.5)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "  Using cached tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.44.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==2.1.0) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.0.1\n",
            "    Uninstalling tensorflow-estimator-2.0.1:\n",
            "      Successfully uninstalled tensorflow-estimator-2.0.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.0.2\n",
            "    Uninstalling tensorboard-2.0.2:\n",
            "      Successfully uninstalled tensorboard-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.0.0 requires tensorboard<2.1.0,>=2.0.0, but you have tensorboard 2.1.1 which is incompatible.\n",
            "tensorflow 2.0.0 requires tensorflow-estimator<2.1.0,>=2.0.0, but you have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-estimator-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow-datasets\n",
        "!pip install -U tensorflow_datasets\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITZmxRghmzi-",
        "outputId": "aec57170-ffee-4d12-87ea-6cb4434d646b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.5.2)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.10.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.63.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.0.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow_datasets) (3.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3zmDin0cYvN",
        "outputId": "088275c7-4ef1-42fe-c36c-de713265ea06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall tensorflow \n",
        "# !pip install tensorflow==2.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cm39lz-9LlRx",
        "outputId": "3ec03a2e-56ae-468e-9d1b-96cbce110677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.0.0\n",
            "Uninstalling tensorflow-2.0.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.0.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/autodiff/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/compat/v1/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/compat/v1/compat/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/compat/v2/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/compat/v2/compat/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/debugging/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/mixed_precision/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/mixed_precision/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/mlir/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v1/mlir/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/autodiff/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/compat/v1/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/compat/v1/compat/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/compat/v2/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/compat/v2/compat/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/debugging/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/experimental/tensorrt/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/mixed_precision/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/mixed_precision/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/mlir/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/compat/v2/mlir/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/debugging/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/experimental/tensorrt/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/mixed_precision/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/mixed_precision/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/mlir/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/mlir/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/core/profiler/profiler_service_monitor_result_pb2.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/core/protobuf/bfc_memory_map_pb2.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/core/protobuf/debug_event_pb2.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/core/protobuf/error_codes_pb2.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/core/protobuf/remote_tensor_handle_pb2.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/Eigen\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/IterativeLinearSolvers\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/MetisSupport\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/Sparse\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/SparseCholesky\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/SparseLU\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/src/Core/arch/AVX512/TypeCasting.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/src/Core/arch/Default/GenericPacketMathFunctionsFwd.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/src/Core/arch/Default/Half.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/src/Core/arch/Default/TypeCasting.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/src/OrderingMethods/Amd.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/src/SparseCholesky/SimplicialCholesky.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/Eigen/src/SparseCholesky/SimplicialCholesky_impl.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/absl/debugging/leak_check.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/cub_archive/LICENSE.TXT\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/curl/lib/quic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/gif/COPYING\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/gif/gif_hash.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/gif/gif_lib.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/gif/gif_lib_private.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/hwloc/static-components.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/autogen/config.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/bitmap.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/deprecated.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/diff.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/distances.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/export.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/helper.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/inlines.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/linux.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/plugins.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/rename.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/hwloc/shmem.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/private/autogen/config.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/private/components.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/private/cpuid-x86.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/private/debug.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/private/internal-components.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/private/misc.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/private/private.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/hwloc/include/private/xml.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/LICENSE.md\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jccolext.c\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jchuff.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jconfig.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jconfigint.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdcoefct.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdcol565.c\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdcolext.c\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdct.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdhuff.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdmainct.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdmaster.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdmrg565.c\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdmrgext.c\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jdsample.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jerror.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jinclude.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jmemsys.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jmorecfg.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jpeg_nbits_table.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jpegcomp.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jpegint.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jpeglib.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jsimd.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jsimddct.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jstdhuff.c\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/jversion.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/libjpeg_turbo/simd/jsimd.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nccl_archive/LICENSE.txt\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/internal/common.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/internal/dll.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/internal/headers.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/internal/sem.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/internal/wait_internal.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/aarch64/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/alpha/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/arm/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/atomic_ind/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/c++11.futex/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/c++11/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/c++11/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/c11/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/clang/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/clang/compiler.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/cygwin/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/decc/compiler.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/freebsd/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/gcc/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/gcc/compiler.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/gcc_new/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/gcc_new_debug/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/gcc_no_tls/compiler.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/gcc_old/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/lcc/compiler.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/lcc/nsync_time_init.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/linux/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/macos/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/macos/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/macos/platform_c++11_os.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/msvc/compiler.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/netbsd/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/netbsd/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/openbsd/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/osf1/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/pmax/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/posix/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/posix/nsync_time_init.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/posix/platform_c++11_os.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/ppc32/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/ppc64/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/s390x/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/shark/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/tcc/compiler.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/win32/atomic.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/win32/platform.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/win32/platform_c++11_os.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/x86_32/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/nsync/platform/x86_64/cputype.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/png/LICENSE\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/snappy/config.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/snappy/snappy-internal.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/snappy/snappy-sinksource.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/snappy/snappy-stubs-internal.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/snappy/snappy-stubs-public.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/external/snappy/snappy.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/common_runtime/gpu/gpu_init.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/framework/shared_ptr_variant.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/grappler/utils/transitive_fanin.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/lib/core/status_test_util.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/lib/random/philox_random_test_utils.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/default/posix_file_system.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/default/subprocess.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/numbers.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/rocm_rocdl_path.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/scanner.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/str_util.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/strcat.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/stringpiece.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/stringprintf.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/threadpool.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/threadpool_interface.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/platform/threadpool_options.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/protobuf/bfc_memory_map.pb.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/protobuf/debug_event.pb.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/protobuf/error_codes.pb.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/protobuf/remote_tensor_handle.pb.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/util/debug_events_writer.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/core/util/xla_config_registry.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/stream_executor/cuda/cuda_activation.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/stream_executor/cuda/cuda_diagnostics.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/stream_executor/cuda/cuda_driver.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/stream_executor/gpu/gpu_activation.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/stream_executor/gpu/gpu_diagnostics.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/stream_executor/gpu/gpu_driver.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/tensorflow/stream_executor/gpu/gpu_types.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/third_party/eigen3/Eigen/OrderingMethods\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/third_party/eigen3/Eigen/SparseCholesky\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/third_party/eigen3/Eigen/SparseCore\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/unsupported/Eigen/CXX11/src/Tensor/TensorBlockV2.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/unsupported/Eigen/CXX11/src/Tensor/TensorScanSycl.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/unsupported/Eigen/src/SpecialFunctions/BesselFunctionsArrayAPI.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/unsupported/Eigen/src/SpecialFunctions/BesselFunctionsFunctors.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/unsupported/Eigen/src/SpecialFunctions/BesselFunctionsHalf.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/unsupported/Eigen/src/SpecialFunctions/BesselFunctionsImpl.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/unsupported/Eigen/src/SpecialFunctions/BesselFunctionsPacketMath.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/include/unsupported/Eigen/src/SpecialFunctions/HipVectorCompatibility.h\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/toco/logging/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/toco/logging/gen_html.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/lite/toco/logging/toco_conversion_log_pb2.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_op_def_registry.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_checkpoint_reader.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_debug_events_writer.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_device_lib.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_events_writer.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_kernel_registry.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_py_exception_registry.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_py_func.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_quantize_training.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_scoped_annotation.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_stacktrace_handler.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_stat_summarizer.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_tfprof.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_toco_api.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_traceme.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_transform_graph.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_util_port.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_pywrap_utils.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/_tf_stack.so\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compiler/mlir/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compiler/mlir/mlir.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/debug_mnist.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/v1/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/v1/debug_errors.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/v1/debug_fibonacci.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/v1/debug_keras.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/v1/debug_mnist_v1.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/v2/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/v2/debug_fibonacci_v2.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/examples/v2/debug_mnist_v2.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/lib/check_numerics_callback.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/lib/debug_events_reader.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/lib/debug_events_writer.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/lib/dumping_callback.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/lib/dumping_callback_test_lib.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/debug/lib/op_callbacks_common.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distributed_file_utils.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/multi_process_lib.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/multi_process_runner.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/multi_process_runner_util.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/sharded_variable.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/backprop_util.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/forwardprop_util.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_callbacks.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/api/_v1/keras/layers/experimental/preprocessing/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/api/keras/layers/experimental/preprocessing/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/distribute/keras_rnn_model_correctness_test.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/image_preprocessing.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/preprocessing/text_vectorization.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/preprocessing/text_vectorization_v1.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/model_subclassing_test_util.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/utils/all_utils.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/kernel_tests/bias_op_base.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/kernel_tests/cudnn_deterministic_base.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_debug_ops.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_sendrecv_ops.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/linalg/linear_operator_permutation.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/linalg/sparse/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/linalg/sparse/conjugate_gradient.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/linalg/sparse/gen_sparse_csr_matrix_ops.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/linalg/sparse/sparse.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/linalg/sparse/sparse_csr_matrix_grad.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/linalg/sparse/sparse_csr_matrix_ops.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/parsing_config.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/structured/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/structured/structured_tensor.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/platform/analytics.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/profiler/traceme.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/experimental/loss_scaling_gradient_tape.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/py_checkpoint_reader.py\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/quantize_training.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.0.0\n",
            "Collecting tensorflow==2.0.0\n",
            "  Using cached tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3 MB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.21.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.44.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.37.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "  Using cached tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Using cached tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.1.0\n",
            "    Uninstalling tensorflow-estimator-2.1.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.1.1\n",
            "    Uninstalling tensorboard-2.1.1:\n",
            "      Successfully uninstalled tensorboard-2.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-gpu 2.1.0 requires tensorboard<2.2.0,>=2.1.0, but you have tensorboard 2.0.2 which is incompatible.\n",
            "tensorflow-gpu 2.1.0 requires tensorflow-estimator<2.2.0,>=2.1.0rc0, but you have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'tfds.features.text.SubwordTextEncoder' not support anymore so we change into \n",
        "# 'tfds.deprecated.text.SubwordTextEncoder' in this tutorial\n",
        "# https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder\n",
        "\n",
        "\n",
        "vocab_file = '/content/drive/My Drive/machine translation/vocab_vi'\n",
        "if os.path.isfile(vocab_file + '.subwords'):\n",
        "  # Load\n",
        "  tokenizer_vi = tfds.deprecated.text.SubwordTextEncoder.load_from_file(vocab_file)\n",
        "else: \n",
        "  # Build\n",
        "  tokenizer_vi = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "      (vie_text), target_vocab_size=2 ** 13)\n",
        "  tokenizer_vi.save_to_file('vocab_vi')\n",
        "\n",
        "sample_string = 'Transformer is fun.'\n",
        "tokenized_string = tokenizer_vi.encode(sample_string)\n",
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_vi.decode([ts])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KSt_YBOlDrw",
        "outputId": "7022486f-c4e7-4771-9f26-cb62398f6bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3412 ----> T\n",
            "2537 ----> ran\n",
            "3443 ----> s\n",
            "3430 ----> f\n",
            "3439 ----> o\n",
            "3442 ----> r\n",
            "3437 ----> m\n",
            "3429 ----> e\n",
            "3442 ----> r\n",
            "3360 ---->  \n",
            "3433 ----> i\n",
            "3443 ----> s\n",
            "3360 ---->  \n",
            "3430 ----> f\n",
            "3445 ----> u\n",
            "3438 ----> n\n",
            "3374 ----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The encoder uses BERT tokenizer."
      ],
      "metadata": {
        "id": "-DzaIQVTs9En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = FullTokenizer(\n",
        "    vocab_file= 'uncased_L-12_H-768_A-12/vocab.txt',\n",
        "    do_lower_case=True)\n",
        "\n",
        "test_tokens = tokenizer_en.tokenize(english_text[-1])\n",
        "test_ids = tokenizer_en.convert_tokens_to_ids(['[CLS]'] + test_tokens + ['[SEP]'])\n",
        "print(test_ids)\n",
        "print(tokenizer_en.convert_ids_to_tokens(test_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BjYCTXps-5U",
        "outputId": "27ea1953-7876-4e84-b2c9-7a6b342a0086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2129, 2106, 2017, 2113, 1045, 2359, 2000, 3713, 2000, 3419, 1029, 102]\n",
            "['[CLS]', 'how', 'did', 'you', 'know', 'i', 'wanted', 'to', 'speak', 'to', 'tom', '?', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode text into integers"
      ],
      "metadata": {
        "id": "8IcJLwKltnV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 50\n",
        "\n",
        "\n",
        "def encode(en, pl, seq_length=MAX_SEQ_LENGTH):\n",
        "  tokens_en = tokenizer_en.tokenize(tf.compat.as_text(en.numpy()))\n",
        "  lang1 = tokenizer_en.convert_tokens_to_ids(['[CLS]'] + tokens_en + ['[SEP]'])\n",
        "  if len(lang1)<seq_length:\n",
        "    lang1 = lang1 + list(np.zeros(seq_length - len(lang1), 'int32'))\n",
        "\n",
        "  lang2 = [tokenizer_vi.vocab_size] + tokenizer_vi.encode(tf.compat.as_text(pl.numpy())) + [tokenizer_vi.vocab_size + 1]\n",
        "  if len(lang2)<seq_length:\n",
        "    lang2 = lang2 + list(np.zeros(seq_length - len(lang2), 'int32'))\n",
        "\n",
        "  return lang1, lang2\n",
        "\n",
        "def tf_encode(en, vi):\n",
        "  result_en, result_vi = tf.py_function(encode, [en, vi], [tf.int32, tf.int32])\n",
        "  result_en.set_shape([None])\n",
        "  result_vi.set_shape([None])\n",
        "\n",
        "  return result_en, result_vi\n",
        "\n",
        "def filter_max_length(x, y, max_length=MAX_SEQ_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "metadata": {
        "id": "TPVRVr68tQHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 40000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_examples.map(tf_encode)\n",
        "# train_dataset = tf.io.decode_raw(train_dataset, tf.int32)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\n",
        "    BATCH_SIZE, padded_shapes=([-1], [-1]), drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "val_dataset = val_examples.map(\n",
        "    lambda en, vi: tf.py_function(encode, [en, vi], [tf.int32, tf.int32]))\n",
        "val_dataset = val_dataset.filter(filter_max_length)\n",
        "val_dataset = val_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))"
      ],
      "metadata": {
        "id": "z56Nn8BytwGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Positional encoding**\n",
        "\n",
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.\n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the similarity of their meaning and their position in the sentence, in the d-dimensional space."
      ],
      "metadata": {
        "id": "KD5Oo_UWt5vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    sines = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    cosines = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
        "\n",
        "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "ptmX9Li0t7dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Masking\n",
        "\n",
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."
      ],
      "metadata": {
        "id": "uc695pGZuITa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions so that we can add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "metadata": {
        "id": "K8-whwsVuJZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
      ],
      "metadata": {
        "id": "g7vnnpDZuPbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "metadata": {
        "id": "hWMv_miouSKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Scaled dot product attention**"
      ],
      "metadata": {
        "id": "B93VQlM2uWNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "      \n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "pGd_EZMiuXsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Multi-Head-Attention**"
      ],
      "metadata": {
        "id": "dBvd-5yfueiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention,\n",
        "                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "IDKmJN-CucGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 768))  # (batch_size, encoder_sequence, d_model)\n",
        "q = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=q, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc_IZNVnumWB",
        "outputId": "baba2606-c44f-4a66-89d8-66a1199229c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Point wise feed forward network**\n",
        "\n",
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
      ],
      "metadata": {
        "id": "f2FvwoFguqR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "metadata": {
        "id": "o-zKdJggusyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Encoder Layer and Encoder with BERT**\n",
        "\n",
        "Use BERT as encoder. The output shape is (batch_size, input_seq_len, d_model)."
      ],
      "metadata": {
        "id": "V_Z03R1duyiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_encoder(config_file):\n",
        "    with tf.io.gfile.GFile(config_file, \"r\") as reader:\n",
        "        stock_params = StockBertConfig.from_json_string(reader.read())\n",
        "        bert_params = stock_params.to_bert_model_layer_params()\n",
        "\n",
        "    return BertModelLayer.from_params(bert_params, name=\"bert\")"
      ],
      "metadata": {
        "id": "davxjIKYu1Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Decoder Layer and Decoder**\n",
        "\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1.) Masked multi-head attention (with look ahead mask and padding mask)\n",
        "\n",
        "2.) Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublaye\n",
        "\n",
        "3.) Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis."
      ],
      "metadata": {
        "id": "yasxxXwuu72L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "             look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "H4icIinQu-TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "sample_encoder_output = tf.random.uniform((64, 128, 768))\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG7abfYrvFcE",
        "outputId": "c92568c2-3dc6-4613-cdbc-2a42e317a517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decoder consists of:\n",
        "1. Output Embedding \n",
        "2. Positional Encoding\n",
        "3. N decoder layers\n",
        "\n"
      ],
      "metadata": {
        "id": "jt-Y2nOavJbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "             look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                   look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "ReTVxLU-vLH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=8000)\n",
        "\n",
        "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False, look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FNFWKp_vRhf",
        "outputId": "ebc35105-9875-489e-9853-39c6d0bf7c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Create the Transformer**\n",
        "\n",
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.\n"
      ],
      "metadata": {
        "id": "NSyRkhtRvVj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "  def __init__(self, num_layers, d_model, dff, num_heads):\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "    self.num_heads= num_heads"
      ],
      "metadata": {
        "id": "yRwV0m20vYXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from bert.loader import map_to_stock_variable_name\n",
        "# # /content/drive/My Drive/machine translation/transformer/bert\n",
        "# class Transformer(tf.keras.Model):\n",
        "#   def __init__(self, config,\n",
        "#                target_vocab_size, \n",
        "#                bert_config_file,\n",
        "#                bert_training=False, \n",
        "#                rate=0.1,\n",
        "#                name='transformer'):\n",
        "#       super(Transformer, self).__init__(name=name)\n",
        "\n",
        "#       self.encoder = build_encoder(config_file=bert_config_file)\n",
        "#       self.encoder.trainable = bert_training\n",
        "\n",
        "#       self.decoder = Decoder(config.num_layers, config.d_model, \n",
        "#                              config.num_heads, config.dff, target_vocab_size, rate)\n",
        "\n",
        "#       self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "#   def load_stock_weights(self, bert: BertModelLayer, ckpt_file):\n",
        "#       assert isinstance(bert, BertModelLayer), \"Expecting a BertModelLayer instance as first argument\"\n",
        "#       assert tf.compat.v1.train.checkpoint_exists(ckpt_file), \"Checkpoint does not exist: {}\".format(ckpt_file)\n",
        "#       ckpt_reader = tf.train.load_checkpoint(ckpt_file)\n",
        "\n",
        "#       bert_prefix = 'transformer/bert'\n",
        "\n",
        "#       weights = []\n",
        "#       for weight in bert.weights:\n",
        "#           stock_name = map_to_stock_variable_name(weight.name, bert_prefix)\n",
        "#           if ckpt_reader.has_tensor(stock_name):\n",
        "#               value = ckpt_reader.get_tensor(stock_name)\n",
        "#               weights.append(value)\n",
        "#           else:\n",
        "#               raise ValueError(\"No value for:[{}], i.e.:[{}] in:[{}]\".format(weight.name, stock_name, ckpt_file))\n",
        "#       bert.set_weights(weights)\n",
        "#       print(\"Done loading {} BERT weights from: {} into {} (prefix:{})\".format(\n",
        "#           len(weights), ckpt_file, bert, bert_prefix))\n",
        "\n",
        "#   def restore_encoder(self, bert_ckpt_file):\n",
        "#       # loading the original pre-trained weights into the BERT layer:\n",
        "#       self.load_stock_weights(self.encoder, bert_ckpt_file)\n",
        "\n",
        "#   def call(self, inp, tar, training, look_ahead_mask, dec_padding_mask):\n",
        "#       enc_output = self.encoder(inp, training=self.encoder.trainable)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "#       # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "#       dec_output, attention_weights = self.decoder(\n",
        "#           tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "#       final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "#       return final_output, attention_weights"
      ],
      "metadata": {
        "id": "CbV1FXFevfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/text/tutorials/transformer#setup\n"
      ],
      "metadata": {
        "id": "gp-n8eTlMXPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bert.loader import map_to_stock_variable_name\n",
        "# /content/drive/My Drive/machine translation/transformer/bert\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, config,\n",
        "               target_vocab_size, \n",
        "               bert_config_file,\n",
        "               bert_training=False, \n",
        "               rate=0.1,\n",
        "               name='transformer'):\n",
        "      super(Transformer, self).__init__(name=name)\n",
        "\n",
        "      self.encoder = build_encoder(config_file=bert_config_file)\n",
        "      self.encoder.trainable = bert_training\n",
        "\n",
        "      self.decoder = Decoder(config.num_layers, config.d_model, \n",
        "                             config.num_heads, config.dff, target_vocab_size, rate)\n",
        "\n",
        "      self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "  def load_stock_weights(self, bert: BertModelLayer, ckpt_file):\n",
        "      assert isinstance(bert, BertModelLayer), \"Expecting a BertModelLayer instance as first argument\"\n",
        "      assert tf.compat.v1.train.checkpoint_exists(ckpt_file), \"Checkpoint does not exist: {}\".format(ckpt_file)\n",
        "      ckpt_reader = tf.train.load_checkpoint(ckpt_file)\n",
        "\n",
        "      bert_prefix = 'transformer/bert'\n",
        "\n",
        "      weights = []\n",
        "      for weight in bert.weights:\n",
        "          stock_name = map_to_stock_variable_name(weight.name, bert_prefix)\n",
        "          if ckpt_reader.has_tensor(stock_name):\n",
        "              value = ckpt_reader.get_tensor(stock_name)\n",
        "              weights.append(value)\n",
        "          else:\n",
        "              raise ValueError(\"No value for:[{}], i.e.:[{}] in:[{}]\".format(weight.name, stock_name, ckpt_file))\n",
        "      bert.set_weights(weights)\n",
        "      print(\"Done loading {} BERT weights from: {} into {} (prefix:{})\".format(\n",
        "          len(weights), ckpt_file, bert, bert_prefix))\n",
        "\n",
        "  def restore_encoder(self, bert_ckpt_file):\n",
        "      # loading the original pre-trained weights into the BERT layer:\n",
        "      self.load_stock_weights(self.encoder, bert_ckpt_file)\n",
        "\n",
        "  def call(self, inp, tar, training,look_ahead_mask, dec_padding_mask):\n",
        "      dec_padding_mask, look_ahead_mask = self.create_masks(inp, tar)\n",
        "      enc_output = self.encoder(inp, training=self.encoder.trainable)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "      # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "      dec_output, attention_weights = self.decoder(\n",
        "          tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "      final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "      return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
        "    padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return padding_mask, look_ahead_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "J4LUcCcn7KW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Set hyperparameters**"
      ],
      "metadata": {
        "id": "C60Pbpbpvmq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_vocab_size = tokenizer_vi.vocab_size + 2\n",
        "dropout_rate = 0.15\n",
        "config = Config(num_layers=6, d_model=512, dff=1024, num_heads=8)"
      ],
      "metadata": {
        "id": "FlnT60wQvoA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_transformer = Transformer(\n",
        "#     num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
        "#     input_vocab_size=8500, target_vocab_size=8000)\n",
        "\n",
        "# temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "# temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "# fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
        "\n",
        "# fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "metadata": {
        "id": "IOEP-FzO4SVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gs_folder_bert\n",
        "# uncased_L-12_H-768_A-12\n",
        "MODEL_DIR = \"uncased_L-12_H-768_A-12\"\n",
        "bert_config_file = os.path.join(MODEL_DIR, \"bert_config.json\")\n",
        "bert_ckpt_file = os.path.join(MODEL_DIR, 'bert_model.ckpt')\n",
        "\n",
        "# with tpu_strategy.scope():\n",
        "transformer = Transformer(config=config,\n",
        "                          target_vocab_size=target_vocab_size,\n",
        "                          bert_config_file=bert_config_file)\n",
        "'''\n",
        "inp : temp_input\n",
        "tar_inp : temp_target\n",
        "fn_out\n",
        "\n",
        "'''\n",
        "inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
        "tar_inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
        "fn_out, _ = transformer(inp, tar_inp, \n",
        "                        True,\n",
        "                        look_ahead_mask=None,\n",
        "                        dec_padding_mask=None)\n",
        "print(tar_inp.shape) # (batch_size, tar_seq_len) \n",
        "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size) \n",
        "\n",
        "# init bert pre-trained weights\n",
        "transformer.restore_encoder(bert_ckpt_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toYL6Q_EvxC2",
        "outputId": "7ed6cf74-8926-49ea-e133-5df43280a785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 50)\n",
            "(64, 50, 3586)\n",
            "WARNING:tensorflow:From <ipython-input-43-f4a6bcdc1f0a>:23: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-43-f4a6bcdc1f0a>:23: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done loading 196 BERT weights from: uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fccdaab1b50> (prefix:transformer/bert)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tar_inp.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwdXOrWxQH2a",
        "outputId": "85a5a710-5c20-49f8-eb6f-c35fc2ca9800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fn_out, _ = transformer(inp, tar_inp, \n",
        "                        True,look_ahead_mask=None,\n",
        "                        dec_padding_mask=None)\n",
        "                        \n",
        "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfVwe-bXQLqC",
        "outputId": "c28ee146-7c7e-4a67-f422-0f49e57acb21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 50, 3586)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init bert pre-trained weights\n",
        "transformer.restore_encoder(bert_ckpt_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOMoC7CvdaLZ",
        "outputId": "3eee35de-11a4-4e05-9aa6-61931b85f8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-42-f4a6bcdc1f0a>:23: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-42-f4a6bcdc1f0a>:23: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done loading 196 BERT weights from: uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f550a185290> (prefix:transformer/bert)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fINlwVMhxH16",
        "outputId": "8792cdcd-0f57-48f0-d741-a5c59cb115da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (BertModelLayer)        multiple                  108890112 \n",
            "_________________________________________________________________\n",
            "decoder_1 (Decoder)          multiple                  22335488  \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             multiple                  1839618   \n",
            "=================================================================\n",
            "Total params: 133,065,218\n",
            "Trainable params: 24,175,106\n",
            "Non-trainable params: 108,890,112\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer\n",
        "\n",
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the paper."
      ],
      "metadata": {
        "id": "0quXLEdDWed2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n"
      ],
      "metadata": {
        "id": "gbozEGlYWd8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(config.d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "bPMfBw6FWzQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(config.d_model)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "jI8qTNqQW2Ln",
        "outputId": "fe63c4f9-705b-4fe7-8134-1107fa63480f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and metrics\n",
        "\n",
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ],
      "metadata": {
        "id": "So-biMoUW7_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "EYt_Nb5cW9sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def loss_function(real, pred):\n",
        "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "#     loss_ = loss_object(real, pred)\n",
        "\n",
        "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#     loss_ *= mask\n",
        "\n",
        "#     return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "90xNKCcxXCBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "u0yIKoFKQ3Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "metadata": {
        "id": "90hBvxdqXEw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and checkpointing\n",
        "\n",
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every n epochs."
      ],
      "metadata": {
        "id": "P54224hdXJSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer = Transformer(config=config,\n",
        "#                           target_vocab_size=target_vocab_size,\n",
        "#                           bert_config_file=bert_config_file)\n",
        "num_layers = 2\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.15\n",
        "# num_layers=2, d_model=512, num_heads=8, \n",
        "#                          dff=2048, target_vocab_size=8000\n",
        "# num_layers=6, d_model=512, dff=1024, num_heads=8"
      ],
      "metadata": {
        "id": "AUPbcMbHRqrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/machine translation/checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "46SY4VXQXK0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training this example uses teacher-forcing (like in the text generation tutorial). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
        "\n",
        "As the transformer predicts each word, self-attention allows it to look at the previous words in the input sequence to better predict the next word.\n",
        "\n",
        "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
      ],
      "metadata": {
        "id": "btvkDLNdXUWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return combined_mask, dec_padding_mask"
      ],
      "metadata": {
        "id": "tXNFqvikXVkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The @tf.function trace-compiles train_step into a TF graph for faster execution. The function specializes to the precise shape of the argument tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "batch sizes (the last batch is smaller), use input_signature to specify more generic shapes."
      ],
      "metadata": {
        "id": "U--qdy4DU5Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                     True,\n",
        "                                     combined_mask,\n",
        "                                     dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "metadata": {
        "id": "dlzK8GNvXc9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))\n"
      ],
      "metadata": {
        "id": "16Xna19XVHb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.19.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Yw9KYtUXfFkS",
        "outputId": "4672cb9a-7242-4895-ae81-1010b24d40d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.0.0 requires tensorboard<2.1.0,>=2.0.0, but you have tensorboard 2.1.1 which is incompatible.\n",
            "tensorflow 2.0.0 requires tensorflow-estimator<2.1.0,>=2.0.0, but you have tensorflow-estimator 2.1.0 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 11\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # inp -> chinese, tar -> english\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 500 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1,\n",
        "                                                            ckpt_save_path))\n",
        "\n",
        "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                        train_loss.result(),\n",
        "                                                        train_accuracy.result()))\n",
        "\n",
        "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "O1QSPpXxXgml",
        "outputId": "8ce7dd37-7f7d-458d-9be1-5c3628e301f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-3effcf76c32c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# inp -> chinese, tar -> english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m       args, kwargs = self._function_spec.canonicalize_function_inputs(\n\u001b[0;32m-> 2107\u001b[0;31m           *args, **kwargs)\n\u001b[0m\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m           \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_signature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m           self._flat_input_signature)\n\u001b[0m\u001b[1;32m   1651\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_convert_inputs_to_signature\u001b[0;34m(inputs, input_signature, flat_input_signature)\u001b[0m\n\u001b[1;32m   1714\u001b[0m       flatten_inputs)):\n\u001b[1;32m   1715\u001b[0m     raise ValueError(\"Python inputs incompatible with input_signature:\\n%s\" %\n\u001b[0;32m-> 1716\u001b[0;31m                      format_error_message(inputs, input_signature))\n\u001b[0m\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mneed_packing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Python inputs incompatible with input_signature:\n  inputs: (\n    tf.Tensor(\n[[ 101 3419 3480 ...    0    0    0]\n [ 101 1045 1005 ...    0    0    0]\n [ 101 1045 3046 ...    0    0    0]\n ...\n [ 101 2043 2079 ...    0    0    0]\n [ 101 2045 2024 ...    0    0    0]\n [ 101 2002 2001 ...    0    0    0]], shape=(64, 50), dtype=int32),\n    tf.Tensor(\n[[3584    2    5 ...    0    0    0]\n [3584    1   18 ...    0    0    0]\n [3584    1   13 ...    0    0    0]\n ...\n [3584    9   23 ...    0    0    0]\n [3584   73   83 ...    0    0    0]\n [3584   24   19 ...    0    0    0]], shape=(64, 50), dtype=int32))\n  input_signature: (\n    TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\n    TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate\n",
        "\n",
        "The following steps are used for evaluation:\n",
        "\n",
        "Encode the input sentence using the english tokenizer (tokenizer_en). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
        "\n",
        "The decoder input is the start token == tokenizer_pl.vocab_size.\n",
        "\n",
        "Calculate the padding masks and the look ahead masks.\n",
        "\n",
        "The decoder then outputs the predictions by looking at the encoder output and its own output (self-attention).\n",
        "\n",
        "Select the last word and calculate the argmax of that.\n",
        "\n",
        "Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
        "\n",
        "In this approach, the decoder predicts the next word based on the previous words it predicted."
      ],
      "metadata": {
        "id": "-Pm7_ZbfXgPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_en(en):\n",
        "    tokens_en = tokenizer_en.tokenize(en)\n",
        "    lang1 = tokenizer_en.convert_tokens_to_ids(['[CLS]'] + tokens_en + ['[SEP]'])\n",
        "    return lang1\n",
        "\n",
        "\n",
        "def evaluate(transformer, inp_sentence):\n",
        "    # normalize input sentence\n",
        "    inp_sentence = encode_en(inp_sentence)\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "    # as the target is english, the first word to the transformer should be the\n",
        "    # english start token.\n",
        "    decoder_input = [tokenizer_vi.vocab_size]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(MAX_SEQ_LENGTH):\n",
        "        combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input,\n",
        "                                                     output,\n",
        "                                                     False,\n",
        "                                                     combined_mask,\n",
        "                                                     dec_padding_mask)\n",
        "\n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if tf.equal(predicted_id, tokenizer_vi.vocab_size + 1):\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        # concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "\n",
        "def translate(transformer, sentence):\n",
        "    result, attention_weights = evaluate(transformer, sentence)\n",
        "\n",
        "    predicted_sentence = tokenizer_vi.decode([i for i in result\n",
        "                                              if i < tokenizer_vi.vocab_size])\n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))"
      ],
      "metadata": {
        "id": "KhOhihsIXOLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(transformer, 'I love you.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82xWY976XoQP",
        "outputId": "fb93e3c9-bd38-4ced-f767-e3381935a3f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I love you.\n",
            "Predicted translation: cắp cắp cắp cắp cắp cắp cắp cắp cắp cắp cắp cắp ssssssssssssssssssssssssssssssssssssss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(transformer, 'beautifull')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnVAF9OcYPAd",
        "outputId": "db624a8f-bfc1-47b9-a1df-eaa2151e7311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: beautifull\n",
            "Predicted translation: biệtbiệtbiệtbiệtbiệtbiệtbiệtbiệtbiệtbiệtbiệtnhậy �nhậy shỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ hỗ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(transformer, 'go')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuESsyJOYqJl",
        "outputId": "cf74a13c-5e09-4f2a-ec53-804cf1ffd435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: go\n",
            "Predicted translation: quỷ quỷ quỷ quỷ \n",
            "quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ ghétghétghétghétghétquỷ quỷ quỷ ghétghétghétghétghétbồi ghétquỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ quỷ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save weights"
      ],
      "metadata": {
        "id": "R9XJwZJ0Y5Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save_weights('/content/drive/My Drive/machine translation/bert_nmt_ckpt')"
      ],
      "metadata": {
        "id": "4rDfsENiY4ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_transformer = Transformer(config=config,\n",
        "                          target_vocab_size=target_vocab_size,\n",
        "                          bert_config_file=bert_config_file)\n",
        "  \n",
        "fn_out, _ = new_transformer(inp, tar_inp, \n",
        "                        True,\n",
        "                        look_ahead_mask=None,\n",
        "                        dec_padding_mask=None)\n",
        "new_transformer.load_weights('/content/drive/My Drive/machine translation/bert_nmt_ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0wYdOHpZFs_",
        "outputId": "efc7ff55-b7f3-47e9-98b7-e5cf067a3501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fccd7044450>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ]
}