{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVB84KrY06Iy"
      },
      "source": [
        "# I. Lý thuyết"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2Eg7BRXPtlv"
      },
      "source": [
        "1) Trong thuật toán YOLO thì biến mục tiêu bao gồm những thành phần nào ?\n",
        "\n",
        "A. Điểm xác suất tồn tại object, bounding box, phân phối xác suất classes. Trường hợp không tồn tại object thì các thành phần khác bỏ trống.\n",
        "\n",
        "B. Điểm xác suất tồn tại object, bounding box, phân phối xác suất classes.\n",
        "\n",
        "C. Điểm IoU, bounding box, phân phối xác suất classes.\n",
        "\n",
        "D. Bounding box và phân phối xác suất classes.\n",
        "\n",
        "2) Điều gì khiến cho việc dự báo bounding box trong YOLOv2 ổn định hơn so với trước đó?\n",
        "Bạn có thể tham khảo thêm tổng hợp các cải tiến của [YOLOv2](https://www.geeksforgeeks.org/yolo-v2-object-detection/)\n",
        "\n",
        "A. Điểm số confidence score của mỗi bounding box được nhân thêm với IoU(prediction, ground truth).\n",
        "\n",
        "B. Coordinator gốc (x, y ,w, h)  được chuyển sang các tham số transformed tương ứng là (tx, ty, tw, th) theo hàm sigmoid và exponential nên ổn định hơn. \n",
        "\n",
        "C. YOLOv2 được huấn luyện trên nhiều epoches hơn.\n",
        "\n",
        "D. Hàm Non Max Suppression trong Post-Processing của YOLOv2 được cải tiến.\n",
        " \n",
        "3) Ý tưởng chính của các thuật toán Keypoint Object Detection là gì?\n",
        "\n",
        "A. Dựa trên các Anchor Box được phân về mỗi một vật thể để hồi qui ra Prediction Box.\n",
        "\n",
        "B. Tìm ra các vùng Region Proposal tiềm năng chứa vật thể, crop và resize chúng và áp dụng bài toán classification trên mạng CNN để phát hiện vật thể.\n",
        "\n",
        "C. Sử dụng bản đồ heatmap để tìm ra các keypoints mà những keypoints này giúp định hình bounding boxes.\n",
        "\n",
        "D. Tập bounding boxes dự báo được ghép cặp 1:1 với ground truth boxes (đã được padding bằng phần tử rỗng để cùng số lượng).\n",
        "\n",
        "4) Trong thuật toán CenterNet, khi nào chúng ta sẽ loại bỏ một Bounding box dự báo ?\n",
        "\n",
        "A. Khi điểm xác suất của ba điểm Top-left, Center, Bottom-down quá chênh lệch.\n",
        "\n",
        "B. Khi Bounding box có IoU so với Ground Truth box thấp hơn một ngưỡng xác định.\n",
        "\n",
        "C. Khi Bounding box có kích thước quá nhỏ hoặc quá lớn.\n",
        "\n",
        "D. Khi điểm center point nằm bên ngoài vùng central region là một ô vuông trung tâm được xác định bởi hai điểm Top-left và Bottom-down.\n",
        "\n",
        "5) Để ghép cặp các điểm Top-left và Bottom-down về cùng một Object trong thuật toán CenterNet chúng ta sẽ tìm ra biểu diễn Embedding vector để so khớp chúng. Những biểu diễn này được huấn luyện trên hàm Loss function như thế nào ?\n",
        "\n",
        "A. Một hàm loss distance để đẩy hai điểm cách xa nếu khác object và hàm loss similarity để kéo hai kiểm lại gần nếu cùng object.\n",
        "\n",
        "B. Một hàm loss distance để đẩy hai điểm cách xa nếu cùng object và hàm loss similarity để kéo hai kiểm lại gần nếu khác object.\n",
        "\n",
        "C. Một hàm loss similarity để đẩy hai điểm cách xa nếu cùng object và hàm loss distance để kéo hai kiểm lại gần nếu khác object.\n",
        "\n",
        "D. Một hàm loss similarity để đẩy hai điểm cách xa nếu khác object và hàm loss distance để kéo hai kiểm lại gần nếu cùng object.\n",
        "\n",
        "6) Thuật toán DETR (detection with transformers) có gì nổi bật ?\n",
        "\n",
        "A. Thuật toán cần sử dụng ít anchor boxes hơn.\n",
        "\n",
        "B. Lần đầu sử dụng kiến trúc Transformers Encoder Decoder để dự báo một set predictions và sử dụng phương pháp Hungarian Optimization để matching và huấn luyện bounding boxes.\n",
        "\n",
        "C. Thuật toán cần sử dụng tới post processing là Non-Max-Suppression.\n",
        "\n",
        "D. Thuật toán hoàn toàn không sử dụng tới CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i53PaZ80ob8w"
      },
      "source": [
        "## Câu 1: A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YszkzQxPob8x"
      },
      "source": [
        "## Câu 2: B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07J9FdXlob8x"
      },
      "source": [
        "## Câu 3: C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATZmFeT2ob8x"
      },
      "source": [
        "## Câu 4: D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-vfuuLcob8x"
      },
      "source": [
        "## Câu 5: A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfx95ZCLob8y"
      },
      "source": [
        "## Câu 6: B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE1no0ze01ZW"
      },
      "source": [
        "# II. Thực hành"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7QBvWG20zOC"
      },
      "source": [
        "[Global Wheat Detection](https://www.kaggle.com/c/global-wheat-detection) là cuộc thi object detection trong nông nghiệp nhằm phát hiện đối tượng là các đầu bông lúa từ ảnh đầu vào của các giống lúa, điều kiện trưởng thành, màu sắc, hướng đầu, điều kiện thời tiết và sinh trưởng. Bạn được cung cấp hơn 3000 ảnh huấn luyện có nhãn từ Châu Âu, Canada và testing trên 1000 ảnh từ Úc, Nhật và Trung Quốc.\n",
        "\n",
        "Hãy thực hành huấn luyện lại các thuật toán cho cuộc thi này bao gồm:\n",
        "\n",
        "7) YOLO\n",
        "\n",
        "8) CenterNet\n",
        "\n",
        "9) DETR\n",
        "\n",
        "Gợi ý: bạn có thể tham khảo từ các notebooks trên kaggle hoặc từ các github resources liên quan tới thuật toán.\n",
        "\n",
        "10) Hãy đề xuất đề tài nghiên cứu cho project cuối khóa. Phác thảo các bước cần triển khai trong nghiên cứu đó bao gồm: nội dung, timeline ước tính và kết quả.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyFHz-qArAly"
      },
      "source": [
        "# Câu 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XHYteZgrrox"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K2RRdbWq_xy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle \n",
        "#Configuration environment\n",
        "os.environ['KAGGLE_USERNAME'] = \"danggduong\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"66f90ff72ba8f5a88b77862b5d95137d\" # key from the json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhzPYLFwrd0q"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c global-wheat-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JHfFY3--pJP"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('/content/global-wheat-detection.zip', 'r') as f:\n",
        "    f.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2cwn4B9zEVF"
      },
      "outputs": [],
      "source": [
        "#load the COCO class labels our YOLO model was trained on\n",
        "labelsPath = '/content/gdrive/MyDrive/Colab Notebooks/data/obj.names'\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tojQ-r2l0I_i"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOdAyY1J0MbU"
      },
      "outputs": [],
      "source": [
        "weightsPath = '/content/gdrive/MyDrive/Colab Notebooks/data/wheat_detection_tiny_last.weights'\n",
        "configPath = '/content/gdrive/MyDrive/Colab Notebooks/data/wheat_detection_tiny.cfg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfrjJKsC7wDo"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] loading YOLO from disk...\")\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgllQGoH7x4b"
      },
      "outputs": [],
      "source": [
        "test = '/content/gdrive/MyDrive/Colab Notebooks/data/test/'\n",
        "test_img_list = os.listdir(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmFV1NEp8eB-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(24,15))\n",
        "for i,image in enumerate(test_img_list):\n",
        "    img = cv2.imread(test + image)\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(image[:-5])\n",
        "    plt.axis('off')\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNULw8ps9aVU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,15))\n",
        "for image_id in range(len(sub)):\n",
        "    img = sub.loc[image_id,'image_id']\n",
        "    image = cv2.imread(test+img+'.jpg')\n",
        "    (H, W) = image.shape[:2]\n",
        "\n",
        "    thresh = 0.2\n",
        "    confi = 0.1\n",
        "    pred_str = ''\n",
        "    ln = net.getLayerNames()\n",
        "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    #construct a blob from the input image and then perform a forward\n",
        "    #pass of the YOLO object detector, giving us our bounding boxes and\n",
        "    #associated probabilities\n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (512,512),swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    start = time.time()\n",
        "    layerOutputs = net.forward(ln)\n",
        "    end = time.time()\n",
        "\n",
        "     #show timing information on YOLO\n",
        "    print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n",
        "\n",
        "    #initialize our lists of detected bounding boxes, confidences, and\n",
        "    #class IDs, respectively\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    classIDs = []\n",
        "\n",
        "    #loop over each of the layer outputs\n",
        "    for output in layerOutputs:\n",
        "        #loop over each of the detections\n",
        "        for detection in output:\n",
        "            #extract the class ID and confidence (i.e., probability) of\n",
        "            #the current object detection\n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            confidence = scores[classID]\n",
        "\n",
        "            #filter out weak predictions by ensuring the detected\n",
        "            #probability is greater than the minimum probability\n",
        "            if confidence > confi:\n",
        "                #scale the bounding box coordinates back relative to the\n",
        "                #size of the image, keeping in mind that YOLO actually\n",
        "                #returns the center (x, y)-coordinates of the bounding\n",
        "                #box followed by the boxes' width and height\n",
        "                box = detection[0:4] * np.array([W, H, W, H])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "                #use the center (x, y)-coordinates to derive the top and left corner of the bounding box\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "\n",
        "                #update our list of bounding box coordinates, confidences,#and class IDs\n",
        "                boxes.append([x, y, int(width), int(height)])\n",
        "                confidences.append(float(confidence))\n",
        "                classIDs.append(classID)\n",
        "    #apply non-maxima suppression to suppress weak, overlapping bounding\n",
        "    #boxes\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confi ,thresh)\n",
        "    #ensure at least one detection exists\n",
        "    if len(idxs) > 0:\n",
        "        #loop over the indexes we are keeping\n",
        "        for i in idxs.flatten():\n",
        "            #extract the bounding box coordinates\n",
        "            (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            (w, h) = (boxes[i][2], boxes[i][3])\n",
        "            pred = '{} {} {} {} {} '.format(np.round(confidences[i],1),x,y,w,h)\n",
        "            pred_str = pred_str + pred\n",
        "            \n",
        "            #draw a bounding box rectangle and label on the image\n",
        "            color = [int(c) for c in COLORS[classIDs[i]]]\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), (0,0,255), 2)\n",
        "            text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
        "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,0,255), 2)\n",
        "    \n",
        "    sub.loc[image_id,'PredictionString'] = pred_str[:-1]\n",
        "\n",
        "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(2,5,image_id+1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(img)\n",
        "    plt.axis('off')\n",
        "    \n",
        "plt.show()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeZ2RW2g_lzU"
      },
      "outputs": [],
      "source": [
        "sub.to_csv(\"sub_wheat.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_LcDOza-gP4"
      },
      "source": [
        "# Câu 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lvjqu0tPBp71"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--0a_xDhJpbh"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"\"\n",
        "input_size = 512\n",
        "IN_SCALE = 1024//input_size \n",
        "MODEL_SCALE = 4\n",
        "batch_size = 2\n",
        "model_name = \"resnet18\"\n",
        "TRAIN = True # True for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwCi-e0QMOCx"
      },
      "outputs": [],
      "source": [
        "DIR_TRAIN = '/content/gdrive/MyDrive/Colab Notebooks/data/train'\n",
        "DIR_TEST = '/content/gdrive/MyDrive/Colab Notebooks/data/test'\n",
        "\n",
        "train_df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/data/train.csv')\n",
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzn-SmDDNvzU"
      },
      "outputs": [],
      "source": [
        "train_df['x'] = -1\n",
        "train_df['y'] = -1\n",
        "train_df['w'] = -1\n",
        "train_df['h'] = -1\n",
        "\n",
        "def expand_bbox(x):\n",
        "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
        "    if len(r) == 0:\n",
        "        r = [-1, -1, -1, -1]\n",
        "    return r\n",
        "\n",
        "train_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
        "train_df.drop(columns=['bbox'], inplace=True)\n",
        "train_df['x'] = train_df['x'].astype(np.float)\n",
        "train_df['y'] = train_df['y'].astype(np.float)\n",
        "train_df['w'] = train_df['w'].astype(np.float)\n",
        "train_df['h'] = train_df['h'].astype(np.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdyINKvAN0ja"
      },
      "outputs": [],
      "source": [
        "# Split train-test\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split by unique image ids.\n",
        "image_ids = train_df['image_id'].unique()\n",
        "train_id, test_id = train_test_split(image_ids, test_size=0.2, random_state=777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_pSNZJiN4Mz"
      },
      "outputs": [],
      "source": [
        "# show image\n",
        "img_id = train_id[0]\n",
        "img = cv2.imread(os.path.join(DIR_TRAIN, img_id+\".jpg\"))\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BUupU2vOAt8"
      },
      "outputs": [],
      "source": [
        "# get targets\n",
        "target = train_df[train_df['image_id']==img_id]\n",
        "# convert targets to its center.\n",
        "try:\n",
        "    center = np.array([target[\"x\"]+target[\"w\"]//2, target[\"y\"]+target[\"h\"]//2]).T\n",
        "except:\n",
        "    center = np.array([int(target[\"x\"]+target[\"w\"]//2), int(target[\"y\"]+target[\"h\"]//2)]).T.reshape(1,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqfeIKwNN6ah"
      },
      "outputs": [],
      "source": [
        "# plot centers on image\n",
        "plt.figure(figsize=(14,14))\n",
        "plt.imshow(img)\n",
        "for x in center:\n",
        "    plt.scatter(x[0], x[1], color='red', s=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy7EmMoWOHmN"
      },
      "outputs": [],
      "source": [
        "# Make heatmaps using the utility functions from the centernet repo\n",
        "def draw_msra_gaussian(heatmap, center, sigma=2):\n",
        "  tmp_size = sigma * 6\n",
        "  mu_x = int(center[0] + 0.5)\n",
        "  mu_y = int(center[1] + 0.5)\n",
        "  w, h = heatmap.shape[0], heatmap.shape[1]\n",
        "  ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "  br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "  if ul[0] >= h or ul[1] >= w or br[0] < 0 or br[1] < 0:\n",
        "    return heatmap\n",
        "  size = 2 * tmp_size + 1\n",
        "  x = np.arange(0, size, 1, np.float32)\n",
        "  y = x[:, np.newaxis]\n",
        "  x0 = y0 = size // 2\n",
        "  g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
        "  g_x = max(0, -ul[0]), min(br[0], h) - ul[0]\n",
        "  g_y = max(0, -ul[1]), min(br[1], w) - ul[1]\n",
        "  img_x = max(0, ul[0]), min(br[0], h)\n",
        "  img_y = max(0, ul[1]), min(br[1], w)\n",
        "  heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]], g[g_y[0]:g_y[1], g_x[0]:g_x[1]])\n",
        "  \n",
        "  return heatmap\n",
        "\n",
        "def draw_dense_reg(regmap, heatmap, center, value, radius, is_offset=False):\n",
        "  diameter = 2 * radius + 1\n",
        "  gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
        "  value = np.array(value, dtype=np.float32).reshape(-1, 1, 1)\n",
        "  dim = value.shape[0]\n",
        "  reg = np.ones((dim, diameter*2+1, diameter*2+1), dtype=np.float32) * value\n",
        "  if is_offset and dim == 2:\n",
        "    delta = np.arange(diameter*2+1) - radius\n",
        "    reg[0] = reg[0] - delta.reshape(1, -1)\n",
        "    reg[1] = reg[1] - delta.reshape(-1, 1)\n",
        "  \n",
        "  x, y = int(center[0]), int(center[1])\n",
        "\n",
        "  height, width = heatmap.shape[0:2]\n",
        "  left, right = min(x, radius), min(width - x, radius + 1)\n",
        "  top, bottom = min(y, radius), min(height - y, radius + 1)\n",
        "\n",
        "  masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
        "  masked_regmap = regmap[:, y - top:y + bottom, x - left:x + right]\n",
        "  masked_gaussian = gaussian[radius - top:radius + bottom,\n",
        "                             radius - left:radius + right]\n",
        "  masked_reg = reg[:, radius - top:radius + bottom,\n",
        "                      radius - left:radius + right]\n",
        "  if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0: \n",
        "     idx = (masked_gaussian >= masked_heatmap).reshape(\n",
        "      1, masked_gaussian.shape[0], masked_gaussian.shape[1])\n",
        "  masked_regmap = (1-idx) * masked_regmap + idx * masked_reg\n",
        "  regmap[:, y - top:y + bottom, x - left:x + right] = masked_regmap\n",
        "  \n",
        "  return regmap\n",
        "\n",
        "def gaussian2D(shape, sigma=1):\n",
        "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
        "    y, x = np.ogrid[-m:m+1,-n:n+1]\n",
        "\n",
        "    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n",
        "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47f7KfG-Opfp"
      },
      "outputs": [],
      "source": [
        "def make_hm_regr(target):\n",
        "    # make output heatmap for single class\n",
        "    hm = np.zeros([input_size//MODEL_SCALE, input_size//MODEL_SCALE])\n",
        "    # make regr heatmap \n",
        "    regr = np.zeros([2, input_size//MODEL_SCALE, input_size//MODEL_SCALE])\n",
        "    \n",
        "    if len(target) == 0:\n",
        "        return hm, regr\n",
        "    \n",
        "    try:\n",
        "        center = np.array([target[\"x\"]+target[\"w\"]//2, target[\"y\"]+target[\"h\"]//2, \n",
        "                       target[\"w\"], target[\"h\"]\n",
        "                      ]).T\n",
        "    except:\n",
        "        center = np.array([int(target[\"x\"]+target[\"w\"]//2), int(target[\"y\"]+target[\"h\"]//2), \n",
        "                       int(target[\"w\"]), int(target[\"h\"])\n",
        "                      ]).T.reshape(1,4)\n",
        "    \n",
        "    # make a center point\n",
        "    # try gaussian points.\n",
        "    for c in center:\n",
        "        hm = draw_msra_gaussian(hm, [int(c[0])//MODEL_SCALE//IN_SCALE, int(c[1])//MODEL_SCALE//IN_SCALE], \n",
        "                                sigma=np.clip(c[2]*c[3]//2000, 2, 4))    \n",
        "\n",
        "    # convert targets to its center.\n",
        "    regrs = center[:, 2:]/input_size/IN_SCALE\n",
        "\n",
        "    # plot regr values to mask\n",
        "    for r, c in zip(regrs, center):\n",
        "        for i in range(-2, 3):\n",
        "            for j in range(-2, 3):\n",
        "                try:\n",
        "                    regr[:, int(c[0])//MODEL_SCALE//IN_SCALE+i, \n",
        "                         int(c[1])//MODEL_SCALE//IN_SCALE+j] = r\n",
        "                except:\n",
        "                    pass\n",
        "    regr[0] = regr[0].T; regr[1] = regr[1].T;\n",
        "    return hm, regr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIqKHYuyO7cN"
      },
      "outputs": [],
      "source": [
        "def pred2box(hm, regr, thresh=0.99):\n",
        "    # make binding box from heatmaps\n",
        "    # thresh: threshold for logits.\n",
        "        \n",
        "    # get center\n",
        "    pred = hm > thresh\n",
        "    pred_center = np.where(hm>thresh)\n",
        "    # get regressions\n",
        "    pred_r = regr[:,pred].T\n",
        "\n",
        "    # wrap as boxes\n",
        "    # [xmin, ymin, width, height]\n",
        "    # size as original image.\n",
        "    boxes = []\n",
        "    scores = hm[pred]\n",
        "    for i, b in enumerate(pred_r):\n",
        "        arr = np.array([pred_center[1][i]*MODEL_SCALE-b[0]*input_size//2, pred_center[0][i]*MODEL_SCALE-b[1]*input_size//2, \n",
        "                      int(b[0]*input_size), int(b[1]*input_size)])\n",
        "        arr = np.clip(arr, 0, input_size)\n",
        "          # filter \n",
        "        #if arr[0]<0 or arr[1]<0 or arr[0]>input_size or arr[1]>input_size:\n",
        "            #pass\n",
        "        boxes.append(arr)\n",
        "    return np.asarray(boxes), scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWL157lTPEzU"
      },
      "outputs": [],
      "source": [
        "# functions for plotting results\n",
        "def showbox(img, hm, regr, thresh=0.9):\n",
        "    boxes, _ = pred2box(hm, regr, thresh=thresh)\n",
        "    print(\"preds:\",boxes.shape)\n",
        "    sample = img\n",
        "\n",
        "    for box in boxes:\n",
        "        # upper-left, lower-right\n",
        "        cv2.rectangle(sample,\n",
        "                      (int(box[0]), int(box[1]+box[3])),\n",
        "                      (int(box[0]+box[2]), int(box[1])),\n",
        "                      (220, 0, 0), 3)\n",
        "    return sample\n",
        "def showgtbox(img, hm, regr, thresh=0.9):\n",
        "    boxes, _ = pred2box(hm, regr, thresh=thresh)\n",
        "    print(\"GT boxes:\", boxes.shape)\n",
        "    sample = img\n",
        "\n",
        "    for box in boxes:\n",
        "        cv2.rectangle(sample,\n",
        "                      (int(box[0]), int(box[1]+box[3])),\n",
        "                      (int(box[0]+box[2]), int(box[1])),\n",
        "                      (0, 220, 0), 3)\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXq9srLGPL-O"
      },
      "outputs": [],
      "source": [
        "img_id = train_id[0]\n",
        "img = cv2.imread(os.path.join(DIR_TRAIN, img_id+\".jpg\"))\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img = cv2.resize(img, (input_size, input_size))\n",
        "sample = img\n",
        "\n",
        "# get labels\n",
        "target = train_df[train_df['image_id']==img_id]\n",
        "\n",
        "# convert target to heatmaps\n",
        "hm, regr = make_hm_regr(target)\n",
        "\n",
        "# get boxes\n",
        "boxes, _ = pred2box(hm, regr)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "sample = showbox(sample, hm, regr, 0.99)\n",
        "plt.imshow(sample)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxfNwzzQPZP9"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self):\n",
        "        self.mean=[0.485, 0.456, 0.406]\n",
        "        self.std=[0.229, 0.224, 0.225]\n",
        "        self.norm = transforms.Normalize(self.mean, self.std)\n",
        "    def __call__(self, image):\n",
        "        image = image.astype(np.float32)/255\n",
        "        axis = (0,1)\n",
        "        image -= self.mean\n",
        "        image /= self.std\n",
        "        return image\n",
        "\n",
        "# pool duplicates\n",
        "def pool(data):\n",
        "    stride = 3\n",
        "    for y in np.arange(1,data.shape[1]-1, stride):\n",
        "        for x in np.arange(1, data.shape[0]-1, stride):\n",
        "            a_2d = data[x-1:x+2, y-1:y+2]\n",
        "            max = np.asarray(np.unravel_index(np.argmax(a_2d), a_2d.shape))            \n",
        "            for c1 in range(3):\n",
        "                for c2 in range(3):\n",
        "                    #print(c1,c2)\n",
        "                    if not (c1== max[0] and c2 == max[1]):\n",
        "                        data[x+c1-1, y+c2-1] = -1\n",
        "    return data\n",
        "\n",
        "class WheatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_id, labels, transform=None):\n",
        "        self.img_id = img_id\n",
        "        self.labels = labels\n",
        "        if transform:\n",
        "            self.transform = transform\n",
        "        self.normalize = Normalize()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_id)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(os.path.join(DIR_INPUT,\"train\", self.img_id[idx]+\".jpg\"))\n",
        "        img = cv2.resize(img, (input_size, input_size))\n",
        "        img = self.normalize(img)\n",
        "        img = img.transpose([2,0,1])\n",
        "        target = self.labels[self.labels['image_id']==self.img_id[idx]]\n",
        "        hm, regr = make_hm_regr(target)\n",
        "        return img, hm, regr\n",
        "\n",
        "# Submission\n",
        "class WheatDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.img_id = os.listdir(self.image_dir)\n",
        "        if transform:\n",
        "            self.transform = transform\n",
        "        self.normalize = Normalize()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_id)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(os.path.join(self.image_dir, self.img_id[idx]))\n",
        "        img = cv2.resize(img, (input_size, input_size))\n",
        "        img = self.normalize(img)\n",
        "        img = img.transpose([2,0,1])\n",
        "        return img, self.img_id[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol7TohbmPmRF"
      },
      "outputs": [],
      "source": [
        "DIR_INPUT = '/content/gdrive/MyDrive/Colab Notebooks/data'\n",
        "traindataset = WheatDataset(train_id, train_df)\n",
        "valdataset = WheatDataset(test_id, train_df)\n",
        "testdataset = WheatDatasetTest(DIR_TEST)\n",
        "\n",
        "# Test dataset\n",
        "img, hm, regr = traindataset[0]\n",
        "plt.imshow(img.transpose([1,2,0]))\n",
        "plt.show()\n",
        "img.std()\n",
        "plt.imshow(hm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q65tUPQaP3lm"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(traindataset,batch_size=batch_size,shuffle=True, num_workers=0)\n",
        "val_loader = torch.utils.data.DataLoader(valdataset,batch_size=batch_size,shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(testdataset,batch_size=batch_size,shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv3NRKAdP-Ex"
      },
      "source": [
        "DEFINE CENTERNET MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_cm-cBhP8jl"
      },
      "outputs": [],
      "source": [
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "        \n",
        "    def forward(self, x1, x2=None):\n",
        "        x1 = self.up(x1)\n",
        "        if x2 is not None:\n",
        "            x = torch.cat([x2, x1], dim=1)\n",
        "            # input is CHW\n",
        "            diffY = x2.size()[2] - x1.size()[2]\n",
        "            diffX = x2.size()[3] - x1.size()[3]\n",
        "            x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                            diffY // 2, diffY - diffY//2))\n",
        "        else:\n",
        "            x = x1\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class centernet(nn.Module):\n",
        "    def __init__(self, n_classes=1, model_name=\"resnet18\"):\n",
        "        super(centernet, self).__init__()\n",
        "        # create backbone.\n",
        "        basemodel = torchvision.models.resnet18(pretrained=False) # turn this on for training\n",
        "        basemodel = nn.Sequential(*list(basemodel.children())[:-2])\n",
        "        # set basemodel\n",
        "        self.base_model = basemodel\n",
        "        \n",
        "        if model_name == \"resnet34\" or model_name==\"resnet18\":\n",
        "            num_ch = 512\n",
        "        else:\n",
        "            num_ch = 2048\n",
        "        \n",
        "        self.up1 = up(num_ch, 512)\n",
        "        self.up2 = up(512, 256)\n",
        "        self.up3 = up(256, 256)\n",
        "        # output classification\n",
        "        self.outc = nn.Conv2d(256, n_classes, 1)\n",
        "        # output residue\n",
        "        self.outr = nn.Conv2d(256, 2, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        x = self.base_model(x)\n",
        "        \n",
        "        # Add positional info        \n",
        "        x = self.up1(x)\n",
        "        x = self.up2(x)\n",
        "        x = self.up3(x)\n",
        "        outc = self.outc(x)\n",
        "        outr = self.outr(x)\n",
        "        return outc, outr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl0UnugCQPHV"
      },
      "outputs": [],
      "source": [
        "model = centernet()\n",
        "# Check if it runs correctly\n",
        "model(torch.rand(1,3,512,512))[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP68IWUmQSQT"
      },
      "outputs": [],
      "source": [
        "# Gets the GPU if there is one, otherwise the cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPE3PyVZQYBa"
      },
      "outputs": [],
      "source": [
        "for id in range(10):\n",
        "    img, hm_gt, regr_gt = valdataset[id]\n",
        "    img = torch.from_numpy(img)\n",
        "    with torch.no_grad():\n",
        "        hm, regr = model(img.to(device).float().unsqueeze(0))\n",
        "\n",
        "    \n",
        "    hm = hm.cpu().numpy().squeeze(0).squeeze(0)\n",
        "    regr = regr.cpu().numpy().squeeze(0)\n",
        "\n",
        "    # show image\n",
        "    img_id = test_id[id]\n",
        "    img = cv2.imread(os.path.join(DIR_TRAIN, img_id+\".jpg\"))\n",
        "    img = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), (input_size, input_size))\n",
        "\n",
        "    # get boxes\n",
        "    hm = torch.sigmoid(torch.from_numpy(hm)).numpy()\n",
        "    hm = pool(hm)\n",
        "    plt.imshow(hm>0.6)\n",
        "    plt.show()\n",
        "    sample = showbox(img, hm, regr, 0.6)\n",
        "    \n",
        "    # show gt\n",
        "    sample = showgtbox(sample, hm_gt, regr_gt, 0.99)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "    plt.imshow(sample)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU-ovIofREmy"
      },
      "source": [
        "# Câu 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDOfdZ2dh8VQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "#Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXpfLTTC8ZW6"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations==0.4.6\n",
        "import albumentations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNIFloRv71z6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/MyDrive/Colab Notebooks/data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3t_pNJQ7dlG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "#CV\n",
        "import cv2\n",
        "\n",
        "from detr.models.matcher import HungarianMatcher\n",
        "from detr.models.detr import SetCriterion\n",
        "#################################################################\n",
        "\n",
        "#Albumenatations\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "#Glob\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pled7ceDBLeG"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENuhQuXZBN8d"
      },
      "outputs": [],
      "source": [
        "n_folds = 5\n",
        "seed = 42\n",
        "num_classes = 2\n",
        "num_queries = 100\n",
        "null_class_coef = 0.5\n",
        "BATCH_SIZE = 8\n",
        "LR = 2e-5\n",
        "EPOCHS = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1ekzclaBQtN"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVv8OXo9C9r-"
      },
      "source": [
        "Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wFaEQ-cBTcy"
      },
      "outputs": [],
      "source": [
        "marking = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/data/train.csv')\n",
        "\n",
        "bboxs = np.stack(marking['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
        "for i, column in enumerate(['x', 'y', 'w', 'h']):\n",
        "    marking[column] = bboxs[:,i]\n",
        "marking.drop(columns=['bbox'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHxAezDNBXQ1"
      },
      "outputs": [],
      "source": [
        "# Creating Folds\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
        "\n",
        "df_folds = marking[['image_id']].copy()\n",
        "df_folds.loc[:, 'bbox_count'] = 1\n",
        "df_folds = df_folds.groupby('image_id').count()\n",
        "df_folds.loc[:, 'source'] = marking[['image_id', 'source']].groupby('image_id').min()['source']\n",
        "df_folds.loc[:, 'stratify_group'] = np.char.add(\n",
        "    df_folds['source'].values.astype(str),\n",
        "    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n",
        ")\n",
        "df_folds.loc[:, 'fold'] = 0\n",
        "\n",
        "for fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n",
        "    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAwMhU1CC5cl"
      },
      "source": [
        "Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0v6eTuUBjXH"
      },
      "outputs": [],
      "source": [
        "def get_train_transforms():\n",
        "    return A.Compose([A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),\n",
        "                               \n",
        "                      A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)],p=0.9),\n",
        "                      \n",
        "                      A.ToGray(p=0.01),\n",
        "                      \n",
        "                      A.HorizontalFlip(p=0.5),\n",
        "                      \n",
        "                      A.VerticalFlip(p=0.5),\n",
        "                      \n",
        "                      A.Resize(height=512, width=512, p=1),\n",
        "                      \n",
        "                      A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
        "                      \n",
        "                      ToTensorV2(p=1.0)],\n",
        "                     \n",
        "                      p=1.0,\n",
        "                     \n",
        "                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n",
        "                      )\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([A.Resize(height=512, width=512, p=1.0),\n",
        "                      ToTensorV2(p=1.0)], \n",
        "                      p=1.0, \n",
        "                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JujGZ71Czwj"
      },
      "source": [
        "Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ4ptaSrBp3O"
      },
      "outputs": [],
      "source": [
        "DIR_TRAIN = '/content/gdrive/MyDrive/Colab Notebooks/data/train'\n",
        "\n",
        "class WheatDataset(Dataset):\n",
        "    def __init__(self,image_ids,dataframe,transforms=None):\n",
        "        self.image_ids = image_ids\n",
        "        self.df = dataframe\n",
        "        self.transforms = transforms\n",
        "        \n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        image_id = self.image_ids[index]\n",
        "        records = self.df[self.df['image_id'] == image_id]\n",
        "        \n",
        "        image = cv2.imread(f'{DIR_TRAIN}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        \n",
        "        # DETR takes in data in coco format \n",
        "        boxes = records[['x', 'y', 'w', 'h']].values\n",
        "        \n",
        "        #Area of bb\n",
        "        area = boxes[:,2]*boxes[:,3]\n",
        "        area = torch.as_tensor(area, dtype=torch.float32)\n",
        "        \n",
        "        # AS pointed out by PRVI It works better if the main class is labelled as zero\n",
        "        labels =  np.zeros(len(boxes), dtype=np.int32)\n",
        "\n",
        "        \n",
        "        if self.transforms:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': boxes,\n",
        "                'labels': labels\n",
        "            }\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "            boxes = sample['bboxes']\n",
        "            labels = sample['labels']\n",
        "            \n",
        "            \n",
        "        #Normalizing BBOXES\n",
        "            \n",
        "        _,h,w = image.shape\n",
        "        boxes = A.augmentations.bbox_utils.normalize_bboxes(sample['bboxes'],rows=h,cols=w)\n",
        "        target = {}\n",
        "        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n",
        "        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "        target['area'] = area\n",
        "        \n",
        "        return image, target, image_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljPhSFVrCrvY"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exxFDQPcCFjD"
      },
      "outputs": [],
      "source": [
        "class DETRModel(nn.Module):\n",
        "    def __init__(self,num_classes,num_queries):\n",
        "        super(DETRModel,self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_queries = num_queries\n",
        "        \n",
        "        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "        self.in_features = self.model.class_embed.in_features\n",
        "        \n",
        "        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n",
        "        self.model.num_queries = self.num_queries\n",
        "        \n",
        "    def forward(self,images):\n",
        "        return self.model(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWM0kDBECp4v"
      },
      "source": [
        "Matcher and Bipartite Matching Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WobJQzbjCH7a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "code taken from github repo detr , 'code present in engine.py'\n",
        "'''\n",
        "matcher = HungarianMatcher()\n",
        "weight_dict = weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
        "losses = ['labels', 'boxes', 'cardinality']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rswyJdVPCm87"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYeUxz-rCLrZ"
      },
      "outputs": [],
      "source": [
        "def train_fn(data_loader,model,criterion,optimizer,device,scheduler,epoch):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    \n",
        "    summary_loss = AverageMeter()\n",
        "    \n",
        "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
        "    \n",
        "    for step, (images, targets, image_ids) in enumerate(tk0):\n",
        "        \n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "\n",
        "        output = model(images)\n",
        "        \n",
        "        loss_dict = criterion(output, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "        \n",
        "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "        summary_loss.update(losses.item(),BATCH_SIZE)\n",
        "        tk0.set_postfix(loss=summary_loss.avg)\n",
        "        \n",
        "    return summary_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th62hFH7CkNT"
      },
      "source": [
        "Eval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6bj6B6KCQTm"
      },
      "outputs": [],
      "source": [
        "def eval_fn(data_loader, model,criterion, device):\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "    summary_loss = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
        "        for step, (images, targets, image_ids) in enumerate(tk0):\n",
        "            \n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            output = model(images)\n",
        "        \n",
        "            loss_dict = criterion(output, targets)\n",
        "            weight_dict = criterion.weight_dict\n",
        "        \n",
        "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "            summary_loss.update(losses.item(),BATCH_SIZE)\n",
        "            tk0.set_postfix(loss=summary_loss.avg)\n",
        "    \n",
        "    return summary_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfZOzbsWChvK"
      },
      "source": [
        "Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDewH6yrCVZD"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9rq6KM0CZdO"
      },
      "outputs": [],
      "source": [
        "def run(fold):\n",
        "    \n",
        "    df_train = df_folds[df_folds['fold'] != fold]\n",
        "    df_valid = df_folds[df_folds['fold'] == fold]\n",
        "    \n",
        "    train_dataset = WheatDataset(\n",
        "    image_ids=df_train.index.values,\n",
        "    dataframe=marking,\n",
        "    transforms=get_train_transforms()\n",
        "    )\n",
        "\n",
        "    valid_dataset = WheatDataset(\n",
        "    image_ids=df_valid.index.values,\n",
        "    dataframe=marking,\n",
        "    transforms=get_valid_transforms()\n",
        "    )\n",
        "    \n",
        "    train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    valid_data_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        "    )\n",
        "    \n",
        "    device = torch.device('cuda')\n",
        "    model = DETRModel(num_classes=num_classes,num_queries=num_queries)\n",
        "    model = model.to(device)\n",
        "    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef = null_class_coef, losses=losses)\n",
        "    criterion = criterion.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    \n",
        "    best_loss = 10**5\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_fn(train_data_loader, model,criterion, optimizer,device,scheduler=None,epoch=epoch)\n",
        "        valid_loss = eval_fn(valid_data_loader, model,criterion, device)\n",
        "        \n",
        "        print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1,train_loss.avg,valid_loss.avg))\n",
        "        \n",
        "        if valid_loss.avg < best_loss:\n",
        "            best_loss = valid_loss.avg\n",
        "            print('Best model found for Fold {} in Epoch {}........Saving Model'.format(fold,epoch+1))\n",
        "            torch.save(model.state_dict(), f'detr_best_{fold}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr_Cm5oJDEu-"
      },
      "outputs": [],
      "source": [
        "run(fold=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwm8iP9mGE2F"
      },
      "source": [
        "Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r11N9p2FGLig"
      },
      "outputs": [],
      "source": [
        "def view_sample(df_valid,model,device):\n",
        "    valid_dataset = WheatDataset(image_ids=df_valid.index.values,\n",
        "                                 dataframe=marking,\n",
        "                                 transforms=get_valid_transforms()\n",
        "                                )\n",
        "     \n",
        "    valid_data_loader = DataLoader(\n",
        "                                    valid_dataset,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    shuffle=False,\n",
        "                                   num_workers=4,\n",
        "                                   collate_fn=collate_fn)\n",
        "    \n",
        "    images, targets, image_ids = next(iter(valid_data_loader))\n",
        "    _,h,w = images[0].shape # for de normalizing images\n",
        "    \n",
        "    images = list(img.to(device) for img in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "    \n",
        "    boxes = targets[0]['boxes'].cpu().numpy()\n",
        "    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n",
        "    sample = images[0].permute(1,2,0).cpu().numpy()\n",
        "    \n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        \n",
        "    outputs = [{k: v.to(cpu_device) for k, v in outputs.items()}]\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
        "\n",
        "    for box in boxes:\n",
        "        cv2.rectangle(sample,\n",
        "                  (box[0], box[1]),\n",
        "                  (box[2]+box[0], box[3]+box[1]),\n",
        "                  (220, 0, 0), 1)\n",
        "        \n",
        "\n",
        "    oboxes = outputs[0]['pred_boxes'][0].detach().cpu().numpy()\n",
        "    oboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(oboxes,h,w)]\n",
        "    prob   = outputs[0]['pred_logits'][0].softmax(1).detach().cpu().numpy()[:,0]\n",
        "    for box,p in zip(oboxes,prob):\n",
        "        \n",
        "        if p >0.5:\n",
        "            color = (0,0,220) #if p>0.5 else (0,0,0)\n",
        "            cv2.rectangle(sample,\n",
        "                  (box[0], box[1]),\n",
        "                  (box[2]+box[0], box[3]+box[1]),\n",
        "                  color, 1)\n",
        "    \n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5tF8q5bGaam"
      },
      "outputs": [],
      "source": [
        "model = DETRModel(num_classes=num_classes,num_queries=num_queries)\n",
        "model.load_state_dict(torch.load(\"./detr_best_0.pth\"))\n",
        "view_sample(df_folds[df_folds['fold'] == 0],model=model,device=torch.device('cuda'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFdNQMYvs4Mp"
      },
      "source": [
        "# Câu 10"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "8.OneStatesOD_Homework_DuongNguyen .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}