{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2HWfZe5bT7u"
      },
      "source": [
        "# I. Lý thuyết"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6_oiMLtbZd2"
      },
      "source": [
        "1) Mạng tích chập CNN sẽ được thực hiện như thế nào?\n",
        "\n",
        "A. Phân chia feature map thành các ma trận theo chiều channel và thực hiện tích chập giữa mỗi ma trận kernel với từng chiều.\n",
        "\n",
        "B. Thực hiện phép nhân ma trận kernel với chuyển vị của ma trận local receptive field để thu được giá trị feature map output.\n",
        "\n",
        "C. Tính trung bình các phân tử trên một ma trận local receptive field.\n",
        "\n",
        "D. Di chuyển vùng local receptive field theo chiều từ trái sang phải và từ trên xuống dưới và nhân tích chập với kernel để thu được ma trận feature map.\n",
        "\n",
        "2) Thực hiện phép nhân tích chập giữa ma trận đầu vào kích thước width và height lần lượt là $(W, H)$ với bộ lọc kích thước $(F, F)$ và bước nhảy $S$ thì thu được ma trận đầu ra với kích thước width và height $(W', H')$ là bao nhiêu? Hãy lựa chọn công thức tổng quát nhất.\n",
        "\n",
        "A. $W' =⌊ \\frac{W-F}{S} ⌋ + 1;~ H' =⌊ \\frac{H-F}{S} ⌋ + 1$\n",
        "\n",
        "B. $W' =\\lceil \\frac{W-F}{S} \\rceil + 1;~ H' = \\lceil \\frac{H-F}{S} \\rceil + 1$ \n",
        "\n",
        "C. $W' =\\lfloor \\frac{W-F+2P}{S} \\rfloor + 1;~ H' = \\lfloor \\frac{H-F+2P}{S} \\rfloor + 1$ với $P$ là số lượng các véc tơ 0 padding ở bên ngoài đều hai phía.\n",
        "\n",
        "D. $W' =\\lceil \\frac{W-F+P}{S} \\rceil + 1;~ H' = \\lceil \\frac{H-F+P}{S} \\rceil + 1$ với $P$ là số lượng các véc tơ 0 padding ở bên ngoài đều hai phía.\n",
        "\n",
        "3) Mạng CNN sẽ học được những gì qua các layers?\n",
        "\n",
        "A. Tại những layers đầu mỗi channels sẽ giúp phân biệt một loại đặc trưng, của vật thể. Những đặc trưng này sau đó được trải phẳng và đưa vào mạng MLP để phân loại vật thể.\n",
        "\n",
        "B. Những layers cuối cùng sẽ zoom vào từng chi tiết của vật thể để phân loại vật thể.\n",
        "\n",
        "C. Tại những layers đầu mạng sẽ học được các đặc trưng chung như các edge dọc, ngang, chéo,.... Các đặc trưng chi tiết và tổng quát giúp nhận diện vật thể được học tại những layers cuối cùng.\n",
        "\n",
        "D. Các layers đầu tiên sẽ tập trung vào các chi tiết bộ phận của vật thể.\n",
        "\n",
        "4) Kiến trúc chung của một mạng CNN là gì?\n",
        "\n",
        "A. _[Conv -> BatchNorm -> Activation -> Maxpooling] x n_\n",
        "\n",
        "B. _[BatchNorm -> Conv -> Activation -> Maxpooling] x n_ \n",
        "\n",
        "C. _[Conv -> BatchNorm -> Maxpooling -> Activation -> Maxpooling] x n_ \n",
        "\n",
        "D. _[Conv -> BatchNorm -> Maxpooling -> Activation -> Maxpooling] x n -> Flatten -> [FullyConnected -> Activation] x m -> Softmax_ \n",
        "\n",
        "5) Thông thường đặc điểm kích thước feature map của mạng neural sẽ như thế nào ?\n",
        "\n",
        "A. Kích thước feature map tăng dần gấp đôi qua thời gian.\n",
        "\n",
        "B. Kích thước feature map sẽ duy trì không đổi qua thời gian.\n",
        "\n",
        "C. Kích thước mạng sẽ giảm dần gấp đôi sau mỗi một lần downsampling và sau đó tăng dần gấp đôi sau mỗi lần upsampling để khôi phục về kích thước ảnh input.\n",
        "\n",
        "D. Kích thước mạng thường giảm gấp đôi sau một lần downsampling và số lượng các filters tăng dần."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TczHxnRt0ZL"
      },
      "source": [
        "1) D\n",
        "<br/>\n",
        "2) C\n",
        "<br/>\n",
        "3) C\n",
        "<br/>\n",
        "4) D\n",
        "<br/>\n",
        "5) D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4U8kdaxbW0e"
      },
      "source": [
        "# II. Thực hành\n",
        "\n",
        "6) Thực hiện xây dựng và huấn luyện một mạng CNN ngẫu nhiên trên bộ dữ liệu [Dog and Cat](https://www.kaggle.com/c/dog-vs-cat-classification/data). Lưu ý cần thực hiện [chuẩn hóa dữ liệu](https://github.com/pytorch/examples/issues/112) đối với bộ dữ liệu ImageNet trước khi huấn luyện.\n",
        "\n",
        "7) Grid Search kiến trúc CNN dựa trên việc tổng quát hóa một thiết kế câu 6.\n",
        "\n",
        "8) Huấn luyện lại mô hình bằng kiến trúc ResNet và MobileNet. Đánh giá chi phí tính toán và submit kết quả độ chính xác trên các ảnh thuộc folder test.\n",
        "\n",
        "9) Thử nghiệm các phương pháp Augmentation như Flip, Rotation, Random Crop, Bright Contrast để cải thiện kết quả mô hình.\n",
        "\n",
        "10) Lập bảng kết quả các thử nghiệm đã sử dụng."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydC-amoGvTjc"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c dog-vs-cat-classification\n",
        "!unzip dog-vs-cat-classification.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFJ9EjG9wWpZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from  tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uic80jd0-_gw"
      },
      "outputs": [],
      "source": [
        "train_folder = './train/train'\n",
        "test_folder = './test/test'\n",
        "pretrained_size = (224,224)\n",
        "batch_size = 30\n",
        "\n",
        "print(\"Getting Data...\")\n",
        "train_gen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n",
        "                             validation_split=0.3) # hold back 30% of the images for validation\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Preparing training dataset...\")\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "validation_generator =  train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "print(\"Preparing test dataset...\")\n",
        "test_generator =  test_gen.flow_from_directory(\n",
        "    test_folder,\n",
        "    shuffle=False,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    # batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "classnames = list(train_generator.class_indices.keys())\n",
        "print(\"class names: \", classnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyHMtrEGAsUp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Dropout, MaxPool2D, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-DeUNStEwbO"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Input(shape=train_generator.image_shape),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(len(classnames), activation='softmax')\n",
        "])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_LEnCILF2po"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPqOYoLqIAhz"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUCKh5ZeKKhS"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.trial import TrialState"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhRVQupDKQ4l"
      },
      "outputs": [],
      "source": [
        "def create_model(trial):\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        num_hidden = trial.suggest_int(\"n_units_l{}\".format(i), 32, 512, log=True)\n",
        "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.6)\n",
        "        model.add(Conv2D(num_hidden, (3, 3), activation='relu'))\n",
        "        model.add(Dropout(p))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(len(classnames), activation='softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxgI-NETMUC5"
      },
      "outputs": [],
      "source": [
        "def create_optimizer(trial):\n",
        "    kwargs = {}\n",
        "    optimizer_options = [\"RMSprop\", \"Adam\", \"SGD\"]\n",
        "    optimizer_selected = trial.suggest_categorical(\"optimizer\", optimizer_options)\n",
        "    if optimizer_selected == \"RMSprop\":\n",
        "        kwargs[\"learning_rate\"] = trial.suggest_float(\n",
        "            \"rmsprop_learning_rate\", 1e-5, 1e-1, log=True\n",
        "        )\n",
        "        kwargs[\"decay\"] = trial.suggest_float(\"rmsprop_decay\", 0.85, 0.99)\n",
        "        kwargs[\"momentum\"] = trial.suggest_float(\"rmsprop_momentum\", 1e-5, 1e-1, log=True)\n",
        "    elif optimizer_selected == \"Adam\":\n",
        "        kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-5, 1e-1, log=True)\n",
        "    elif optimizer_selected == \"SGD\":\n",
        "        kwargs[\"learning_rate\"] = trial.suggest_float(\n",
        "            \"sgd_opt_learning_rate\", 1e-5, 1e-1, log=True\n",
        "        )\n",
        "        kwargs[\"momentum\"] = trial.suggest_float(\"sgd_opt_momentum\", 1e-5, 1e-1, log=True)\n",
        "\n",
        "    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHPH0_XmMjJs"
      },
      "outputs": [],
      "source": [
        "def learn(model, optimizer, dataset, mode=\"eval\"):\n",
        "    accuracy = tf.metrics.Accuracy(\"accuracy\", dtype=tf.float32)\n",
        "\n",
        "    for batch, (images, labels) in enumerate(dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(images, training=(mode == \"train\"))\n",
        "            loss_value = tf.reduce_mean(\n",
        "                tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "            )\n",
        "            if mode == \"eval\":\n",
        "                accuracy(\n",
        "                    tf.argmax(logits, axis=1, output_type=tf.int64), tf.cast(labels, tf.int64)\n",
        "                )\n",
        "            else:\n",
        "                grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    if mode == \"eval\":\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdpNSTBYS8Oz"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "def objective(trial):\n",
        "    # Build model and optimizer.\n",
        "    model = create_model(trial)\n",
        "    optimizer = create_optimizer(trial)\n",
        "\n",
        "    # Training and validating cycle.\n",
        "    with tf.device(\"/cpu:0\"):\n",
        "        for _ in range(EPOCHS):\n",
        "            learn(model, optimizer, train_generator, \"train\")\n",
        "\n",
        "        accuracy = learn(model, optimizer, validation_generator, \"eval\")\n",
        "\n",
        "    # Return last validation accuracy.\n",
        "    return accuracy.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl-2wC6RTQCd"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUpwX8z_rNuq"
      },
      "source": [
        "###ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR8akFA1TmTr"
      },
      "outputs": [],
      "source": [
        "resnet_base_model = keras.applications.resnet.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "print(resnet_base_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ-qOtODrSNN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Freeze the already-trained layers in the base model\n",
        "for layer in resnet_base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create prediction layer for classification of our images\n",
        "x = resnet_base_model.output\n",
        "x = Flatten()(x)\n",
        "prediction_layer_resnet = Dense(len(classnames), activation='softmax')(x) \n",
        "model_resnet = Model(inputs=resnet_base_model.input, outputs=prediction_layer_resnet)\n",
        "\n",
        "# Compile the model\n",
        "model_resnet.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now print the full model, which will include the layers of the base model plus the dense layer we added\n",
        "print(model_resnet.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqFL-9aUred9"
      },
      "outputs": [],
      "source": [
        "# Train the model over 5 epochs\n",
        "num_epochs = 5\n",
        "history_resnet = model_resnet.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XUcUwD7r4FB"
      },
      "outputs": [],
      "source": [
        "# Tensorflow doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Generating predictions from validation data...\")\n",
        "# Get the image and label arrays for the first batch of validation data\n",
        "x_test = validation_generator[0][0]\n",
        "y_test = validation_generator[0][1]\n",
        "\n",
        "# Use the model to predict the class\n",
        "class_probabilities = model_resnet.predict(x_test)\n",
        "\n",
        "# The model returns a probability value for each class\n",
        "# The one with the highest probability is the predicted class\n",
        "predictions = np.argmax(class_probabilities, axis=1)\n",
        "\n",
        "# The actual labels are hot encoded (e.g. [0 1 0], so get the one with the value 1\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classnames))\n",
        "plt.xticks(tick_marks, classnames, rotation=85)\n",
        "plt.yticks(tick_marks, classnames)\n",
        "plt.xlabel(\"Predicted Shape\")\n",
        "plt.ylabel(\"Actual Shape\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIzMINc6rP4s"
      },
      "source": [
        "###MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JtdCd6_qvcJ"
      },
      "outputs": [],
      "source": [
        "mobilenet_base_model = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "print(resnet_base_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbDRCv-6rGUi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Freeze the already-trained layers in the base model\n",
        "for layer in mobilenet_base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create prediction layer for classification of our images\n",
        "x = mobilenet_base_model.output\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "prediction_layer_mobilenet = Dense(len(classnames), activation='softmax')(x) \n",
        "model_mobilenet = Model(inputs=mobilenet_base_model.input, outputs=prediction_layer_mobilenet)\n",
        "\n",
        "# Compile the model\n",
        "model_mobilenet.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now print the full model, which will include the layers of the base model plus the dense layer we added\n",
        "print(model_mobilenet.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzJmfeVosltH"
      },
      "outputs": [],
      "source": [
        "# Train the model over 5 epochs\n",
        "num_epochs = 5\n",
        "history_mobilenet = model_mobilenet.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIuFvMJEswtJ"
      },
      "outputs": [],
      "source": [
        "# Tensorflow doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Generating predictions from validation data...\")\n",
        "# Get the image and label arrays for the first batch of validation data\n",
        "x_test = validation_generator[0][0]\n",
        "y_test = validation_generator[0][1]\n",
        "\n",
        "# Use the model to predict the class\n",
        "class_probabilities = model_mobilenet.predict(x_test)\n",
        "\n",
        "# The model returns a probability value for each class\n",
        "# The one with the highest probability is the predicted class\n",
        "predictions = np.argmax(class_probabilities, axis=1)\n",
        "\n",
        "# The actual labels are hot encoded (e.g. [0 1 0], so get the one with the value 1\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classnames))\n",
        "plt.xticks(tick_marks, classnames, rotation=85)\n",
        "plt.yticks(tick_marks, classnames)\n",
        "plt.xlabel(\"Predicted Shape\")\n",
        "plt.ylabel(\"Actual Shape\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrESyACr6Bj7"
      },
      "outputs": [],
      "source": [
        "def random_crop(img, random_crop_size):\n",
        "    # Note: image_data_format is 'channel_last'\n",
        "    assert img.shape[2] == 3\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "    dy, dx = random_crop_size\n",
        "    x = np.random.randint(0, width - dx + 1)\n",
        "    y = np.random.randint(0, height - dy + 1)\n",
        "    return img[y:(y+dy), x:(x+dx), :]\n",
        "\n",
        "\n",
        "def crop_generator(batches, crop_length):\n",
        "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
        "    crops from the image batches generated by the original iterator.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        batch_x, batch_y = next(batches)\n",
        "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
        "        for i in range(batch_x.shape[0]):\n",
        "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
        "        yield (batch_crops, batch_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD2_joAas5W1"
      },
      "outputs": [],
      "source": [
        "train_folder = './train/train'\n",
        "test_folder = './test/test'\n",
        "pretrained_size = (224,224)\n",
        "batch_size = 30\n",
        "CROP_LENGTH = 224\n",
        "\n",
        "# Augmentation như Flip, Rotation, Random Crop, Bright Contrast\n",
        "print(\"Getting Data...\")\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255, # normalize pixel values\n",
        "    rotation_range=10, # rotation image\n",
        "    horizontal_flip=True, # flip image\n",
        "    validation_split=0.3) # hold back 30% of the images for validation\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Preparing training dataset...\")\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "validation_generator =  train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "print(\"Preparing test dataset...\")\n",
        "test_generator =  test_gen.flow_from_directory(\n",
        "    test_folder,\n",
        "    shuffle=False,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    # batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "\n",
        "classnames = list(train_generator.class_indices.keys())\n",
        "print(\"class names: \", classnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxyIhbgE90G-"
      },
      "outputs": [],
      "source": [
        "train_crops = crop_generator(train_generator, CROP_LENGTH)\n",
        "valid_crops = crop_generator(validation_generator, CROP_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voJZXy_97yYt"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "mobilenet_base_model_aug = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "print(resnet_base_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb3EPD-S9vOl"
      },
      "outputs": [],
      "source": [
        "# Freeze the already-trained layers in the base model\n",
        "for layer in mobilenet_base_model_aug.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = tf.image.adjust_contrast(inputs, 2.)\n",
        "x = mobilenet_base_model_aug(x, training=False)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "prediction_layer_mobilenet = Dense(len(classnames), activation='softmax')(x) \n",
        "model_mobilenet_aug = Model(inputs, outputs=prediction_layer_mobilenet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOpI7-Fi-6co"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model_mobilenet_aug.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYS-iENS_re6"
      },
      "outputs": [],
      "source": [
        "# Train the model over 5 epochs\n",
        "num_epochs = 5\n",
        "history_mobilenet_aug = model_mobilenet_aug.fit(\n",
        "    train_crops,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = valid_crops, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0yuCp-cAQ9I"
      },
      "source": [
        "### Only flip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3K6IpnP_tV6"
      },
      "outputs": [],
      "source": [
        "train_folder = './train/train'\n",
        "test_folder = './test/test'\n",
        "pretrained_size = (224,224)\n",
        "batch_size = 30\n",
        "CROP_LENGTH = 224\n",
        "\n",
        "# Augmentation như Flip\n",
        "print(\"Getting Data...\")\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255, # normalize pixel values\n",
        "    horizontal_flip=True, # flip image\n",
        "    validation_split=0.3) # hold back 30% of the images for validation\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Preparing training dataset...\")\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "validation_generator =  train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "print(\"Preparing test dataset...\")\n",
        "test_generator =  test_gen.flow_from_directory(\n",
        "    test_folder,\n",
        "    shuffle=False,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    # batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "classnames = list(train_generator.class_indices.keys())\n",
        "print(\"class names: \", classnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PycB3LqA8tT"
      },
      "outputs": [],
      "source": [
        "mobilenet_base_model_flip = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "print(mobilenet_base_model_flip.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "robJiG6tAmhg"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Freeze the already-trained layers in the base model\n",
        "for layer in mobilenet_base_model_flip.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create prediction layer for classification of our images\n",
        "x = mobilenet_base_model_flip.output\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "prediction_layer_mobilenet = Dense(len(classnames), activation='softmax')(x) \n",
        "model_mobilenet_flip = Model(inputs=mobilenet_base_model_flip.input, outputs=prediction_layer_mobilenet)\n",
        "\n",
        "# Compile the model\n",
        "model_mobilenet_flip.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now print the full model, which will include the layers of the base model plus the dense layer we added\n",
        "print(model_mobilenet_flip.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlqg7Jo3BJtj"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model_mobilenet_flip.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbEoqCrYBMuT"
      },
      "outputs": [],
      "source": [
        "# Train the model over 5 epochs\n",
        "num_epochs = 5\n",
        "history_mobilenet_flip = model_mobilenet_flip.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ezBupCBWOG"
      },
      "source": [
        "### Only Rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yrzpfRKBWOH"
      },
      "outputs": [],
      "source": [
        "train_folder = './train/train'\n",
        "test_folder = './test/test'\n",
        "pretrained_size = (224,224)\n",
        "batch_size = 30\n",
        "CROP_LENGTH = 224\n",
        "\n",
        "# Augmentation như Rotation\n",
        "print(\"Getting Data...\")\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255, # normalize pixel values\n",
        "    validation_split=0.3) # hold back 30% of the images for validation\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Preparing training dataset...\")\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "validation_generator =  train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "print(\"Preparing test dataset...\")\n",
        "test_generator =  test_gen.flow_from_directory(\n",
        "    test_folder,\n",
        "    shuffle=False,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    # batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "classnames = list(train_generator.class_indices.keys())\n",
        "print(\"class names: \", classnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCcMazpKBWOJ"
      },
      "outputs": [],
      "source": [
        "mobilenet_base_model_rot = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "print(mobilenet_base_model_rot.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpIOSyoiBWOK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Freeze the already-trained layers in the base model\n",
        "for layer in mobilenet_base_model_rot.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create prediction layer for classification of our images\n",
        "x = mobilenet_base_model_rot.output\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "prediction_layer_mobilenet = Dense(len(classnames), activation='softmax')(x) \n",
        "model_mobilenet_rot = Model(inputs=mobilenet_base_model_rot.input, outputs=prediction_layer_mobilenet)\n",
        "\n",
        "# Compile the model\n",
        "model_mobilenet_rot.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now print the full model, which will include the layers of the base model plus the dense layer we added\n",
        "print(model_mobilenet_rot.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmVSTiKjBWOL"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model_mobilenet_rot.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3WwMGPEBWOL"
      },
      "outputs": [],
      "source": [
        "# Train the model over 5 epochs\n",
        "num_epochs = 5\n",
        "history_mobilenet_rot = model_mobilenet_rot.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5-nyjuwB3xJ"
      },
      "source": [
        "### Only crop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC6rWg4GB3xm"
      },
      "outputs": [],
      "source": [
        "train_folder = './train/train'\n",
        "test_folder = './test/test'\n",
        "pretrained_size = (224,224)\n",
        "batch_size = 30\n",
        "CROP_LENGTH = 224\n",
        "\n",
        "# Augmentation như Rotation\n",
        "print(\"Getting Data...\")\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255, # normalize pixel values\n",
        "    rotation_range=10, # flip image\n",
        "    validation_split=0.3) # hold back 30% of the images for validation\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Preparing training dataset...\")\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "validation_generator =  train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "print(\"Preparing test dataset...\")\n",
        "test_generator =  test_gen.flow_from_directory(\n",
        "    test_folder,\n",
        "    shuffle=False,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    # batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "classnames = list(train_generator.class_indices.keys())\n",
        "print(\"class names: \", classnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgBEO84CC4UT"
      },
      "outputs": [],
      "source": [
        "train_crops = crop_generator(train_generator, CROP_LENGTH)\n",
        "valid_crops = crop_generator(validation_generator, CROP_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7104-jWB3xo"
      },
      "outputs": [],
      "source": [
        "mobilenet_base_model_crop = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "print(mobilenet_base_model_crop.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djZk4igZB3xp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Freeze the already-trained layers in the base model\n",
        "for layer in mobilenet_base_model_crop.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create prediction layer for classification of our images\n",
        "x = mobilenet_base_model_crop.output\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "prediction_layer_mobilenet = Dense(len(classnames), activation='softmax')(x) \n",
        "model_mobilenet_crop = Model(inputs=mobilenet_base_model_crop.input, outputs=prediction_layer_mobilenet)\n",
        "\n",
        "# Compile the model\n",
        "model_mobilenet_crop.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now print the full model, which will include the layers of the base model plus the dense layer we added\n",
        "print(model_mobilenet_crop.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr-wTYhVB3xq"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model_mobilenet_crop.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWnQJ-6DB3xq"
      },
      "outputs": [],
      "source": [
        "# Train the model over 5 epochs\n",
        "num_epochs = 5\n",
        "history_mobilenet_crop = model_mobilenet_crop.fit(\n",
        "    train_crops,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = valid_crops, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jctxiuzlDIXb"
      },
      "source": [
        "### Only bright contrast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w0Yvb_kDIXc"
      },
      "outputs": [],
      "source": [
        "train_folder = './train/train'\n",
        "test_folder = './test/test'\n",
        "pretrained_size = (224,224)\n",
        "batch_size = 30\n",
        "CROP_LENGTH = 224\n",
        "\n",
        "# Augmentation như Rotation\n",
        "print(\"Getting Data...\")\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255, # normalize pixel values\n",
        "    rotation_range=10, # flip image\n",
        "    validation_split=0.3) # hold back 30% of the images for validation\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"Preparing training dataset...\")\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "validation_generator =  train_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "print(\"Preparing test dataset...\")\n",
        "test_generator =  test_gen.flow_from_directory(\n",
        "    test_folder,\n",
        "    shuffle=False,\n",
        "    target_size=pretrained_size, # resize to match model expected input\n",
        "    # batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "classnames = list(train_generator.class_indices.keys())\n",
        "print(\"class names: \", classnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jdmYy2cDIXd"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "mobilenet_base_model_bc = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "print(resnet_base_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_e1pvjhDIXd"
      },
      "outputs": [],
      "source": [
        "# Freeze the already-trained layers in the base model\n",
        "for layer in mobilenet_base_model_bc.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = tf.image.adjust_contrast(inputs, 2.)\n",
        "x = mobilenet_base_model_bc(x, training=False)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Flatten()(x)\n",
        "prediction_layer_mobilenet = Dense(len(classnames), activation='softmax')(x) \n",
        "model_mobilenet_bc = Model(inputs, outputs=prediction_layer_mobilenet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHeSCGViDIXe"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model_mobilenet_bc.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfgv-rs4DIXe"
      },
      "outputs": [],
      "source": [
        "# Train the model over 5 epochs\n",
        "num_epochs = 5\n",
        "history_mobilenet_bc = model_mobilenet_bc.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyIWGylHEsWH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "d = {'Rotation': [history_mobilenet_rot.accuracy], 'Flip': [history_mobilenet_flip.accuracy], 'Crop': [history_mobilenet_crop.accuracy], 'Bright Contrast': [history_mobilenet_bc.accuracy], 'All Augmentation': [history_mobilenet_aug.accuracy]}\n",
        "pd.DataFrame(data=d, index=['Accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFYz0--EGAqz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "5.CNN_HomeWork_NguyenDanhDuc.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}