{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.Homework_NguyenVietLong.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEr1eFvAmug6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Lý thuyết\n",
        "\n",
        "1) Tại sao các mô hình deep learning lại chiếm ưu thế hơn so với các mô hình machine learning truyền thống đối với dữ liệu lớn ?\n",
        "\n",
        "A. Do mô hình deep learning có thể được thiết kế với kích thước tùy ý nên có khả năng xấp xỉ mọi hàm số. Do đó nó có khả năng biểu diễn tốt và hoạt động hiệu quả trên dữ liệu lớn.\n",
        "\n",
        "B. Các mô hình machine learning thường bị overfitting đối với dữ liệu lớn ?\n",
        "\n",
        "C. Các mô hình deep learning có chi phí huấn luyện tốn kém hơn so với machine learning.\n",
        "\n",
        "D. Do kiến trúc của mô hình Machine Learning bao gồm nhiều layers xếp chồng.\n",
        "\n",
        "\n",
        "2) Ý nghĩa của hàm loss function trong mạng neural network là gì ?\n",
        "\n",
        "A. Là hàm số đánh giá độ chính xác của mô hình.\n",
        "\n",
        "B. Mục tiêu của quá trình huấn luyện là tối thiểu hóa hàm loss function bằng thuật toán gradient descent. Giá trị của hàm số này giúp đo lường mức độ khớp của dự báo từ mô hình trên dữ liệu huấn luyện.\n",
        "\n",
        "C. Khi loss function giảm thì luôn đảm bảo độ chính xác của mô hình tăng.\n",
        "\n",
        "D. Là hàm số cần tối đa hóa trong quá trình huấn luyện.\n",
        "\n",
        "\n",
        "3) Khi huấn luyện trên các bộ dữ liệu bigdata thì chúng ta nên sử dụng phương pháp nào ?\n",
        "\n",
        "A) Sử dụng gradient descent trên toàn bộ dữ liệu.\n",
        "\n",
        "B) Sử dụng stochastic gradient descent trên từng điểm dữ liệu.\n",
        "\n",
        "C) Mini-batch gradient descent huấn luyện mô hình trên từng tập dữ liệu con có kích thước nhỏ hơn memory CPU/GPU.\n",
        "\n",
        "D) Có thể sử dụng stochastic gradient descent hoặc mini-batch gradient descent.\n",
        "\n",
        "\n",
        "4) Quá trình feed forward và backpropagation thực hiện những gì ?\n",
        "\n",
        "A) feed forward tính toán output và loss function, backpropagation tính đạo hàm trên từng layer và cập nhật trọng số.\n",
        "\n",
        "B) feed forward cập nhật trọng số cho mô hình, backpropagation tính toán output và loss function.\n",
        "\n",
        "C) feed forward tính ra output của mô hình, backpropagation tính toán loss function\n",
        "\n",
        "D) feed forward được thực hiện sau backpropagation.\n",
        "\n",
        "5) Tác dụng của batch normalization là gì ?\n",
        "\n",
        "A) Loại bỏ một tỷ lệ ngẫu nhiên số lượng units tại mỗi layer để tạo thành nhiều kiến trúc kết hợp ngẫu nhiên.\n",
        "\n",
        "B) Tìm ra các tham số phân phối là trung bình và phương sai trên từng mini-batch.\n",
        "\n",
        "C) Đồng nhất phân phối xác suất của $z^{[l]}$ trên mỗi layer $l$.\n",
        "\n",
        "D) Giảm thiểu ảnh hưởng của input distribution shift nhằm giúp huấn luyện loss function nhanh và ổn định hơn."
      ],
      "metadata": {
        "id": "PDDUcokDmyX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Thực hành\n",
        "\n",
        "Xuất phát từ mô hình tốt nhất của bạn xây dựng được đối với bài toán phân loại income classification tại bài trước. Bạn hãy thực hiện một số thử nghiệm sau:\n",
        "\n",
        "6) Thay đổi hàm loss function, batch size và optimizer.\n",
        "\n",
        "7) Thử nghiệm thêm các layers mà bạn đã học được trong bài này vào kiến trúc của mình.\n",
        "\n",
        "8) Thay đổi các khởi tạo trọng số theo các phân phối khác nhau và đánh giá độ chính xác của kết quả huấn luyện.\n",
        "\n",
        "9) Thiết lập không gian search và tự động hóa tìm kiếm kiến trúc tốt nhất trên optuna.\n",
        "\n",
        "10) Deploy model sử dụng flask ap. Tham khảo [Flaskapp tutorial](https://drive.google.com/file/d/1AZNtzrmnhJ-OBgijWoaAqXbPhJ6xL0Po/view?usp=sharing)."
      ],
      "metadata": {
        "id": "zYoLCUgcm1Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part1 \n",
        "1) A\n",
        "2) B\n",
        "3) C\n",
        "4) A\n",
        "5) C\n"
      ],
      "metadata": {
        "id": "WZeUWLVi9EDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "os.chdir(\"gdrive/MyDrive/Colab Notebooks/KhanhML\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "9QCKPFZJ0btG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3dee10-55a4-4f9a-94ab-d6c45ea93c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Colab Notebooks/KhanhML\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "sdvlmZ3q0b1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm3CtOGYyqaJ",
        "outputId": "7ad75893-3504-4468-852f-8e09b315d064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train  = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/KhanhML/train_income.csv\")\n",
        "test  = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/KhanhML/test_income.csv\")"
      ],
      "metadata": {
        "id": "2RzKd-XX0QFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_categorical_features(df, cat_features):\n",
        "    df_new=pd.get_dummies(df, columns=cat_features)\n",
        "    return df_new\n",
        "\n",
        "def preprocess_data(data, continuous_feat, categorical_feat, income_feat):\n",
        "    scaler=StandardScaler()\n",
        "    data = data[continuous_feat+categorical_feat+income_feat]\n",
        "    for col in data.columns:\n",
        "        data[col].replace(\"?\", np.nan, inplace=True)\n",
        "        \n",
        "        if col in continuous_feat:\n",
        "            data[col] = scaler.fit_transform(np.array(data[col]).reshape(-1,1))      \n",
        "    return data"
      ],
      "metadata": {
        "id": "msugH5DB0j3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Process\n",
        "categorical_feat = ['work_type','education','marital_state','job','status','ethnicity','sex','nationality']\n",
        "continuous_feat = ['age','final_weight','total_education_yrs','capital_gain','capital_loss','hrs_per_week']\n",
        "\n",
        "train_process = preprocess_data(train.iloc[:,1:],continuous_feat,categorical_feat,['target_income'])\n",
        "test_process = preprocess_data(test.iloc[:,1:],continuous_feat,categorical_feat,[])\n",
        "\n",
        "train_process = one_hot_categorical_features(train_process,categorical_feat)\n",
        "test_process = one_hot_categorical_features(test_process,categorical_feat)"
      ],
      "metadata": {
        "id": "QxqcqvTO0j5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b780bc-c2d3-49a1-fe3d-deb8d837fdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6619: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return self._update_inplace(result)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6619: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return self._update_inplace(result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = 'target_income'\n",
        "features = [i for i in train_process.columns if 'target_income' not in i and i in test_process.columns]\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_process[features].values, # input variable\n",
        "                                                    train_process[label].values, # output variable\n",
        "                                                    test_size=0.20, # test dataset proportion\n",
        "                                                    stratify=train_process['target_income'], # assign equal proportion of target label in train/test \n",
        "                                                    random_state=0) # keep train/test split the same if run again. \n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "metadata": {
        "id": "u_55ATPP0j_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hl = 20 # Number of hidden layer nodes\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(hl, input_dim=len(features), activation='sigmoid', kernel_initializer=tf.initializers.HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(hl, input_dim=hl, activation='relu', kernel_initializer=tf.initializers.HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(hl, input_dim=hl, activation='tanh', kernel_initializer=tf.initializers.RandomNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Dense(hl, input_dim=hl, activation='relu', kernel_initializer=tf.initializers.RandomNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, input_dim=hl, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "0Xgaz9bV0wFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3485d69-c637-4e78-f0ac-957da317424b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 20)                2100      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 20)               80        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 20)               80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 20)               80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 20)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 20)               80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,701\n",
            "Trainable params: 3,541\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "opt = optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "model.compile(loss='BinaryCrossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['Accuracy'])\n",
        "\n",
        "# Train the model over 50 epochs using 10-observation batches and using the test holdout dataset for validation\n",
        "num_epochs = 10\n",
        "history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=16)"
      ],
      "metadata": {
        "id": "O5gVIIry0v3N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "52ea31ae-78c0-4f02-bf88-88a3d6b88bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ebb306d0a312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model.compile(loss='BinaryCrossentropy',\n\u001b[0m\u001b[1;32m      5\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               metrics=['Accuracy'])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/gdrive/MyDrive/Colab Notebooks/KhanhML/income_dl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJA8q9fHJugJ",
        "outputId": "0808170f-db79-4308-c348-57be67f7afb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /gdrive/MyDrive/Colab Notebooks/KhanhML/income_dl/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Get predictions for the test data\n",
        "predicted = model.predict(x_test)\n",
        "predicted = np.where(predicted > 0.5, 1, 0)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y_test, predicted)\n",
        "# plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "# plt.colorbar()\n",
        "# # tick_marks = np.arange(len(label))\n",
        "# # plt.xticks(tick_marks, label, rotation=45)\n",
        "# # plt.yticks(tick_marks, label)\n",
        "# plt.xlabel(\"Predicted\")\n",
        "# plt.ylabel(\"Actual\")\n",
        "# # plt.show()"
      ],
      "metadata": {
        "id": "q57PKT3kImTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "hOpCn18EImc0",
        "outputId": "4b27f660-d4e4-49db-b78a-3997773f6b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01t20Ycy1uKT",
        "outputId": "3dd754d0-4b08-4076-da56-34e02cda165d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8586"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "__hCKd8t08Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTUNA"
      ],
      "metadata": {
        "id": "uQUlQU36pZZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna \n",
        "import joblib\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwlmeYnnpgvh",
        "outputId": "81e9c56c-91b6-4e46-a7f0-78ff0d4faa61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 184 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 235 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 256 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 276 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 296 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 308 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.6-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 68.0 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.0.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 62.8 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=215df73501ebf0d33faff1e46db35a5f5eff03be5c77ee76ffdd5651159fd751\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.6 alembic-1.7.6 autopage-0.5.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0uJXo4Dj3xLv",
        "outputId": "a656cd0f-46a5-47f5-a4c6-a7b91b58fc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/Colab Notebooks/KhanhML'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_dl_optuna(X_train, y_train, X_test, y_test,n_trials=50):\n",
        "  # study_name = \"/gdrive/MyDrive/Colab Notebooks/KhanhML/params_{}.pkl\".format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  study=optuna.create_study(direction='maximize')\n",
        "  study.optimize(lambda trial:obj_dl_model(trial, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test),\n",
        "                                          n_trials=n_trials)\n",
        "  \n",
        "  # Save model\n",
        "  # joblib.dump(value=study, filename=study_name)\n",
        "\n",
        "  # Get best params\n",
        "  best_params = study.best_trial\n",
        "  for key, value in best_params.params.items():\n",
        "    print(\"{}: {}\".format(key, value))\n",
        "\n",
        "  return best_params"
      ],
      "metadata": {
        "id": "1ewklOGKpgyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obj_dl_model(trial, X_train, y_train, X_test, y_test):\n",
        "    # Param Setiings\n",
        "    hl = trial.suggest_int(\"num_layers\", 15, 30) # Number of hidden layer nodes\n",
        "    drop = trial.suggest_float(\"drop\", 0.25, 0.5) # Dropout\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.2)\n",
        "\n",
        "    # Init model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hl, input_dim=x_train.shape[1], activation='sigmoid', kernel_initializer=tf.initializers.HeNormal()))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(hl, input_dim=hl, activation='relu', kernel_initializer=tf.initializers.HeNormal()))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(hl, input_dim=hl, activation='tanh', kernel_initializer=tf.initializers.RandomNormal()))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(drop))\n",
        "    model.add(Dense(hl, input_dim=hl, activation='relu', kernel_initializer=tf.initializers.RandomNormal()))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(1, input_dim=hl, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='BinaryCrossentropy',\n",
        "                  optimizer=optimizers.Adam(lr=learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model over 50 epochs using 10-observation batches and using the test holdout dataset for validation\n",
        "    num_epochs = 25\n",
        "    history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=16, verbose=0)\n",
        "\n",
        "    predicted = model.predict(X_test)\n",
        "    predicted = np.where(predicted > 0.5, 1, 0)\n",
        "\n",
        "    return accuracy_score(y_test, predicted)\n"
      ],
      "metadata": {
        "id": "6JzuDCFars0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_dl_optuna(x_train,y_train,x_test,y_test,n_trials=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIoEeS9zrYRH",
        "outputId": "294b4fd3-cfaa-4393-f3c3-2dd8cc563364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-02-18 10:33:57,906]\u001b[0m A new study created in memory with name: no-name-d3bc03da-828d-4eec-a968-01896a7406e5\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:35:21,657]\u001b[0m Trial 0 finished with value: 0.8086 and parameters: {'num_layers': 22, 'drop': 0.307602821627309, 'learning_rate': 0.11337488987447}. Best is trial 0 with value: 0.8086.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:36:20,705]\u001b[0m Trial 1 finished with value: 0.7598 and parameters: {'num_layers': 20, 'drop': 0.4600969533448482, 'learning_rate': 0.03094477390989158}. Best is trial 0 with value: 0.8086.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:37:44,348]\u001b[0m Trial 2 finished with value: 0.8434 and parameters: {'num_layers': 17, 'drop': 0.3360725642957966, 'learning_rate': 0.06038852803035392}. Best is trial 2 with value: 0.8434.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:38:48,079]\u001b[0m Trial 3 finished with value: 0.7606 and parameters: {'num_layers': 25, 'drop': 0.3769310854258001, 'learning_rate': 0.13160433300374902}. Best is trial 2 with value: 0.8434.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:40:11,849]\u001b[0m Trial 4 finished with value: 0.7942 and parameters: {'num_layers': 21, 'drop': 0.4867089889750084, 'learning_rate': 0.05434880492279319}. Best is trial 2 with value: 0.8434.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:41:35,441]\u001b[0m Trial 5 finished with value: 0.786 and parameters: {'num_layers': 20, 'drop': 0.3855991620502332, 'learning_rate': 0.09506965810146319}. Best is trial 2 with value: 0.8434.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:42:59,055]\u001b[0m Trial 6 finished with value: 0.7606 and parameters: {'num_layers': 28, 'drop': 0.2732667628789387, 'learning_rate': 0.15323217325759234}. Best is trial 2 with value: 0.8434.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:44:22,715]\u001b[0m Trial 7 finished with value: 0.7586 and parameters: {'num_layers': 29, 'drop': 0.3894232597796767, 'learning_rate': 0.09452705740215155}. Best is trial 2 with value: 0.8434.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:45:46,329]\u001b[0m Trial 8 finished with value: 0.7986 and parameters: {'num_layers': 16, 'drop': 0.26028525415947856, 'learning_rate': 0.07590451519218289}. Best is trial 2 with value: 0.8434.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:46:50,153]\u001b[0m Trial 9 finished with value: 0.7606 and parameters: {'num_layers': 22, 'drop': 0.2598869299736109, 'learning_rate': 0.1870179683393343}. Best is trial 2 with value: 0.8434.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:47:55,684]\u001b[0m Trial 10 finished with value: 0.8582 and parameters: {'num_layers': 15, 'drop': 0.3365040739711397, 'learning_rate': 0.00443790780577516}. Best is trial 10 with value: 0.8582.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:49:00,906]\u001b[0m Trial 11 finished with value: 0.8572 and parameters: {'num_layers': 15, 'drop': 0.3276071714738425, 'learning_rate': 0.008592135100953397}. Best is trial 10 with value: 0.8582.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:50:04,853]\u001b[0m Trial 12 finished with value: 0.8584 and parameters: {'num_layers': 15, 'drop': 0.33115828350160287, 'learning_rate': 0.0038756847359684397}. Best is trial 12 with value: 0.8584.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:51:28,343]\u001b[0m Trial 13 finished with value: 0.8264 and parameters: {'num_layers': 18, 'drop': 0.41691812927308824, 'learning_rate': 0.007705893702552511}. Best is trial 12 with value: 0.8584.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:52:33,778]\u001b[0m Trial 14 finished with value: 0.7698 and parameters: {'num_layers': 18, 'drop': 0.34414073253062627, 'learning_rate': 0.03765113652934528}. Best is trial 12 with value: 0.8584.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:53:57,222]\u001b[0m Trial 15 finished with value: 0.8506 and parameters: {'num_layers': 15, 'drop': 0.29807903728670626, 'learning_rate': 0.026218391721857567}. Best is trial 12 with value: 0.8584.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:55:03,237]\u001b[0m Trial 16 finished with value: 0.8578 and parameters: {'num_layers': 25, 'drop': 0.4192883082688972, 'learning_rate': 0.001026346529156137}. Best is trial 12 with value: 0.8584.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:56:26,820]\u001b[0m Trial 17 finished with value: 0.8322 and parameters: {'num_layers': 18, 'drop': 0.356209094688555, 'learning_rate': 0.05140723722163087}. Best is trial 12 with value: 0.8584.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:57:32,111]\u001b[0m Trial 18 finished with value: 0.8522 and parameters: {'num_layers': 25, 'drop': 0.29015700876878175, 'learning_rate': 0.02297412382078322}. Best is trial 12 with value: 0.8584.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "\u001b[32m[I 2022-02-18 10:58:55,823]\u001b[0m Trial 19 finished with value: 0.76 and parameters: {'num_layers': 15, 'drop': 0.31762153975739954, 'learning_rate': 0.07237987905735409}. Best is trial 12 with value: 0.8584.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_layers: 15\n",
            "drop: 0.33115828350160287\n",
            "learning_rate: 0.0038756847359684397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trắc nghiệm sai câu 5\n",
        "# Chưa làm khởi tạo trọng số, thay đổi loss, optim, batchsize,\n",
        "# Chưa train lại sau khi tìm đc tham số tốt khi dùng optuna"
      ],
      "metadata": {
        "id": "JW0PXgNf2Ahm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JlSVlliH2axs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}