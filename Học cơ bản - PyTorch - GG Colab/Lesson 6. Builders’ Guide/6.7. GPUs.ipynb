{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOur1NNhPOQB5qjTT2NMtEQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9EY6VonkKNZ8"},"outputs":[],"source":["!pip install d2l==1.0.0-beta0"]},{"cell_type":"markdown","source":["Trong Bảng 1.5.1 , chúng tôi đã thảo luận về sự phát triển nhanh chóng của tính toán trong hai thập kỷ qua. Tóm lại, hiệu suất GPU đã tăng lên gấp 1000 lần sau mỗi thập kỷ kể từ năm 2000. Điều này mang lại những cơ hội tuyệt vời nhưng cũng cho thấy nhu cầu đáng kể để cung cấp hiệu suất như vậy.\n","\n","Trong phần này, chúng ta bắt đầu thảo luận về cách khai thác hiệu suất tính toán này cho nghiên cứu của bạn. Đầu tiên bằng cách sử dụng các GPU đơn lẻ và sau đó là cách sử dụng nhiều GPU và nhiều máy chủ (với nhiều GPU).\n","\n","Cụ thể, chúng tôi sẽ thảo luận về cách sử dụng một GPU NVIDIA duy nhất để tính toán. Trước tiên, hãy đảm bảo rằng bạn đã cài đặt ít nhất một GPU NVIDIA. Sau đó, tải xuống trình điều khiển NVIDIA và CUDA và làm theo lời nhắc để đặt đường dẫn thích hợp. Khi các bước chuẩn bị này hoàn tất, nvidia-smilệnh có thể được sử dụng để xem thông tin cạc đồ họa."],"metadata":{"id":"IjybJ9eyK79n"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NVnVG6mLB-G","executionInfo":{"status":"ok","timestamp":1678026527248,"user_tz":-420,"elapsed":592,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"beaaa316-480e-4c1d-ced6-96ee95190f31"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Mar  5 14:28:46 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   65C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["Trong PyTorch, mọi mảng đều có một thiết bị, chúng tôi thường gọi nó là một ngữ cảnh. Cho đến nay, theo mặc định, tất cả các biến và tính toán liên quan đã được gán cho CPU. Thông thường, các bối cảnh khác có thể là các GPU khác nhau. Mọi thứ có thể trở nên rắc rối hơn khi chúng tôi triển khai công việc trên nhiều máy chủ. Bằng cách gán các mảng cho các ngữ cảnh một cách thông minh, chúng ta có thể giảm thiểu thời gian truyền dữ liệu giữa các thiết bị. Ví dụ: khi đào tạo mạng thần kinh trên máy chủ có GPU, chúng tôi thường muốn các tham số của mô hình tồn tại trên GPU.\n","\n","Để chạy các chương trình trong phần này, bạn cần ít nhất hai GPU. Lưu ý rằng điều này có thể xa hoa đối với hầu hết các máy tính để bàn nhưng nó dễ dàng có sẵn trên đám mây, chẳng hạn như bằng cách sử dụng các phiên bản đa GPU AWS EC2. Hầu như tất cả các phần khác không yêu cầu nhiều GPU. Thay vào đó, điều này chỉ đơn giản là để minh họa cách dữ liệu di chuyển giữa các thiết bị khác nhau."],"metadata":{"id":"ZdKI_m5eLTRU"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from d2l import torch as d2l"],"metadata":{"id":"MJ-0slv5Ldv7","executionInfo":{"status":"ok","timestamp":1678026603819,"user_tz":-420,"elapsed":5156,"user":{"displayName":"vu tung","userId":"03394210267989276557"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# 6.7.1. Thiết bị tính toán\n","\n","Chúng tôi có thể chỉ định các thiết bị, chẳng hạn như CPU ​​và GPU, để lưu trữ và tính toán. Theo mặc định, các tenxơ được tạo trong bộ nhớ chính và sau đó sử dụng CPU để tính toán nó.\n","\n","Trong PyTorch, CPU và GPU có thể được biểu thị bằng `torch.device('cpu')` và `torch.device('cuda')`. Cần lưu ý rằng cputhiết bị có nghĩa là tất cả các CPU và bộ nhớ vật lý. Điều này có nghĩa là các tính toán của PyTorch sẽ cố gắng sử dụng tất cả các lõi CPU. Tuy nhiên, một gputhiết bị chỉ đại diện cho một thẻ và bộ nhớ tương ứng. Nếu có nhiều GPU, chúng tôi sử dụng torch.device(f'cuda:{i}')để đại diện cho \n","GPU $i^\\mathrm{th}$ ($i$ bắt đầu từ 0). Ngoài ra, gpu:0 và gpu là tương đương."],"metadata":{"id":"8PrfyjftLhnH"}},{"cell_type":"code","source":["def cpu():  \n","    \"\"\"Get the CPU device.\"\"\"\n","    return torch.device('cpu')\n","\n","def gpu(i=0): \n","    \"\"\"Get a GPU device.\"\"\"\n","    return torch.device(f'cuda:{i}')\n","\n","cpu(), gpu(), gpu(1), gpu(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ip8GgS8EL6jU","executionInfo":{"status":"ok","timestamp":1678026739547,"user_tz":-420,"elapsed":659,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"5e3aae83-2220-4813-9779-5970a599dac6"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cpu'),\n"," device(type='cuda', index=0),\n"," device(type='cuda', index=1),\n"," device(type='cuda', index=2))"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Chúng tôi có thể truy vấn số lượng GPU có sẵn."],"metadata":{"id":"3gGcGHxbMDMr"}},{"cell_type":"code","source":["def num_gpus(): \n","    \"\"\"Get the number of available GPUs.\"\"\"\n","    return torch.cuda.device_count()\n","\n","num_gpus()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FwOh-OpiMDvC","executionInfo":{"status":"ok","timestamp":1678026758230,"user_tz":-420,"elapsed":2,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"e851e610-b0a0-40b1-d75e-aab0ffff33e8"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Bây giờ chúng tôi xác định hai chức năng thuận tiện cho phép chúng tôi chạy mã ngay cả khi GPU được yêu cầu không tồn tại."],"metadata":{"id":"8lv8wpHYMO0t"}},{"cell_type":"code","source":["def try_gpu(i=0):  \n","    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n","    if num_gpus() >= i + 1:\n","        return gpu(i)\n","    return cpu()\n","\n","def try_all_gpus():  \n","    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n","    return [gpu(i) for i in range(num_gpus())]\n","\n","try_gpu(), try_gpu(10), try_all_gpus()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4W1-Bt9MOcP","executionInfo":{"status":"ok","timestamp":1678026809061,"user_tz":-420,"elapsed":371,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"9ff425d4-665a-42b6-cd81-f228ef577db9"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cuda', index=0),\n"," device(type='cpu'),\n"," [device(type='cuda', index=0)])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# 6.7.2. Tensor và GPU\n","\n","Theo mặc định, các tenxơ được tạo trên CPU. Chúng ta có thể truy vấn thiết bị nơi đặt tensor.\n"],"metadata":{"id":"oaLKHRGqMbV7"}},{"cell_type":"code","source":["x = torch.tensor([1, 2, 3])\n","x.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTGmVD2QMjEL","executionInfo":{"status":"ok","timestamp":1678026882831,"user_tz":-420,"elapsed":393,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"f842c47e-804e-4c1a-d9a6-72041fcb98b6"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Điều quan trọng cần lưu ý là bất cứ khi nào chúng tôi muốn hoạt động trên nhiều thuật ngữ, chúng cần phải ở trên cùng một thiết bị. Chẳng hạn, nếu chúng ta tính tổng hai tenxơ, chúng ta cần đảm bảo rằng cả hai đối số đều nằm trên cùng một thiết bị—nếu không, khung sẽ không biết nơi lưu trữ kết quả hoặc thậm chí cách quyết định nơi thực hiện phép tính."],"metadata":{"id":"Td5LU8twMn9m"}},{"cell_type":"markdown","source":["# 6.7.2.1. Lưu trữ trên GPU\n","\n","Có một số cách để lưu trữ tensor trên GPU. Ví dụ: chúng ta có thể chỉ định thiết bị lưu trữ khi tạo tensor. Tiếp theo, chúng ta tạo biến tensor X trên biến đầu tiên gpu. Tenor được tạo trên GPU chỉ tiêu thụ bộ nhớ của GPU này. Chúng ta có thể sử dụng nvidia-smi lệnh để xem mức sử dụng bộ nhớ GPU. Nói chung, chúng tôi cần đảm bảo rằng chúng tôi không tạo dữ liệu vượt quá giới hạn bộ nhớ GPU.\n"],"metadata":{"id":"R9FZ7iEnMolP"}},{"cell_type":"code","source":["X = torch.ones(2, 3, device=try_gpu())\n","X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q8ZzHzBmMu0M","executionInfo":{"status":"ok","timestamp":1678026934882,"user_tz":-420,"elapsed":4262,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"7f6f576d-b136-4d33-ae61-b2e20a488ada"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["Giả sử rằng bạn có ít nhất hai GPU, đoạn mã sau sẽ tạo một tensor ngẫu nhiên trên GPU thứ hai."],"metadata":{"id":"EMAirE9zMy5z"}},{"cell_type":"code","source":["Y = torch.rand(2, 3, device=try_gpu(1))\n","Y.device # do không có gpu thứ 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWJe9fiwMzKm","executionInfo":{"status":"ok","timestamp":1678026956877,"user_tz":-420,"elapsed":2,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"5690c1d3-6c5e-4513-b9ef-357c8ade1db5"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## 6.7.2.2. Sao chép\n","\n","Nếu như ta mối tính toán ví dụ như cộng 2 tensor X + Y ở 2 GPU khác nhau, ta phải copy sang cùng một GPU rồi thực hiện phép tính.\n","\n","![Copy data to perform an operation on the same device.](http://d2l.ai/_images/copyto.svg)\n"],"metadata":{"id":"6KWLBn8RM4vz"}},{"cell_type":"code","source":["Z = Y.cuda()\n","print(Y)\n","print(Z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5IEiOe0Neeb","executionInfo":{"status":"ok","timestamp":1678027378074,"user_tz":-420,"elapsed":2,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"8476c21b-7189-4112-81a2-010a021c7f38"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4622, 0.2333, 0.7128],\n","        [0.0933, 0.8761, 0.2962]])\n","tensor([[0.4622, 0.2333, 0.7128],\n","        [0.0933, 0.8761, 0.2962]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["X + Z"],"metadata":{"id":"GyfisYskOeNY","executionInfo":{"status":"ok","timestamp":1678027389766,"user_tz":-420,"elapsed":410,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"a210fd24-0222-4430-c967-502b988bc90e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.4622, 1.2333, 1.7128],\n","        [1.0933, 1.8761, 1.2962]], device='cuda:0')"]},"metadata":{},"execution_count":22}]}]}