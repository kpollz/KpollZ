{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLIubSqAH+rToBQIMPBHXM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Cho đến lúc này, ta đã huấn luyện các mô hình với giải thuật hạ gradient ngẫu nhiên theo minibatch. Tuy nhiên, khi lập trình thuật toán, ta mới chỉ bận tâm đến các phép tính trong quá trình lan truyền xuôi qua mô hình. Khi cần tính gradient, ta mới chỉ đơn giản gọi hàm backward và mô-đun autograd sẽ lo các chi tiết tính toán.\n","\n","Việc tính toán gradient tự động sẽ giúp công việc lập trình các thuật toán học sâu được đơn giản hóa đi rất nhiều. Trước đây, khi chưa có công cụ tính vi phân tự động, ngay cả khi ta chỉ thay đổi một chút các mô hình phức tạp, các đạo hàm rắc rối cũng cần phải được tính lại một cách thủ công. Điều đáng ngạc nhiên là các bài báo học thuật thường có các công thức cập nhật mô hình dài hàng trang giấy. Vậy nên dù vẫn phải tiếp tục dựa vào autograd để có thể tập trung vào những phần thú vị của học sâu, bạn vẫn nên nắm rõ thay vì chỉ hiểu một cách hời hợt cách tính gradient nếu bạn muốn tiến xa hơn.\n","\n","Trong mục này, ta sẽ đi sâu vào chi tiết của lan truyền ngược (thường được gọi là backpropagation hoặc backprop). Ta sẽ sử dụng một vài công thức toán học cơ bản và đồ thị tính toán để giải thích một cách chi tiết cách thức hoạt động cũng như cách lập trình các kỹ thuật này. Và để bắt đầu, ta sẽ tập trung giải trình một perceptron đa tầng gồm ba tầng (một tầng ẩn) đi kèm với suy giảm trọng số (điều chuẩn  $\\ell_2$).\n"],"metadata":{"id":"RsENF6RjMnDX"}},{"cell_type":"markdown","source":["# 5.3.1. Lan truyền Xuôi\n","\n","Lan truyền xuôi là quá trình tính toán cũng như lưu trữ các biến trung gian (bao gồm cả đầu ra) của mạng nơ-ron theo thứ tự từ tầng đầu vào đến tầng đầu ra. Bây giờ ta sẽ thực hiện qua từng bước trong cơ chế vận hành của mạng nơ-ron sâu có một tầng ẩn. Điều này nghe có vẻ tẻ nhạt nhưng theo như cách nói dân giã, bạn phải “tập đi trước khi tập chạy”.\n","\n","Để đơn giản hóa vấn đề, ta giả sử mẫu đầu vào là  $x∈R^d$ và tầng ẩn của ta không có hệ số điều chỉnh. Ở đây biến trung gian là:\n","\n","$$\\mathbf{z}= \\mathbf{W}^{(1)} \\mathbf{x},$$\n","\n","Trong đó $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$ la tham số trọng số của tầng ẩn. Sau khi đưa biến trung gian $\\mathbf{z}\\in \\mathbb{R}^h$ qua hàm kích hoạt $\\phi$, ta thu được vector kích hoạt ẩn với $h$ phần tử,\n","\n","$$\\mathbf{h}= \\phi (\\mathbf{z}).$$\n","\n","Biến ẩn **$h$** cũng là một biến trung gian. Giả sử tham số của tầng đầu ra chỉ gồm trọng số  $W^{(2)}∈R^{q \\times h}$, ta sẽ thu được một vector với **$q$** phần tử ở tầng đầu ra:\n","\n","$$\\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h}.$$\n","\n","Giả sử hàm mất mát là $l$ và nhãn của mẫu là $y$, ta có thể tính được lượng mất mat cho một mẫu dữ liệu duy nhất,\n","\n","$$L = l(\\mathbf{o}, y).$$\n","\n","Theo định nghĩa của điều chuẩn $\\ell_2$ với siêu tham số $\\lambda$, lượng điều chuẩn là:\n","\n","$$s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right),$$\n","\n","trong đó chuẩn Frobenius của ma trận chỉ đơn giản là chuẩn  $L2$ của vector thu được sau khi trải phẳng ma trận. Cuối cùng, hàm mất mát được điều chuẩn của mô hình trên một mẫu dữ liệu cho trước là:\n","\n","$$J = L + s.$$\n","\n","Ta sẽ bàn thêm về *hàm mục tiêu* $J$ ở dưới.\n","\n","\n","\n","\n"],"metadata":{"id":"4u-_4DmLO-GJ"}},{"cell_type":"markdown","source":["#  5.3.2. Đồ thị tính toán của Lan truyền Xuôi\n","\n","Vẽ đồ thị tính toán giúp chúng ta hình dung được sự phụ thuộc giữa các toán tử và các biến trong quá trình tính toán. Hình dưới thể hiện đồ thị tương ứng với mạng nơ-ron đã miêu tả ở trên. Góc trái dưới biểu diễn đầu vào trong khi góc phải trên biểu diễn đầu ra. Lưu ý rằng hướng của các mũi tên (thể hiện luồng dữ liệu) chủ yếu là đi qua phải và hướng lên trên.\n","\n","![Computational graph of forward propagation.](http://d2l.ai/_images/forward.svg)"],"metadata":{"id":"CRTH74cvTNLA"}},{"cell_type":"markdown","source":["# 5.3.3. Lan truyền Ngược\n","\n","Lan truyền ngược là phương pháp tính gradient của các tham số mạng nơ-ron. Nói một cách đơn giản, phương thức này duyệt qua mạng nơ-ron theo chiều ngược lại, từ đầu ra đến đầu vào, tuân theo quy tắc dây chuyền trong giải tích.\n","\n","Thuật toán lan truyền ngược lưu trữ các biến trung gian (là các đạo hàm riêng) cần thiết trong quá trình tính toán gradient theo các tham số. Giả sử ta có hàm $\\mathsf{Y}=f(\\mathsf{X})$ và $\\mathsf{Z}=g(\\mathsf{Y}) = g \\circ f(\\mathsf{X})$, trong đó đầu vào và đầu ra $\\mathsf{X}, \\mathsf{Y}, \\mathsf{Z}$ là các tensor có kích thước bất kỳ. Bằng cách sử dụng quy tắc dây chuyền, ta có thể tính đạo hàm của $\\mathsf{Z}$ theo $\\mathsf{X}$ như sau:\n","$$\n","\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{X}} = \\text{prod}\\left(\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{Y}}, \\frac{\\partial \\mathsf{Y}}{\\partial \\mathsf{X}}\\right).\n","$$\n","\n","Ở đây, ta sử dụng toán từ $\\text{prod}$ để nhân các đối số sau khi các phép tính cần thiết như là chuyển vị và hoán đổi đã được thực hiện. Với vector, điều này khá đơn giản: nó chỉ đơn thuần là phép nhân ma trận. Với các tensor nhiều chiều thì sẽ có các phương án tương ứng phù hợp. Toán tử $\\text{prod}$ sẽ đơn giản hóa việc ký hiệu.\n","\n","Các tham số của mạng nơ-ron đơn giản với một tầng ẩn là $\\mathbf{W}^{(1)}$ và $\\mathbf{W}^{(2)}$. Mục đích của lan truyền ngược là để tính gradient $\\partial J/\\partial \\mathbf{W}^{(1)}$ và $\\partial J/\\partial \\mathbf{W}^{(2)}$. Để làm được điều này, ta áp dụng quy tắc dây chuyền và lần lượt tính gradient của các biến trung gian và tham số. Các phép tính trong lan truyền ngược có thứ tự ngược lại so với các phép tính trong lan truyền xuôi, bởi ta muốn bắt đầu từ kết quả của đồ thị tính toán rồi dần đi tới các tham số. Bước đầu tiên đó là tính gradient của hàm mục tiêu \n","$J=L+s$ theo mất mát $L$ và điều chuẩn $s$.\n","\n","$$\\frac{\\partial J}{\\partial L} = 1 \\; \\text{và} \\; \\frac{\\partial J}{\\partial s} = 1.$$\n","\n","Tiếp theo, ta tính gradient của hàm mục tiêu theo các biến của lớp đầu ra  $\\mathbf{o}$, sử dụng quy tắc dây chuyền.\n","\n","$$\n","\\frac{\\partial J}{\\partial \\mathbf{o}}\n","= \\text{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\mathbf{o}}\\right)\n","= \\frac{\\partial L}{\\partial \\mathbf{o}}\n","\\in \\mathbb{R}^q.\n","$$\n","\n","Kế tiếp, ta tính gradient của điều chuẩn theo cả hai tham số.\n","\n","$$\n","\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)}\n","\\; \\text{và} \\;\n","\\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}.\n","$$\n","\n","Bây giờ, ta có thể tính gradient $\\partial J/\\partial \\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$ của các tham số mô hình gần nhất với tầng đầu ra. Áp dụng quy tắc dây chuyền, ta có:\n","\n","$$\n","\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}\n","= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}\\right)\n","= \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}.\n","$$\n","\n","Để tính được gradient theo $\\mathbf{W}^{(1)}$ ta cần tiếp tục lan truyền ngược từ tầng đầu ra đến các tầng ẩn. Gradient theo các đầu ra của tầng ần \\partial J/\\partial \\mathbf{h} \\in \\mathbb{R}^h được tính như sau:\n","$$\n","\\frac{\\partial J}{\\partial \\mathbf{h}}\n","= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}}\\right)\n","= {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}.\n","$$\n","\n","Vì hàm kích hoạt $\\phi$ áp dụng cho từng phần tử, việc tính gradient $\\partial J/\\partial \\mathbf{z} \\in \\mathbb{R}^h$ của biến trung gian $z$ cũng yêu cầu sử dụng phép nhân theo từng phần tử kí hiệu bởi $\\odot$.\n","\n","$$\n","\\frac{\\partial J}{\\partial \\mathbf{z}}\n","= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{h}}, \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\\right)\n","= \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'\\left(\\mathbf{z}\\right).\n","$$\n","\n","Cuối cùng, ta có thể tính gradient $\\partial J/\\partial \\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$ của các tham số mô hình gần nhất với tầng đầu vào. Theo quy tắc dây chuyền ta có,\n","\n","$$\n","\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n","= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{z}}, \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}\\right)\n","= \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}.\n","$$"],"metadata":{"id":"T1EjAa87TfpS"}},{"cell_type":"markdown","source":["# 5.3.4 Huấn luyện Mô hình\n","\n","Khi huấn luyện các mạng nơ-ron, lan truyền xuôi và lan truyền ngược phụ thuộc lẫn nhau. Cụ thể với lan truyền xuôi, ta duyệt đồ thị tính toán theo hướng của các quan hệ phụ thuộc và tính tất cả các biến trên đường đi. Những biến này sau đó được sử dụng trong lan truyền ngược khi thứ tự tính toán trên đồ thị bị đảo ngược lại. Hệ quả là ta cần lưu trữ các giá trị trung gian cho đến khi lan truyền ngược hoàn tất. Đây cũng chính là một trong những lý do khiến lan truyền ngược yêu cầu nhiều bộ nhớ hơn đáng kể so với khi chỉ cần đưa ra dự đoán.\n","Ta tính các tensor gradient và giữ các biến trung gian lại để sử dụng trong quy tắc dây chuyền. Việc huấn luyện trên các minibatch chứa nhiều mẫu, do đó cần lưu trữ nhiều giá trị kích hoạt trung gian hơn cũng là một lý do khác."],"metadata":{"id":"Cp_SFikfcU8e"}}]}