{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvSbIs2sZxpqm31m0l9oPY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Ở phần 3.1, ta đã được giới thiệu hồi quy tuyến tính, triển khai toàn bộ từ đầu trong phần 3.4 và dùng API cao cấp trong phần 3.5.\n","\n","Hồi quy là công cụ đắc lực có thể sử dụng khi ta muốn trả lời câu hỏi bao nhiêu?. Nếu bạn muốn dự đoán một ngôi nhà sẽ được bán với giá bao nhiêu tiền (Đô la), hay số trận thắng mà một đội bóng có thể đạt được, hoặc số ngày một bệnh nhân phải điều trị nội trú trước khi được xuất viện, thì có lẽ bạn đang cần một mô hình hồi quy.\n","\n","Trong thực tế, chúng ta thường quan tâm đến việc phân loại hơn: không phải câu hỏi bao nhiêu? mà là loại nào?\n","\n","- Email này có phải thư rác hay không?\n","- Khách hàng này nhiều khả năng đăng ký hay không đăng ký một dịch vụ thuê bao?\n","- Hình ảnh này mô tả một con lừa, một con chó, một con mèo hay một con gà trống?\n","- Bộ phim nào có khả năng cao nhất được Aston xem tiếp theo?\n","\n","Thông thường, những người làm về học máy dùng từ phân loại để mô tả đôi chút sự khác nhau giữa hai bài toán: (i) ta chỉ quan tâm đến việc gán cứng một danh mục cho mỗi ví dụ: là chó, là gà, hay là mèo?; và (ii) ta muốn gán mềm tất cả các danh mục cho mỗi ví dụ, tức đánh giá xác suất một ví dụ rơi vào từng danh mục khả dĩ: là chó (92%), là gà (1%), là mèo (7%). Sự khác biệt này thường không rõ ràng, một phần bởi vì thông thường ngay cả khi chúng ta chỉ quan tâm đến việc gán cứng, chúng ta vẫn sử dụng các mô hình thực hiện các phép gán mềm."],"metadata":{"id":"pcre1CT-sGrz"}},{"cell_type":"markdown","source":["# 3.4.1. Bài toán phân loại\n","\n","Hãy khởi động với một bài toán phân loại hình ảnh đơn giản. Ở đây, mỗi đầu vào là một ảnh xám có kích thước  $2×2$. Bằng cách biểu diễn mỗi giá trị điểm ảnh bởi một số vô hướng, ta thu được bốn đặc trưng $x_1,x_2,x_3,x_4$. Hơn nữa, giả sử rằng mỗi hình ảnh đều thuộc về một trong các danh mục “mèo”, “gà” và “chó”.\n","\n","Tiếp theo, ta cần phải chọn cách biểu diễn nhãn. Ta có hai cách làm hiển nhiên. Cách tự nhiên nhất có lẽ là chọn  $y∈{1,2,3}$ lần lượt ứng với {chó, mèo, gà}. Đây là một cách lưu trữ thông tin tuyệt vời trên máy tính. Nếu các danh mục có một thứ tự tự nhiên giữa chúng, chẳng hạn như {trẻ sơ sinh, trẻ tập đi, thiếu niên, thanh niên, người trưởng thành, người cao tuổi}, sẽ là tự nhiên hơn nếu coi bài toán này là một bài toán hồi quy và nhãn sẽ được giữ nguyên dưới dạng số.\n","\n","Nhưng nhìn chung các lớp của bài toán phân loại không tuân theo một trật tự tự nhiên nào. May mắn thay, các nhà thông kê từ lâu đã tìm ra một cách đơn giản để có thể biểu diễn dữ liệu danh mục: biểu diễn one-hot. Biểu diễn one-hot là một vector với số lượng thành phần bằng số danh mục mà ta có. Thành phần tương ứng với từng danh mục cụ thể sẽ được gán giá trị 1 và tất cả các thành phần khác sẽ được gán giá trị 0.\n","$$y∈{(1,0,0),(0,1,0),(0,0,1)}.$$\n","\n","Trong trường hợp này,  $y$ sẽ là một vector 3 chiều, với $(1,0,0)$ tương ứng với “mèo”, $(0,1,0)$ ứng với “gà” và $(0,0,1)$ ứng với “chó”."],"metadata":{"id":"vAdY5JQbtS5m"}},{"cell_type":"markdown","source":["## 3.4.1.1. Kiến trúc mạng\n","\n","Để tính xác suất có điều kiện ứng với mỗi lớp, chúng ta cần một mô hình có nhiều đầu ra với một đầu ra cho mỗi lớp. Để phân loại với các mô hình tuyến tính, chúng ta cần số hàm tuyến tính tương đương số đầu ra. Mỗi đầu ra sẽ tương ứng với hàm tuyến tính của chính nó. Trong trường hợp này, vì có 4 đặc trưng và 3 đầu ra, chúng ta sẽ cần 12 số vô hướng để thể hiện các trọng số, ( $w$ với các chỉ số dưới) và 3 số vô hướng để thể hiện các hệ số điều chỉnh ( $b$ với các chỉ số dưới). Chúng ta sẽ tính ba logits,  $o_1$,$o_2$, và $o_3$, cho mỗi đầu vào:\n","\n","$$\n","\\begin{aligned}\n","o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n","o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n","o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n","\\end{aligned}\n","$$\n","\n","Chúng ta có thể mô tả phép tính này với biểu đồ mạng nơ-ron được thể hiện trong Fig. 3.4.1. Như hồi quy tuyến tính, hồi quy softmax cũng là một mạng nơ-ron đơn tầng. Và vì sự tính toán của mỗi đầu ra, $o_1,o_2,$ và  $o_3$, phụ thuộc vào tất cả đầu vào,  $x_1,  x_2, x_3,$ và  $x_4$, tầng đầu ra của hồi quy softmax cũng có thể được xem như một tầng kết nối đầy đủ.\n","\n","![Softmax regression is a single-layer neural network.](http://d2l.ai/_images/softmaxreg.svg)\n","\n","Để biểu diễn mô hình gọn hơn, chúng ta có thể sử dụng ký hiệu đại số tuyến tính. Ở dạng vector, ta có  $o=Wx+b$, một dạng phù hợp hơn cho cả toán và lập trình. Chú ý rằng chúng ta đã tập hợp tất cả các trọng số vào một ma trận  $3×4$ và với một mẫu cho trước $x$, các đầu ra được tính bởi tích ma trận-vector của các trọng số và đầu vào cộng với vector hệ số điều chỉnh  $b$.\n","\n","## 3.4.1.2. Hàm Softmax\n","\n","Chúng ta sẽ xem các giá trị đầu ra của mô hình là các giá trị xác suất. Ta sẽ tối ưu hóa các tham số của mô hình sao cho khả năng xuất hiện dữ liệu quan sát được là cao nhất. Sau đó, ta sẽ đưa ra dự đoán bằng cách đặt ngưỡng xác suất, ví dụ dự đoán nhãn đúng là nhãn có xác suất cao nhất (dùng hàm argmax).\n","\n","Nói một cách chính quy hơn, ta mong muốn diễn dịch kết quả $\\hat{y}_k$ là xác suất để một điểm dữ liệu cho trước thuộc về một lớp k nào đó. Sau đó, ta có thể chọn lớp cho điểm đó tương ứng với giá trị lớn nhất mà mô hình dự đoán $argmax_ky_k$. Ví dụ, nếu  $\\hat{y}_1, \\hat{y}_2$ và $\\hat{y}_3$ lần lượt là $0.1, 0.8, 0.1$, thì ta có thể dự đoán điểm đó thuộc về lớp số 2 là “gà” (ứng với trong ví dụ trước).\n","\n","Bạn có thể muốn đề xuất rằng ta lấy trực tiếp logit  $o$ làm đầu ra mong muốn. Tuy nhiên, sẽ có vấn đề khi coi kết qủa trả về trực tiếp từ tầng tuyến tính như là các giá trị xác suất. Lý do là không có bất cứ điều kiện nào để ràng buộc tổng của những con số này bằng  1. Hơn nữa, tùy thuộc vào đầu vào mà ta có thể nhận được giá trị âm. Các lý do trên khiến kết quả của tầng tuyến tính vi phạm vào các tiên đề cơ bản của xác xuất đã được nhắc đến trong Section 2.6.\n","\n","Để có thể diễn dịch kết quả đầu ra là xác xuất, ta phải đảm bảo rằng các kết quả không âm và tổng của chúng phải bằng 1 (điều này phải đúng trên cả dữ liệu mới). Hơn nữa, ta cần một hàm mục tiêu trong quá trình huấn luyện để cho mô hình có thể ước lượng xác suất một cách chính xác. Trong tất cả các trường hợp, khi kết quả phân lớp cho ra xác suất là  $0.5$ thì ta hy vọng phân nửa số mẫu đó thực sự thuộc về đúng lớp được dự đoán. Đây được gọi là hiệu chuẩn.\n","\n","Hàm softmax, được phát minh vào năm 1959 bởi nhà khoa học xã hội R Duncan Luce với chủ đề mô hình lựa chọn, thỏa mãn chính xác những điều trên. Để biến đổi kết quả logit thành kết quả không âm và có tổng là  $1$, trong khi vẫn giữ tính chất khả vi, đầu tiên ta cần lấy hàm mũ cho từng logit (để chắc chắn chúng không âm) và sau đó ta chia cho tổng của chúng (để chắc rằng tổng của chúng luôn bằng 1).\n","\n","$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\text{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}.$$\n","\n","Dễ thấy rằng, $\\hat{y}_1 + \\hat{y}_2 + \\hat{y}_3 = 1$ với $0≤\\hat{y}_i≤1$ với mọi $i$. Do đó $\\hat{y}$ là phân phối xác suất phù hợp với các giá trị của $\\hat{\\mathbf{y}}$ có thể được hiểu theo đó. Lưu ý rằng hàm softmax không thay đổi thứ tự giữa các logit và do đó ta vẫn có thể chọn ra lớp phù hợp nhất bằng cách:\n","$$\n","\\operatorname*{argmax}_j \\hat y_j = \\operatorname*{argmax}_j o_j.\n","$$\n","\n","Các logit $o$ đơn giản chỉ là các giá trị trước khi cho qua hàm softmax để xác định xác xuất thuộc về mỗi danh mục. "],"metadata":{"id":"iRMoUXRzyP-a"}},{"cell_type":"markdown","source":["## 3.4.1.3. Vector hóa Minibatch\n","\n","Để cải thiện hiệu suất tính toán và tận dụng GPU, ta thường phải thực hiện các phép tính vector cho các minibatch dữ liệu. Giả sử, ta có một minibatch $X$ của mẫu với số chiều $d$ và kích thước batch là $n$. Thêm vào đó, chúng ta có $q$ lớp đầu ra. Như vậy, minibatch đặc trưng $X$ sẽ thuộc $R^{n×d}$, trọng số  $W∈R^{d×q}$, và độ chệch sẽ thỏa mãn $b∈R^q$.\n","\n","$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$\n","\n","Việc tăng tốc diễn ra chủ yếu tại tích ma trận - ma trận  $WX$ so với tích ma trận - vector nếu chúng ta xử lý từng mẫu một. Bản thân softmax có thể được tính bằng cách lũy thừa tất cả các mục trong  $\\mathbf{O}$ và sau đó chuẩn hóa chúng theo tổng.\n"],"metadata":{"id":"-agsQwju5xJF"}},{"cell_type":"markdown","source":["# 3.4.2. Hàm mất mát.\n","\n","Tiếp theo, chúng ta cần một hàm mất mát để đánh giá chất lượng các dự đoán xác suất. Chúng ta sẽ dựa trên hợp lý cực đại, khái niệm tương tự đã gặp khi đưa ra lý giải xác suất cho hàm mục tiêu bình phương nhỏ nhất trong hồi quy tuyến tính (Section 3.1)."],"metadata":{"id":"q1rTtwRn85Ad"}},{"cell_type":"markdown","source":["## 3.4.2.1. Log hợp lý\n","\n","Hàm softmax cho ta một vector $\\hat{y}$, có thể được hiểu như các xác suất có điều khiện của từng lớp với đầu vào $x$. Ví dụ: $\\hat{y}_1 = \\hat{P}( y = cat |  \\mathbf{x})$. Để biết các ước lượng có sát vs thực tế hay không, ta kiểm tra xác suất mà mô hình gán cho lớp thật khi biết các đặc trưng.\n","\n","$$\n","P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n","$$\n","\n","Vì thế,\n","\n","$$\n","-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n","= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n","$$\n","\n","Cực đại hóa $P(Y|X)$ (tương đương với cực tiểu hóa $-P(Y|X)$) giúp việc dự đoán nhãn tốt hơn. Từ đó, ta ra được hàm mất mát\n","\n","$$\n","l = -\\log P(y \\mid x) = -\\sum_{j}y_j \\log\\hat{y}_j $$\n","\n","Bởi vì những lý do sẽ được giải thích sau đây, hàm mất mát này thường được gọi là mất mát entropy chéo. Ở đây, chúng ta đã sử dụng nó bằng cách xây dựng $\\hat{y}$ giống như một phân phối xác suất rời rạc và vector  $\\mathbf{y}$ là một vector one-hot. Do đó, tổng các số hạng với chỉ số  $j$ sẽ tiêu biến tạo thành một giá trị duy nhất. Bởi mọi $\\hat{y}_j$ đều là xác suất, log của chúng không bao giờ lớn hơn  0. Vì vậy, hàm mất mát sẽ không thể giảm thêm được nữa nếu chúng ta dự đoán chính xác  $y$ với độ chắc chắn tuyệt đối, tức  P(y∣x)=1 cho nhãn đúng. Chú ý rằng điều này thường không khả thi. Ví dụ, nhãn bị nhiễu sẽ xuất hiện trong tập dữ liệu (một vài mẫu bị dán nhầm nhãn). Điều này cũng khó xảy ra khi những đặc trưng đầu vào không chứa đủ thông tin để phân loại các mẫu một cách hoàn hảo."],"metadata":{"id":"CVGNg5wT9CpR"}},{"cell_type":"markdown","source":["## 3.4.2.2. Softmax và Đạo hàm\n","\n","Vì softmax và hàm mất mát softmax rất phổ biến, nên việc hiểu cách tính giá trị các hàm này sẽ có ích về sau. Thay  $o$ vào định nghĩa của hàm mất mát  $l$ và dùng định nghĩa của softmax, ta được:\n","\n","$$\n","\\begin{aligned}\n","l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n","&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j \\\\\n","&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n","\\end{aligned}\n","$$\n","\n","để hiểu rõ hơn, hãy xét đạo hàm riêng của $l$ theo $o$. Ta có:\n","$$\n","\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j = P(y=j|x)-y_j.\n","$$\n","\n","Nói cách khác, gradient chính là hiệu giữa xác xuất mô hình gán cho lớp đúng  $P(y∣x)$, và nhãn của dữ liệu $y$. Điều này cũng tương tự như trong bài toán hồi quy, khi gradient là hiệu giữa dữ liệu quan sát được $y$ và kết quả ước lượng $\\hat{y}$. Đây không phải là ngẫu nhiên. Trong mọi mô hình họ lũy thừa, gradient của hàm log hợp lý đều có dạng như thế này. Điều này giúp cho việc tính toán gradient trong thực tế trở nên dễ dàng hơn."],"metadata":{"id":"DEWUzSuTAQmJ"}},{"cell_type":"markdown","source":["## 3.4.2.3. hàm mất mát Entropy chéo\n","\n","Giờ hãy xem xét trường hợp mà ta quan sát được toàn bộ phân phối của đầu ra thay vì chỉ một giá trị đầu ra duy nhất. Ta có thể biểu diễn  $y$ giống hệt như trước. Sự khác biệt duy nhất là thay vì có một vector chỉ chứa các phần tử nhị phân, giả sử như $(0,0,1)$, giờ ta có một vector xác suất tổng quát, ví dụ như  $(0.1,0.2,0.7)$. Các công thức toán học ta dùng trước đó để định nghĩa hàm mất mát  $l$ vẫn áp dụng tốt ở đây nhưng khái quát hơn một chút. Giá trị của các phần tử trong vector tương ứng giá trị kỳ vọng của hàm mất mát trên phân phối của nhãn.\n","\n","$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$\n","\n","Hàm trên được gọi là hàm mát mát entropy chéo và là một trong những hàm mất mát phổ biến nhất dùng cho bài toán phân loại đa lớp. Ta có thể làm sáng tỏ cái tên entropy chéo bằng việc giới thiệu các kiến thức cơ bản trong lý thuyết thông tin."],"metadata":{"id":"YIbmZ7fBcLMb"}},{"cell_type":"markdown","source":["# 3.4.3. Lý thuyết Thông tin Cơ bản\n","\n","Lý thuyết thông tin giải quyết các bài toán mã hóa, giải mã, truyền tải và xử lý thông tin (hay còn được gọi là dữ liệu) dưới dạng ngắn gọn nhất có thể."],"metadata":{"id":"EvZvHaZegQMZ"}},{"cell_type":"markdown","source":["##3.4.3.1. Entropy\n","\n","Ý tưởng cốt lõi trong lý thuyết thông tin chính là việc định lượng lượng thông tin chứa trong dữ liệu. Giá trị định lượng này chỉ ra giới hạn tối đa cho khả năng nén dữ liệu (khi tìm biểu diễn ngắn gọn nhất mà không mất thông tin). Giá trị định lượng này gọi là entropy, xác định trên phân phối $p$ của bộ dữ liệu, được định nghĩa bằng phương trình dưới đây:\n","\n","$$H[P] = \\sum_j - P(j) \\log P(j).$$\n","\n","Một định lý căn bản của lý thuyết thông tin là để có thể mã hóa dữ liệu thu thập ngẫu nhiên từ phân phối $p$, chúng ta cần sử dụng ít nhất $H_{[p]}$ “nat”. “nat” là đơn vị biểu diễn dữ liệu sử dụng cơ số $e$, tương tự với bit biểu diễn dữ liệu sử dụng cơ số 2. Một nat bằng  1log(2)≈1.44 bit. $H_{[p]}/2$ thường được gọi là entropy nhị phân.\n","\n"],"metadata":{"id":"3DraqSm9gtNI"}},{"cell_type":"markdown","source":["## 3.4.3.2. Lượng tin\n","\n","Có lẽ bạn sẽ tự hỏi việc nén dữ liệu thì liên quan gì với việc đưa ra dự đoán? Hãy tưởng tượng chúng ta có một luồng (stream) dữ liệu cần nén. Nếu ta luôn có thể dễ dàng đoán được đơn vị dữ liệu (token) kế tiếp thì dữ liệu này rất dễ nén! Ví như tất cả các đơn vị dữ liệu trong dòng dữ liệu luôn có một giá trị cố định thì đây là một dòng dữ liệu tẻ nhạt! Không những tẻ nhạt, mà nó còn dễ đoán nữa. Bởi vì chúng luôn có cùng giá trị, ta sẽ không phải truyền bất cứ thông tin nào để trao đổi nội dung của dòng dữ liệu này. Dễ đoán thì cũng dễ nén là vậy.\n","\n","Tuy nhiên, nếu ta không thể dự đoán một cách hoàn hảo cho mỗi sự kiện, thì thi thoảng ta sẽ thấy ngạc nhiên. Sự ngạc nhiên trong chúng ta sẽ lớn hơn khi ta gán một xác suất thấp hơn cho sự kiện. Vì nhiều lý do mà chúng ta sẽ nghiên cứu trong phần phụ lục, Claude Shannon đã đưa ra giải pháp  $log(1/p(j))=−logp(j)$\n","để định lượng sự ngạc nhiên của một người lúc quan sát sự kiện  $j$\n"," sau khi đã gán cho sự kiện đó một xác suất (chủ quan)  $p(j)$. Entropy lúc này sẽ là lượng tin (độ ngạc nhiên) kỳ vọng khi mà xác suất của các sự kiện đó được gán chính xác, khớp với phân phối sinh dữ liệu. Nói cách khác, entropy là lượng thông tin hay mức độ ngạc nhiên tối thiểu mà dữ liệu sẽ đem lại theo kỳ vọng."],"metadata":{"id":"KERYoKYhpZLI"}},{"cell_type":"markdown","source":["## 3.4.3.3. Xem xét lại Entropy chéo\n","\n","Nếu entropy là mức độ ngạc nhiên trải nghiểm bởi 1 người nắm rõ xác suất thật, thì bạn có thể bằng khoăn rằng entropy chéo là gì ?!\n","\n","Entropy chéo từ $p$ đến $q$ ký hiệu $H(p,q)$, là sự ngạc nhiên kỳ vọng của một người quan sát với xác suất chủ quan $q$ đối với dữ liệu sinh ra dựa trên các xác suất $p$. Giá trị entropy chéo thấp nhất có thể đạt được khi $p=q$. Trong trường hợp này, entropy chéo từ $p$ đến $q$ là $H(p,p) = H(p)$. Liên hệ điều này lại với mục tiêu phân loại của chúng ta, thậm chí khi ta có khả năng dự đoán tốt nhất có thể và cho rằng việc này khả thi, thì ta sẽ không bao giờ đạt mức hoàn hảo. Mất mát của ta bị giới hạn dưới bởi entropy tạo bởi các phân phối thực tế có điều kiện $P(y|x)$"],"metadata":{"id":"gyQiV3Vfp8Cs"}},{"cell_type":"markdown","source":["#3.4.3.4. Phân kỳ Kullback Leibler\n","\n","Có lẽ cách thông dụng nhất để đo lường khoảng cách giữa hai phân phối là tính toán phân kỳ Kullback Leibler $D(p∥q)$. Phân kỳ Kullback Leibler đơn giản là sự khác nhau giữa entropy chéo và entropy, có nghĩa là giá trị entropy chéo bổ sung phát sinh so với giá trị nhỏ nhất không thể giảm được mà nó có thể nhận:\n","![image.png](data:image/png;base64,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)\n"],"metadata":{"id":"S3PX2f3yrsW0"}},{"cell_type":"markdown","source":["Lưu ý rằng trong bài toán phân loại, ta không biết giá trị thật của  p\n"," , vì thế mà ta không thể tính toán entropy trực tiếp được. Tuy nhiên, bởi vì entropy nằm ngoài tầm kiểm soát của chúng ta, việc giảm thiểu $D(p∥q)$ so với  $q$ là tương đương với việc giảm thiểu mất mát entropy chéo.\n","\n","Tóm lại, chúng ta có thể nghĩ đến mục tiêu của phân loại entropy chéo theo hai hướng: (i) cực đại hóa khả năng xảy ra của dữ liệu được quan sát; và (ii) giảm thiểu sự ngạc nhiên của ta (cũng như số lượng các bit) cần thiết để truyền đạt các nhãn."],"metadata":{"id":"YZzCigqTsras"}},{"cell_type":"markdown","source":["#3.4.4. Sử dụng Mô hình để dự đoán và đánh giá\n","\n","Sau khi huấn luyện mô hình hồi quy softmax với các đặc trưng đầu vào bất kì, chúng ta có thể dự đoán xác suất đầu ra ứng với mỗi lớp. Thông thường, chúng ta sử dụng lớp với xác suất dự đoán cao nhất làm lớp đầu ra. Một dự đoán được xem là chính xác nếu nó trùng khớp hay tương thích với lớp thật sự (nhãn). Ở phần tiếp theo của thí nghiệm, chúng ta sẽ sử dụng độ chính xác để đánh giá chất lượng của mô hình. Giá trị này là tỉ lệ giữa số mẫu được dự đoán chính xác so với tổng số mẫu được dự đoán."],"metadata":{"id":"KJyr94Sis27t"}}]}