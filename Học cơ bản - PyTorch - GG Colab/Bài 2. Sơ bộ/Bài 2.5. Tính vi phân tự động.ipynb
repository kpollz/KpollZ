{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6Ya3ntq0/N3YH0vziAhPJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["**-- Giới Thiệu --**\n","\n","Như đã giải thích trong Section 2.4, vi phân là phép tính thiết yếu trong hầu như tất cả mọi thuật toán học sâu. Mặc dù các phép toán trong việc tính đạo hàm khá trực quan và chỉ yêu cầu một chút kiến thức giải tích, nhưng với các mô hình phức tạp, việc tự tính rõ ràng từng bước khá là mệt (và thường rất dễ sai).\n","\n","Gói thư viện autograd giải quyết vấn đề này một cách nhanh chóng và hiệu quả bằng cách tự động hoá các phép tính đạo hàm (automatic differentiation). Trong khi nhiều thư viện yêu cầu ta phải biên dịch một đồ thị biểu tượng (symbolic graph) để có thể tự động tính đạo hàm, autograd cho phép ta tính đạo hàm ngay lập tức thông qua các dòng lệnh thông thường. Mỗi khi đưa dữ liệu chạy qua mô hình, autograd xây dựng một đồ thị và theo dõi xem dữ liệu nào kết hợp với các phép tính nào để tạo ra kết quả. Với đồ thị này autograd sau đó có thể lan truyền ngược gradient lại theo ý muốn. Lan truyền ngược ở đây chỉ đơn thuần là truy ngược lại đồ thị tính toán và điền vào đó các giá trị đạo hàm riêng theo từng tham số."],"metadata":{"id":"AVFkpShksiYn"}},{"cell_type":"code","execution_count":71,"metadata":{"id":"6yAzUccEsOtW","executionInfo":{"status":"ok","timestamp":1676733740988,"user_tz":-420,"elapsed":329,"user":{"displayName":"vu tung","userId":"03394210267989276557"}}},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["# 2.5.1. Một ví dụ đơn giản\n","\n","Lấy ví dụ đơn giản, giả sử chúng ta muốn tính vi phân của hàm số $y=2x^⊤x$ theo vector cột $x$. Để bắt đầu, ta sẽ tạo biến x và gán cho nó một giá trị ban đầu."],"metadata":{"id":"AbKYhrEvu3Tv"}},{"cell_type":"code","source":["x = torch.arange(4, dtype = torch.float32)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOsTUryIvhuu","executionInfo":{"status":"ok","timestamp":1676733741516,"user_tz":-420,"elapsed":14,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"407b5128-b02a-4d0d-ae54-edb690583ec7"},"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 1., 2., 3.])"]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","source":["Lưu ý rằng trước khi có thể tính gradient của $y$ theo $x$ , chúng ta cần một nơi để lưu giữ nó. Điều quan trọng là ta không được cấp phát thêm bộ nhớ mới mỗi khi tính đạo hàm theo một biến xác định, vì ta thường cập nhật cùng một tham số hàng ngàn vạn lần và sẽ nhanh chóng dùng hết bộ nhớ.\n","\n","Cũng lưu ý rằng, bản thân giá trị gradient của hàm số đơn trị theo một vector  x  cũng là một vector với cùng kích thước. Do vậy trong mã nguồn sẽ trực quan hơn nếu chúng ta lưu giá trị gradient tính theo x dưới dạng một thuộc tính của chính ndarray x. Chúng ta cấp bộ nhớ cho gradient của một ndarray bằng cách gọi phương thức attach_grad."],"metadata":{"id":"gAtLOdGpvy7c"}},{"cell_type":"code","source":["# Can also create x = torch.arange(4.0, requires_grad=True)\n","x.requires_grad_(True)\n","x.grad  # The gradient is None by default"],"metadata":{"id":"jN3WxWX1ySZV","executionInfo":{"status":"ok","timestamp":1676733741517,"user_tz":-420,"elapsed":13,"user":{"displayName":"vu tung","userId":"03394210267989276557"}}},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["Giờ ta sẽ tính $y$ theo $x$"],"metadata":{"id":"kZu2pNk2zDIG"}},{"cell_type":"code","source":["y = 2 * torch.dot(x,x)\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3qOx4YNzX7p","executionInfo":{"status":"ok","timestamp":1676733741517,"user_tz":-420,"elapsed":13,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"98474878-9864-4e5f-c321-1895df6521d3"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(28., grad_fn=<MulBackward0>)"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["Bởi vì $x$ là một tensor có độ dài bằng 4, torch.dot sẽ tính toán tích vô hướng của $x$ và $x$, trả về một số vô hướng mà sẽ được gán cho y. Tiếp theo, ta có thể tính toán gradient của y theo mỗi thành phần của $x$ một cách tự động bằng cách gọi hàm `backward` của y."],"metadata":{"id":"4qp8XDYwzs0N"}},{"cell_type":"code","source":["y.backward()"],"metadata":{"id":"SUCeQgrPz5qy","executionInfo":{"status":"ok","timestamp":1676733741517,"user_tz":-420,"elapsed":11,"user":{"displayName":"vu tung","userId":"03394210267989276557"}}},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":["Nếu kiểm tra lại giá trị của x.grad, ta sẽ thấy nó đã được ghi đè bằng gradient mới được tính toán."],"metadata":{"id":"ljDpp0ro0F74"}},{"cell_type":"code","source":["x.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4a7n6vk0HKF","executionInfo":{"status":"ok","timestamp":1676733741518,"user_tz":-420,"elapsed":12,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"a8945fe3-de95-455b-b406-3bc472372a58"},"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.,  4.,  8., 12.])"]},"metadata":{},"execution_count":76}]},{"cell_type":"markdown","source":["Gradient của hàm $y=2x^⊤x$ theo $x$ phải là $4x$. Hãy kiểm tra một cách nhanh chóng rằng giá trị gradient mong muốn được tính toán đúng. Nếu hai tensor là giống nhau, thì mọi cặp phần tử tương ứng cũng bằng nhau."],"metadata":{"id":"1_9WIVIp0OIM"}},{"cell_type":"code","source":["x.grad == 4*x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHRYqIxX0Yag","executionInfo":{"status":"ok","timestamp":1676733741518,"user_tz":-420,"elapsed":11,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"fe106a77-a6eb-4780-c238-503e2cd4e6cf"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True])"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","source":["Nếu ta tiếp tục tính gradient của một biến khác mà giá trị của nó là kết quả của một hàm theo biến x, thì nội dung trong x.grad sẽ bị ghi đè. Ngoài ra ta cũng có thể reset bộ nhớ của nó bằng hàm `x.grad.zero()`"],"metadata":{"id":"_nEZH6N900f_"}},{"cell_type":"code","source":["\"\"\" Thắc mắc \"\"\"\n","# x.grad.zero_()\n","print(x.grad)\n","y = x.sum()\n","y.backward()\n","print(x.grad, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vS1H7Eww1C8u","executionInfo":{"status":"ok","timestamp":1676733741518,"user_tz":-420,"elapsed":9,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"114841c7-8c30-4d42-96f4-56add396d3c4"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.,  4.,  8., 12.])\n","tensor([ 1.,  5.,  9., 13.]) tensor(6., grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"markdown","source":["# 2.5.2. Truyền ngược cho các biến không phải Số vô hướng\n","\n","Về mặt kỹ thuật, khi y không phải một số vô hướng, cách diễn giải tự nhiên nhất cho vi phân của một vector y theo vector x đó là một ma trận. Với các bậc và chiều cao hơn của y và x, kết quả của phép vi phân có thể là một tensor bậc cao.\n","\n","Tuy nhiên, trong khi những đối tượng như trên xuất hiện trong học máy nâng cao (bao gồm học sâu), thường thì khi ta gọi lan truyền ngược trên một vector, ta đang cố tính toán đạo hàm của hàm mất mát theo mỗi batch bao gồm một vài mẫu huấn luyện. Ở đây, ý định của ta không phải là tính toán ma trận vi phân mà là tổng của các đạo hàm riêng được tính toán một cách độc lập cho mỗi mẫu trong batch.\n","\n","Vậy nên khi ta gọi backward lên một biến vector y – là một hàm của x, MXNet sẽ cho rằng ta muốn tính tổng của các gradient. Nói ngắn gọn, MXNet sẽ tạo một biến mới có giá trị là số vô hướng bằng cách cộng lại các phần tử trong y và tính gradient theo x của biến mới này."],"metadata":{"id":"PLJaYlxF3p2t"}},{"cell_type":"code","source":["# x.grad.zero_()\n","# y = x * x\n","# print(y)\n","# y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n","# x.grad\n","\n","x.grad.zero_()\n","y = torch.dot(x,x)\n","y.backward()  # Faster: y.sum().backward()\n","x.grad == 2*x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x1-hXiT25jfA","executionInfo":{"status":"ok","timestamp":1676733741518,"user_tz":-420,"elapsed":8,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"1aa772e8-0f7f-4fe0-b7ee-0cf5c66840c4"},"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True])"]},"metadata":{},"execution_count":79}]},{"cell_type":"markdown","source":["# 2.5.3. Tách rời Tính toán\n","\n","Đôi khi chúng ta muốn chuyển một số phép tính ra khỏi đồ thị tính toán. Ví dụ, giả sử y đã được tính như một hàm của x, rồi sau đó z được tính như một hàm của cả y và x. Bây giờ, giả sử ta muốn tính gradient của z theo x, nhưng vì lý do nào đó ta lại muốn xem y như là một hằng số và chỉ xét đến vai trò của x như là biến số của z sau khi giá trị của y đã được tính.\n","\n","Trong trường hợp này, ta có thể gọi u = y.detach() để trả về một biến u mới có cùng giá trị như y nhưng không còn chứa các thông tin về cách mà y đã được tính trong đồ thị tính toán. Nói cách khác, gradient sẽ không thể chảy ngược qua u về x được. Bằng cách này, ta đã tính u như một hàm của x ở ngoài phạm vi của autograd.record, dẫn đến việc biến u sẽ được xem như là một hằng số mỗi khi ta gọi backward. Chính vì vậy, hàm backward sau đây sẽ tính đạo hàm riêng của z = u * x theo x khi xem u như là một hằng số, thay vì đạo hàm riêng của z = x * x * x theo x."],"metadata":{"id":"YNg_vaVm7I38"}},{"cell_type":"code","source":["x.grad.zero_()\n","y = x * x\n","u = y.detach()\n","z = u*x\n","\n","z.sum().backward()\n","x.grad == u"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7nuf0tNl3Zf","executionInfo":{"status":"ok","timestamp":1676733741519,"user_tz":-420,"elapsed":8,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"8d50cfc6-5cf2-4d8b-e8cc-f35909929f61"},"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True])"]},"metadata":{},"execution_count":80}]},{"cell_type":"markdown","source":["Bởi vì sự tính toán của y đã được ghi lại, chúng ta có thể gọi y.backward() sau đó để lấy đạo hàm của y = x * x theo x, tức là 2 * x."],"metadata":{"id":"La8QlMpdszcp"}},{"cell_type":"code","source":["x.grad.zero_()\n","y.sum().backward()\n","x.grad == 2 * x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSVKy-1Msy9h","executionInfo":{"status":"ok","timestamp":1676733741519,"user_tz":-420,"elapsed":7,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"8991dea2-20af-483a-a9d9-694d7e89583e"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True])"]},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","source":["# 2.5.4. Tính gradient của Luồng điều khiển Python\n","\n","Một lợi thế của việc sử dụng vi phân tự động là khi việc xây dựng đồ thị tính toán đòi hỏi trải qua một loạt các câu lệnh điều khiển luồng Python, (ví dụ như câu lệnh điều kiện, vòng lặp và các lệnh gọi hàm tùy ý), ta vẫn có thể tính gradient của biến kết quả. Trong đoạn mã sau, hãy lưu ý rằng số lần lặp của vòng lặp while và kết quả của câu lệnh if đều phụ thuộc vào giá trị của đầu vào a."],"metadata":{"id":"_9tr2ykDtAhC"}},{"cell_type":"code","source":["def f(a):\n","  b = a * 2\n","  while b.norm() < 1000:\n","    b = b * 2\n","  if b.sum() > 0:\n","    c = b \n","  else:\n","    c = 100*b \n","  return c\n"],"metadata":{"id":"ORamnz4hwkpT","executionInfo":{"status":"ok","timestamp":1676734441409,"user_tz":-420,"elapsed":332,"user":{"displayName":"vu tung","userId":"03394210267989276557"}}},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":["Trước khi chạy, ta tạo 1 tập input ngẫu nhiên nên do đó không biết được dạng đồ thị sẽ nhận được. "],"metadata":{"id":"9d6XG1kVJz0N"}},{"cell_type":"code","source":["a = torch.randn(size=(), requires_grad=True)\n","d = f(a)\n","d.backward()"],"metadata":{"id":"J4OQDN9qKEGH","executionInfo":{"status":"ok","timestamp":1676734443478,"user_tz":-420,"elapsed":358,"user":{"displayName":"vu tung","userId":"03394210267989276557"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["Giờ ta có thể phân tích hàm f được định nghĩa ở phía trên. Hãy để ý rằng hàm này tuyến tính từng khúc theo đầu vào a. Nói cách khác, với mọi giá trị của a tồn tại một hằng số k sao cho $f(a) = k * a$, ở đó giá trị của k phụ thuộc vào đầu vào a. Do đó, ta có thể kiểm tra giá trị của gradient bằng cách tính $d / a$."],"metadata":{"id":"i36LS9ANKrkw"}},{"cell_type":"code","source":["a.grad == d/a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fTs0gwsrKvFW","executionInfo":{"status":"ok","timestamp":1676734565537,"user_tz":-420,"elapsed":356,"user":{"displayName":"vu tung","userId":"03394210267989276557"}},"outputId":"abb049a0-9306-4fc4-f52b-464337bbac2d"},"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(True)"]},"metadata":{},"execution_count":86}]}]}